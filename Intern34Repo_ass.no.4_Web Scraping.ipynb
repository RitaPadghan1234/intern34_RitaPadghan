{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c2a144",
   "metadata": {},
   "source": [
    "#                                        WEB SCRAPING – ASSIGNMENT 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcca28",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f8311015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bbaf5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f36b87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4ffadc9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No.', 'Video name', 'Uploader', 'Views (billions)', 'Publication date']\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #opening a wikipedia page on automated chrome browser\n",
    "    driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "    #scroll down\n",
    "    driver.execute_script(\"window.scrollTo(0,900)\") #scroll down\n",
    "    title = []\n",
    "    \n",
    "    #titletags = driver.find_elements(By.XPATH,'/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/caption')\n",
    "    #for i in titletags:\n",
    "    #    title = i.text\n",
    "    #print(title)\n",
    "    \n",
    "    #scrape column heading\n",
    "    leadgetags = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]')\n",
    "    headingtags = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tr[1]//th')\n",
    "    heading = []\n",
    "    for i in leadgetags:\n",
    "        for j in headingtags:\n",
    "            colum = j.text\n",
    "            heading.append(colum)\n",
    "    print(heading[0:5])\n",
    "    \n",
    "    #scraping all table data\n",
    "    allrowstags = driver.find_elements(By.XPATH,'//*[@class=\"wikitable sortable jquery-tablesorter\"]//tr//td')\n",
    "    allrows = []\n",
    "    num = []\n",
    "    Rank = []\n",
    "    col1 = []\n",
    "    for i in allrowstags:\n",
    "        num = i.text\n",
    "        allrows.append(num)\n",
    "    #allrows\n",
    "    \n",
    "    #scraping all data columnwise we use indexing method of slicing of data\n",
    "    column1_Rank = [30]\n",
    "    column2_Video_name = [30]\n",
    "    column3_Uploader = [30]\n",
    "    column4_Views = [30]\n",
    "    column5_Publication_Date = [30]\n",
    "\n",
    "    column1_Rank = allrows[0],allrows[6],allrows[12],allrows[18],allrows[24],allrows[30],allrows[36],allrows[42],allrows[48],allrows[54],allrows[60],allrows[66],allrows[72],allrows[78],allrows[84],allrows[90],allrows[96],allrows[102],allrows[108],allrows[114],allrows[120],allrows[126],allrows[132],allrows[138],allrows[144],allrows[150],allrows[156],allrows[162],allrows[168],allrows[174]\n",
    "\n",
    "    column2_Video_name = [allrows[1],allrows[7],allrows[13],allrows[19],allrows[25],allrows[31],allrows[37],allrows[43],allrows[49],allrows[55],allrows[61],allrows[67],allrows[73],allrows[79],allrows[85],allrows[91],allrows[97],allrows[103],allrows[109],allrows[115],allrows[121],allrows[127],allrows[133],allrows[139],allrows[145],allrows[151],allrows[157],allrows[163],allrows[169],allrows[175]]\n",
    "\n",
    "    column3_Uploader = [allrows[2],allrows[8],allrows[14],allrows[20],allrows[26],allrows[32],allrows[38],allrows[44],allrows[50],allrows[56],allrows[62],allrows[68],allrows[74],allrows[80],allrows[86],allrows[92],allrows[98],allrows[104],allrows[110],allrows[116],allrows[122],allrows[128],allrows[134],allrows[140],allrows[146],allrows[152],allrows[158],allrows[164],allrows[170],allrows[176]]\n",
    "\n",
    "    column4_Views = [allrows[3],allrows[9],allrows[15],allrows[21],allrows[27],allrows[33],allrows[39],allrows[45],allrows[51],allrows[57],allrows[63],allrows[69],allrows[75],allrows[81],allrows[87],allrows[93],allrows[99],allrows[105],allrows[111],allrows[117],allrows[123],allrows[129],allrows[135],allrows[141],allrows[147],allrows[153],allrows[159],allrows[165],allrows[171],allrows[177]]\n",
    "\n",
    "    column5_Publication_Date = [allrows[4],allrows[10],allrows[16],allrows[22],allrows[28],allrows[34],allrows[40],allrows[46],allrows[52],allrows[58],allrows[64],allrows[70],allrows[76],allrows[82],allrows[88],allrows[94],allrows[100],allrows[106],allrows[112],allrows[118],allrows[124],allrows[130],allrows[136],allrows[142],allrows[148],allrows[154],allrows[160],allrows[166],allrows[172],allrows[178]]\n",
    "\n",
    "    #column5_Publication_Date\n",
    "    #we cheaking length of allcolumns wheathere it is equal in length\n",
    "    print(len(column1_Rank))\n",
    "    print(len(column2_Video_name))\n",
    "    print(len(column3_Uploader))\n",
    "    print(len(column4_Views))\n",
    "    print(len(column5_Publication_Date))\n",
    "    \n",
    "    \n",
    "except NoSuchElementException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dcfdc53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 30 most-viewed YouTube videos[3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Video_name</th>\n",
       "      <th>Uploader</th>\n",
       "      <th>Views</th>\n",
       "      <th>Publication_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[4]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>12.00</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[7]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.05</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[14]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>6.57</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[15]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>5.89</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[16]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>5.88</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[18]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>5.74</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[23]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>5.08</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Uptown Funk\"[24]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>4.79</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Wheels on the Bus\"[25]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>4.77</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[26]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>4.76</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[27]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>4.64</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[32]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.52</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[33]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>4.18</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Sugar\"[34]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.80</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Roar\"[35]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.70</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[36]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.70</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>3.67</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[38]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.62</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Thinking Out Loud\"[39]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.53</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[40]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>3.46</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Dark Horse\"[41]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.42</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[42]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.40</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Faded\"[43]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.38</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Let Her Go\"[44]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.36</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Girls Like You\"[45]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.35</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Perfect\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.33</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Bailando\"[47]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>3.31</td>\n",
       "      <td>April 11, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Lean On\"[48]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>3.31</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[49]</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.26</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[50]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.24</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                       Video_name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[4]   \n",
       "1    2.                                   \"Despacito\"[7]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
       "3    4.                                  \"Bath Song\"[15]   \n",
       "4    5.                               \"Shape of You\"[16]   \n",
       "5    6.                              \"See You Again\"[18]   \n",
       "6    7.                \"Phonics Song with Two Words\"[23]   \n",
       "7    8.                                \"Uptown Funk\"[24]   \n",
       "8    9.                          \"Wheels on the Bus\"[25]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[26]   \n",
       "10  11.                              \"Gangnam Style\"[27]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[32]   \n",
       "12  13.                             \"Dame Tu Cosita\"[33]   \n",
       "13  14.                                      \"Sugar\"[34]   \n",
       "14  15.                                       \"Roar\"[35]   \n",
       "15  16.                             \"Counting Stars\"[36]   \n",
       "16  17.                                     \"Axel F\"[37]   \n",
       "17  18.                                      \"Sorry\"[38]   \n",
       "18  19.                          \"Thinking Out Loud\"[39]   \n",
       "19  20.                        \"Baa Baa Black Sheep\"[40]   \n",
       "20  21.                                 \"Dark Horse\"[41]   \n",
       "21  22.           \"Waka Waka (This Time for Africa)\"[42]   \n",
       "22  23.                                      \"Faded\"[43]   \n",
       "23  24.                                 \"Let Her Go\"[44]   \n",
       "24  25.                             \"Girls Like You\"[45]   \n",
       "25  26.                                    \"Perfect\"[46]   \n",
       "26  27.                                   \"Bailando\"[47]   \n",
       "27  28.                                    \"Lean On\"[48]   \n",
       "28  29.          \"Humpty the train on a fruits ride\"[49]   \n",
       "29  30.                             \"Lakdi Ki Kathi\"[50]   \n",
       "\n",
       "                                         Uploader  Views   Publication_Date  \n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories  12.00      June 17, 2016  \n",
       "1                                      Luis Fonsi   8.05   January 12, 2017  \n",
       "2                                     LooLoo Kids   6.57    October 8, 2016  \n",
       "3                      Cocomelon – Nursery Rhymes   5.89        May 2, 2018  \n",
       "4                                      Ed Sheeran   5.88   January 30, 2017  \n",
       "5                                     Wiz Khalifa   5.74      April 6, 2015  \n",
       "6                                       ChuChu TV   5.08      March 6, 2014  \n",
       "7                                     Mark Ronson   4.79  November 19, 2014  \n",
       "8                      Cocomelon – Nursery Rhymes   4.77       May 24, 2018  \n",
       "9                                     Miroshka TV   4.76  February 27, 2018  \n",
       "10                                            Psy   4.64      July 15, 2012  \n",
       "11                                     Get Movies   4.52   January 31, 2012  \n",
       "12                                      El Chombo   4.18      April 5, 2018  \n",
       "13                                       Maroon 5   3.80   January 14, 2015  \n",
       "14                                     Katy Perry   3.70  September 5, 2013  \n",
       "15                                    OneRepublic   3.70       May 31, 2013  \n",
       "16                                     Crazy Frog   3.67      June 16, 2009  \n",
       "17                                  Justin Bieber   3.62   October 22, 2015  \n",
       "18                                     Ed Sheeran   3.53    October 7, 2014  \n",
       "19                     Cocomelon – Nursery Rhymes   3.46      June 25, 2018  \n",
       "20                                     Katy Perry   3.42  February 20, 2014  \n",
       "21                                        Shakira   3.40       June 4, 2010  \n",
       "22                                    Alan Walker   3.38   December 3, 2015  \n",
       "23                                      Passenger   3.36      July 25, 2012  \n",
       "24                                       Maroon 5   3.35       May 31, 2018  \n",
       "25                                     Ed Sheeran   3.33   November 9, 2017  \n",
       "26                               Enrique Iglesias   3.31     April 11, 2014  \n",
       "27                                    Major Lazer   3.31     March 22, 2015  \n",
       "28  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   3.26   January 26, 2018  \n",
       "29                                   Jingle Toons   3.24      June 14, 2018  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print title a dataframe\n",
    "titletags = driver.find_elements(By.XPATH,'/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/caption')\n",
    "for i in titletags:\n",
    "    title = i.text\n",
    "print(\"\\n\",title)\n",
    "#creat a dataframe\n",
    "df1 = pd.DataFrame({'Rank':column1_Rank,'Video_name':column2_Video_name,'Uploader':column3_Uploader,'Views':column4_Views,'Publication_Date':column5_Publication_Date})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b727b",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1\n",
    "st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f2fcd788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ae61a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "e4bbb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "dd3b0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #opening a bcci.tv page on automated chrome browser\n",
    "    driver.get(\"https://www.bcci.tv/\")\n",
    "    driver.maximize_window()\n",
    "    #for clicking on international option \n",
    "    option=driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a')\n",
    "    option.click()\n",
    "    Match_title = []\n",
    "    Series = []\n",
    "    Place = []\n",
    "    Match_Date = []\n",
    "    Match_Time = []\n",
    "    \n",
    "    allmatchestags = driver.find_elements(By.XPATH,'//div[@class=\"fixture-card-main col-lg-3 col-md-6 col-sm-12 ng-scope\"]')\n",
    "    for i in allmatchestags:\n",
    "        matchtags = driver.find_elements(By.XPATH,'//div[@class=\"fixture-card ng-scope border-orange\"]')\n",
    "        for j in matchtags:\n",
    "            match_titletag = driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]') \n",
    "            #for Scraping titles\n",
    "            for k in match_titletag:\n",
    "                title = k.text.replace(\" -\",\"\")\n",
    "                Match_title.append(title)\n",
    "            #for scraping series\n",
    "            seriestags = driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]')\n",
    "            for series in seriestags:\n",
    "                matchseries = series.text\n",
    "                Series.append(matchseries)\n",
    "            #for scraping place\n",
    "            placetags = driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]')\n",
    "            for venues in placetags:\n",
    "                venue = venues.text.replace(\",\",\"\")\n",
    "                Place.append(venue)\n",
    "            \n",
    "            #for scraping date\n",
    "            datetags = driver.find_elements(By.XPATH,'//h5[@class=\"ng-binding\"]')\n",
    "            for day in datetags:\n",
    "                tarikh = day.text\n",
    "                Match_Date.append(tarikh)\n",
    "            \n",
    "            #for scraping match timing\n",
    "            timetags = driver.find_elements(By.XPATH,'//h5[@class=\"text-right ng-binding\"]')\n",
    "            for m_time in timetags:\n",
    "                matchtime = m_time.text.replace(\" IST\",\"\")\n",
    "                Match_Time.append(matchtime)\n",
    "                \n",
    "                #time.sleep(2)\n",
    "except ElementNotInteractableException as e:\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "    driver.get(\"https://www.bcci.tv/international/fixtures\")\n",
    "    driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "600407e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 286 286 288 288\n"
     ]
    }
   ],
   "source": [
    "print(len(Match_title),len(Series),len(Place),len(Match_Date),len(Match_Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e34f7c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Match_Date</th>\n",
       "      <th>Match_time(in IST)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>31 DEC 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>2 JAN 2023</td>\n",
       "      <td>5:15 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Wankhede Stadium</td>\n",
       "      <td>3 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>4 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Maharashtra Cricket Association Stadium</td>\n",
       "      <td>5 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Saurashtra Cricket Association Stadium</td>\n",
       "      <td>7 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>10 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>12 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Wankhede Stadium</td>\n",
       "      <td>31 DEC 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>2 JAN 2023</td>\n",
       "      <td>5:15 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Maharashtra Cricket Association Stadium</td>\n",
       "      <td>3 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Saurashtra Cricket Association Stadium</td>\n",
       "      <td>4 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Barsapara Cricket Stadium</td>\n",
       "      <td>5 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Eden Gardens</td>\n",
       "      <td>7 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>10 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>12 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Wankhede Stadium</td>\n",
       "      <td>31 DEC 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>2 JAN 2023</td>\n",
       "      <td>5:15 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Maharashtra Cricket Association Stadium</td>\n",
       "      <td>3 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Saurashtra Cricket Association Stadium</td>\n",
       "      <td>4 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Barsapara Cricket Stadium</td>\n",
       "      <td>5 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Eden Gardens</td>\n",
       "      <td>7 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>10 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>12 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Wankhede Stadium</td>\n",
       "      <td>31 DEC 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19</td>\n",
       "      <td>Steyn City School Ground</td>\n",
       "      <td>2 JAN 2023</td>\n",
       "      <td>5:15 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Maharashtra Cricket Association Stadium</td>\n",
       "      <td>3 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA T20 SERIES 2022-23</td>\n",
       "      <td>Saurashtra Cricket Association Stadium</td>\n",
       "      <td>4 JAN 2023</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Barsapara Cricket Stadium</td>\n",
       "      <td>5 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Eden Gardens</td>\n",
       "      <td>7 JAN 2023</td>\n",
       "      <td>7:00 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Match Title                                          Series  \\\n",
       "0     3rd T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "1     4th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "2     1st T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "3     5th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "4     2nd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "5     3rd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "6     3rd T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "7     4th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "8     1st T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "9     5th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "10    2nd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "11    3rd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "12     1st ODI      SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "13     2nd ODI      SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "14    3rd T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "15    4th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "16    1st T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "17    5th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "18    2nd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "19    3rd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "20     1st ODI      SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "21     2nd ODI      SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "22    3rd T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "23    4th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "24    1st T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "25    5th T20I  INDIA WOMEN U19 TOUR OF SOUTH AFRICA WOMEN U19   \n",
       "26    2nd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "27    3rd T20I      SRI LANKA TOUR OF INDIA T20 SERIES 2022-23   \n",
       "28     1st ODI      SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "29     2nd ODI      SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "\n",
       "                                      Place   Match_Date Match_time(in IST)   \n",
       "0                  Steyn City School Ground  31 DEC 2022             1:30 PM  \n",
       "1                  Steyn City School Ground   2 JAN 2023             5:15 PM  \n",
       "2                          Wankhede Stadium   3 JAN 2023             7:00 PM  \n",
       "3                  Steyn City School Ground   4 JAN 2023             1:30 PM  \n",
       "4   Maharashtra Cricket Association Stadium   5 JAN 2023             7:00 PM  \n",
       "5    Saurashtra Cricket Association Stadium   7 JAN 2023             7:00 PM  \n",
       "6                  Steyn City School Ground  10 JAN 2023             1:30 PM  \n",
       "7                  Steyn City School Ground  12 JAN 2023             1:30 PM  \n",
       "8                          Wankhede Stadium  31 DEC 2022             1:30 PM  \n",
       "9                  Steyn City School Ground   2 JAN 2023             5:15 PM  \n",
       "10  Maharashtra Cricket Association Stadium   3 JAN 2023             7:00 PM  \n",
       "11   Saurashtra Cricket Association Stadium   4 JAN 2023             1:30 PM  \n",
       "12                Barsapara Cricket Stadium   5 JAN 2023             7:00 PM  \n",
       "13                             Eden Gardens   7 JAN 2023             7:00 PM  \n",
       "14                 Steyn City School Ground  10 JAN 2023             1:30 PM  \n",
       "15                 Steyn City School Ground  12 JAN 2023             1:30 PM  \n",
       "16                         Wankhede Stadium  31 DEC 2022             1:30 PM  \n",
       "17                 Steyn City School Ground   2 JAN 2023             5:15 PM  \n",
       "18  Maharashtra Cricket Association Stadium   3 JAN 2023             7:00 PM  \n",
       "19   Saurashtra Cricket Association Stadium   4 JAN 2023             1:30 PM  \n",
       "20                Barsapara Cricket Stadium   5 JAN 2023             7:00 PM  \n",
       "21                             Eden Gardens   7 JAN 2023             7:00 PM  \n",
       "22                 Steyn City School Ground  10 JAN 2023             1:30 PM  \n",
       "23                 Steyn City School Ground  12 JAN 2023             1:30 PM  \n",
       "24                         Wankhede Stadium  31 DEC 2022             1:30 PM  \n",
       "25                 Steyn City School Ground   2 JAN 2023             5:15 PM  \n",
       "26  Maharashtra Cricket Association Stadium   3 JAN 2023             7:00 PM  \n",
       "27   Saurashtra Cricket Association Stadium   4 JAN 2023             1:30 PM  \n",
       "28                Barsapara Cricket Stadium   5 JAN 2023             7:00 PM  \n",
       "29                             Eden Gardens   7 JAN 2023             7:00 PM  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for creating dataframe\n",
    "match_df = pd.DataFrame(data={'Match Title':Match_title[0:30],'Series':Series[0:30],'Place':Place[0:30],'Match_Date':Match_Date[0:30],'Match_time(in IST) ':Match_Time[0:30]})\n",
    "match_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381c358",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ec9b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c971281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e9475f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0786559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #first connect to the driver\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "    \n",
    "    #opening a guru99.com page on automated chrome browser\n",
    "    driver.get(\"https://www.guru99.com/\")\n",
    "    driver.maximize_window()\n",
    "    time.sleep(2)\n",
    "    optiontag = driver.find_element(By.XPATH,'/html/body/div[1]/header/div[1]/div/div/div/div/div/div/div/div[2]/div[1]/nav/div/ul/li[2]/a/span')\n",
    "    optiontag.click()\n",
    "    time.sleep(2)\n",
    "    selectselenium = driver.find_element(By.XPATH,'/html/body/div[1]/header/div[1]/div/div/div/div/div/div/div/div[2]/div[1]/nav/div/ul/li[2]/ul/li[12]/a')\n",
    "    selectselenium.click()\n",
    "    time.sleep(2)\n",
    "    #go to file handaling option and click on that\n",
    "    fh = driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[1]')\n",
    "    fh.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #scroll down\n",
    "    driver.execute_script(\"window.scrollBy(0,4200)\")\n",
    "    #scraping of top heading\n",
    "    titletop = driver.find_element(By.XPATH,'//h2[@id=\"types-of-exceptions-in-selenium-webdriver\"]')\n",
    "    top = titletop.text\n",
    "    #scraping all text content of the page\n",
    "    heading = []\n",
    "    allcontent = driver.find_elements(By.XPATH,'//*[@class=\"entry-content single-content\"]//p')\n",
    "    for i in allcontent:\n",
    "        allheading = driver.find_elements(By.XPATH,'//*[@class=\"entry-content single-content\"]//p')\n",
    "        for i in allheading:\n",
    "            text = i.text\n",
    "            heading.append(text)\n",
    "    \n",
    "    #slicing the required data\n",
    "    newheading = heading[2:43]\n",
    "    newheading\n",
    "    \n",
    "    sr_no = [newheading[0][0],newheading[1][0],newheading[2][0],newheading[3][0],newheading[4][0],newheading[5][0],newheading[6][0],newheading[7][0],newheading[8][0],newheading[9][:2],newheading[10][:2],newheading[11][:2],newheading[12][:2],newheading[13][:2],newheading[14][:2],newheading[15][:2],newheading[16][:2],newheading[17][:2],newheading[18][:2],newheading[19][:2],newheading[20][:2],newheading[21][:2],newheading[22][:2],newheading[23][:2],newheading[24][:2],newheading[25][:2],newheading[26][:2],newheading[27][:2],newheading[28][:2],newheading[29][:2],newheading[30][:2],newheading[31][:2],newheading[32][:2],newheading[33][:2],newheading[34][:2],newheading[35][:2],newheading[36][:2],newheading[37][:2],newheading[38][:2],newheading[39][:2],newheading[40][:2]]\n",
    "    #sr_no\n",
    "    name = [newheading[0][3:30],newheading[1][3:33],newheading[2][3:26],newheading[3][3:24],newheading[4][3:26],newheading[5][3:26],newheading[6][3:34],newheading[7][3:28],newheading[8][3:21],newheading[9][3:23],newheading[10][3:30],newheading[11][3:37],newheading[12][3:36],newheading[13][3:30],newheading[14][3:38],newheading[15][3:33],newheading[16][3:30],newheading[17][3:33],newheading[18][3:29],newheading[19][3:33],newheading[20][3:32],newheading[21][3:33],newheading[22][3:30],newheading[23][3:35],newheading[24][3:25],newheading[25][3:19],newheading[26][3:29],newheading[27][3:35],newheading[28][3:28],newheading[23][3:34],newheading[30][3:23],newheading[31][3:32],newheading[32][3:25],newheading[33][3:31],newheading[38][3:28],newheading[35][3:32],newheading[36][3:28],newheading[37][3:37],newheading[38][3:28],newheading[39][3:33],newheading[40][3:33]]\n",
    "    description = [newheading[0][31:],newheading[1][34:],newheading[2][27:],newheading[3][25:],newheading[4][27:],newheading[5][26:],newheading[6][34:],newheading[7][29:],newheading[8][21:],newheading[9][24:],newheading[10][31:],newheading[11][38:],newheading[12][37:],newheading[13][30:],newheading[14][41:],newheading[15][34:],newheading[16][29:],newheading[17][33:],newheading[18][30:],newheading[19][34:],newheading[20][33:],newheading[21][34:],newheading[22][31:],newheading[23][36:],newheading[24][25:],newheading[25][19:],newheading[26][30:],newheading[27][36:],newheading[28][28:],newheading[29][27:],newheading[30][23:],newheading[31][33:],newheading[32][25:],newheading[33][32:],newheading[34][32:],newheading[35][32:],newheading[36][29:],newheading[37][37:],newheading[38][28:],newheading[39][33:],newheading[40][33:]]\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d65edf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Exceptions in Selenium Webdriver\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr.No.</th>\n",
       "      <th>Exception_Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ElementNotVisibleException:</td>\n",
       "      <td>This type of Selenium exception occurs when an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ElementNotSelectableException:</td>\n",
       "      <td>This Selenium exception occurs when an element...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NoSuchElementException:</td>\n",
       "      <td>This Exception occurs if an element could not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NoSuchFrameException:</td>\n",
       "      <td>This Exception occurs if the frame target to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NoAlertPresentException</td>\n",
       "      <td>This Exception occurs when you switch to no p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NoSuchWindowException:</td>\n",
       "      <td>This Exception occurs if the window target to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>StaleElementReferenceException:</td>\n",
       "      <td>This Selenium exception occurs happens when t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>SessionNotFoundException:</td>\n",
       "      <td>The WebDriver is acting after you quit the bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>TimeoutException:</td>\n",
       "      <td>Thrown when there is not enough time for a com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>WebDriverException:</td>\n",
       "      <td>This Exception takes place when the WebDriver ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>ConnectionClosedException:</td>\n",
       "      <td>This type of Exception takes place when there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>ElementClickInterceptedException:</td>\n",
       "      <td>The command may not be completed as the elemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>ElementNotInteractableException:</td>\n",
       "      <td>This Selenium exception is thrown when any ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>ErrorInResponseException:</td>\n",
       "      <td>This happens while interacting with the Firefo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>ErrorHandler.UnknownServerExceptio</td>\n",
       "      <td>Exception is used as a placeholder in case if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>ImeActivationFailedException:</td>\n",
       "      <td>This expectation will occur when IME engine ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>ImeNotAvailableException:</td>\n",
       "      <td>It takes place when IME support is unavailable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>InsecureCertificateException:</td>\n",
       "      <td>Navigation made the user agent to hit a certi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>InvalidArgumentException:</td>\n",
       "      <td>It occurs when an argument does not belong to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>InvalidCookieDomainException:</td>\n",
       "      <td>This happens when you try to add a cookie unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>InvalidCoordinatesException:</td>\n",
       "      <td>This type of Exception matches an interacting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>InvalidElementStateException:</td>\n",
       "      <td>It occurs when command can’t be finished when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>InvalidSessionIdException:</td>\n",
       "      <td>This Exception took place when the given sessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>InvalidSwitchToTargetException:</td>\n",
       "      <td>This occurs when the frame or window target to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>JavascriptException:</td>\n",
       "      <td>This issue occurs while executing JavaScript g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>JsonException:</td>\n",
       "      <td>It occurs when you afford to get the session w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>NoSuchAttributeException:</td>\n",
       "      <td>This kind of Exception occurs when the attribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>MoveTargetOutOfBoundsException:</td>\n",
       "      <td>It takes place if the target provided to the A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>NoSuchContextException:</td>\n",
       "      <td>ContextAware does mobile device testing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>InvalidSwitchToTargetException</td>\n",
       "      <td>This Exception occurs when no cookie matching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>NotFoundException:</td>\n",
       "      <td>This Exception is a subclass of WebDriverExcep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>RemoteDriverServerException:</td>\n",
       "      <td>This Selenium exception is thrown when the ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>ScreenshotException:</td>\n",
       "      <td>It is not possible to capture a screen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>SessionNotCreatedException:</td>\n",
       "      <td>It happens when a new session could not be suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>UnknownMethodException:</td>\n",
       "      <td>This occurs if a driver is unable to set a coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>UnexpectedTagNameException:</td>\n",
       "      <td>Happens if a support class did not get a web e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>UnhandledAlertException:</td>\n",
       "      <td>This expectation occurs when there is an alert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>UnexpectedAlertPresentException:</td>\n",
       "      <td>It occurs when there is the appearance of an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>UnknownMethodException:</td>\n",
       "      <td>This Exception happens when the requested comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>UnreachableBrowserException:</td>\n",
       "      <td>This Exception occurs only when the browser is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>UnsupportedCommandException:</td>\n",
       "      <td>This occurs when remote WebDriver doesn’t send...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr.No.                       Exception_Name  \\\n",
       "0       1          ElementNotVisibleException:   \n",
       "1       2       ElementNotSelectableException:   \n",
       "2       3              NoSuchElementException:   \n",
       "3       4                NoSuchFrameException:   \n",
       "4       5              NoAlertPresentException   \n",
       "5       6              NoSuchWindowException:    \n",
       "6       7      StaleElementReferenceException:   \n",
       "7       8            SessionNotFoundException:   \n",
       "8       9                   TimeoutException:    \n",
       "9      10                  WebDriverException:   \n",
       "10     11           ConnectionClosedException:   \n",
       "11     12    ElementClickInterceptedException:   \n",
       "12     13     ElementNotInteractableException:   \n",
       "13     14           ErrorInResponseException:    \n",
       "14     15   ErrorHandler.UnknownServerExceptio   \n",
       "15     16        ImeActivationFailedException:   \n",
       "16     17           ImeNotAvailableException:    \n",
       "17     18        InsecureCertificateException:   \n",
       "18     19            InvalidArgumentException:   \n",
       "19     20        InvalidCookieDomainException:   \n",
       "20     21         InvalidCoordinatesException:   \n",
       "21     22        InvalidElementStateException:   \n",
       "22     23           InvalidSessionIdException:   \n",
       "23     24      InvalidSwitchToTargetException:   \n",
       "24     25                JavascriptException:    \n",
       "25     26                      JsonException:    \n",
       "26     27            NoSuchAttributeException:   \n",
       "27     28      MoveTargetOutOfBoundsException:   \n",
       "28     29             NoSuchContextException:    \n",
       "29     30       InvalidSwitchToTargetException   \n",
       "30     31                  NotFoundException:    \n",
       "31     32         RemoteDriverServerException:   \n",
       "32     33                ScreenshotException:    \n",
       "33     34          SessionNotCreatedException:   \n",
       "34     35             UnknownMethodException:    \n",
       "35     36         UnexpectedTagNameException:    \n",
       "36     37             UnhandledAlertException:   \n",
       "37     38    UnexpectedAlertPresentException:    \n",
       "38     39             UnknownMethodException:    \n",
       "39     40        UnreachableBrowserException:    \n",
       "40     41        UnsupportedCommandException:    \n",
       "\n",
       "                                          Description  \n",
       "0   This type of Selenium exception occurs when an...  \n",
       "1   This Selenium exception occurs when an element...  \n",
       "2   This Exception occurs if an element could not ...  \n",
       "3   This Exception occurs if the frame target to b...  \n",
       "4    This Exception occurs when you switch to no p...  \n",
       "5   This Exception occurs if the window target to ...  \n",
       "6    This Selenium exception occurs happens when t...  \n",
       "7   The WebDriver is acting after you quit the bro...  \n",
       "8   Thrown when there is not enough time for a com...  \n",
       "9   This Exception takes place when the WebDriver ...  \n",
       "10  This type of Exception takes place when there ...  \n",
       "11  The command may not be completed as the elemen...  \n",
       "12  This Selenium exception is thrown when any ele...  \n",
       "13  This happens while interacting with the Firefo...  \n",
       "14  Exception is used as a placeholder in case if ...  \n",
       "15  This expectation will occur when IME engine ac...  \n",
       "16    It takes place when IME support is unavailable.  \n",
       "17   Navigation made the user agent to hit a certi...  \n",
       "18  It occurs when an argument does not belong to ...  \n",
       "19  This happens when you try to add a cookie unde...  \n",
       "20  This type of Exception matches an interacting ...  \n",
       "21  It occurs when command can’t be finished when ...  \n",
       "22  This Exception took place when the given sessi...  \n",
       "23  This occurs when the frame or window target to...  \n",
       "24  This issue occurs while executing JavaScript g...  \n",
       "25  It occurs when you afford to get the session w...  \n",
       "26  This kind of Exception occurs when the attribu...  \n",
       "27  It takes place if the target provided to the A...  \n",
       "28           ContextAware does mobile device testing.  \n",
       "29  This Exception occurs when no cookie matching ...  \n",
       "30  This Exception is a subclass of WebDriverExcep...  \n",
       "31  This Selenium exception is thrown when the ser...  \n",
       "32            It is not possible to capture a screen.  \n",
       "33  It happens when a new session could not be suc...  \n",
       "34  This occurs if a driver is unable to set a coo...  \n",
       "35  Happens if a support class did not get a web e...  \n",
       "36  This expectation occurs when there is an alert...  \n",
       "37  It occurs when there is the appearance of an u...  \n",
       "38  This Exception happens when the requested comm...  \n",
       "39  This Exception occurs only when the browser is...  \n",
       "40  This occurs when remote WebDriver doesn’t send...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(top)\n",
    "df_filehandling = pd.DataFrame({'Sr.No.':sr_no,'Exception_Name':name,'Description':description})\n",
    "df_filehandling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d72463",
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = []\n",
    "allcontent = driver.find_elements(By.XPATH,'//*[@class=\"entry-content single-content\"]//p')\n",
    "for i in allcontent:\n",
    "    allheading = driver.find_elements(By.XPATH,'//*[@class=\"entry-content single-content\"]//p')\n",
    "    for i in allheading:\n",
    "        text = i.text\n",
    "        heading.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing the required data\n",
    "newheading = heading[2:43]\n",
    "newheading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d91eaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. ElementNotVisibleException: This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newheading[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f16f5453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ElementNotSelectableException: This Selenium exception occurs when an element is presented in the DOM, but you can be able to select. Therefore, it is not possible to interact.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newheading[0][3:40]\n",
    "newheading[1][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff4028ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_no = [newheading[0][0],newheading[1][0],newheading[2][0],newheading[3][0],newheading[4][0],newheading[5][0],newheading[6][0],newheading[7][0],newheading[8][0],newheading[9][:2],newheading[10][:2],newheading[11][:2],newheading[12][:2],newheading[13][:2],newheading[14][:2],newheading[15][:2],newheading[16][:2],newheading[17][:2],newheading[18][:2],newheading[19][:2],newheading[20][:2],newheading[21][:2],newheading[22][:2],newheading[23][:2],newheading[24][:2],newheading[25][:2],newheading[26][:2],newheading[27][:2],newheading[28][:2],newheading[29][:2],newheading[30][:2],newheading[31][:2],newheading[32][:2],newheading[33][:2],newheading[34][:2],newheading[35][:2],newheading[36][:2],newheading[37][:2],newheading[38][:2],newheading[39][:2],newheading[40][:2]]\n",
    "#sr_no\n",
    "name = [newheading[0][3:30],newheading[1][3:33],newheading[2][3:26],newheading[3][3:24],newheading[4][3:26],newheading[5][3:26],newheading[6][3:34],newheading[7][3:28],newheading[8][3:21],newheading[9][3:23],newheading[10][3:30],newheading[11][3:37],newheading[12][3:36],newheading[13][3:30],newheading[14][3:38],newheading[15][3:33],newheading[16][3:30],newheading[17][3:33],newheading[18][3:29],newheading[19][3:33],newheading[20][3:32],newheading[21][3:33],newheading[22][3:30],newheading[23][3:35],newheading[24][3:25],newheading[25][3:19],newheading[26][3:29],newheading[27][3:35],newheading[28][3:28],newheading[23][3:34],newheading[30][3:23],newheading[31][3:32],newheading[32][3:25],newheading[33][3:31],newheading[38][3:28],newheading[35][3:32],newheading[36][3:28],newheading[37][3:37],newheading[38][3:28],newheading[39][3:33],newheading[40][3:33]]\n",
    "#name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "918e3456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "description = [[newheading[0][31:],newheading[1][34:],newheading[2][27:],newheading[3][25:],newheading[4][27:],newheading[5][26:],newheading[6][34:],newheading[7][29:],newheading[8][21:],newheading[9][24:],newheading[10][31:],newheading[11][38:],newheading[12][37:],newheading[13][30:],newheading[14][41:],newheading[15][34:],newheading[16][29:],newheading[17][33:],newheading[18][30:],newheading[19][34:],newheading[20][33:],newheading[21][34:],newheading[22][31:],newheading[23][36:],newheading[24][25:],newheading[25][19:],newheading[26][30:],newheading[27][36:],newheading[28][28:],newheading[29][27:],newheading[30][23:],newheading[31][33:],newheading[32][25:],newheading[33][32:],newheading[34][32:],newheading[35][32:],newheading[36][29:],newheading[37][37:],newheading[38][28:],newheading[39][33:],newheading[40][33:]]]\n",
    "#description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b52e80",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a141f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "36767028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "70c7ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8f10dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #opening a statisticstimes page on automated chrome browser\n",
    "    driver.get(\"http://statisticstimes.com/\")\n",
    "    driver.maximize_window()\n",
    "    time.sleep(2)\n",
    "    #select - Economy\n",
    "    optiontag = driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]/button')\n",
    "    optiontag.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #select sub_option- India\n",
    "    suboption = driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]')\n",
    "    suboption.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #select option GDP of Indian states\n",
    "    suboption = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "    suboption.click()\n",
    "    \n",
    "    #scroll down\n",
    "    driver.execute_script(\"window.scrollBy(0,1900)\")\n",
    "\n",
    "    leadge_tabletags = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]')\n",
    "    for i in leadge_tabletags:\n",
    "        allrowstags = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tr')\n",
    "        allrows_col1= []\n",
    "        allrows_col2= []\n",
    "        allrows_col3= []\n",
    "        allrows_col4= []\n",
    "        allrows_col5= []\n",
    "        allrows_col6= []\n",
    "        for rows in allrowstags[2:]:\n",
    "            #for scraping Rank\n",
    "            column1 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[1]')\n",
    "            for c1 in column1:\n",
    "                row = c1.text\n",
    "                allrows_col1.append(row[0:2])\n",
    "            \n",
    "            #for scraping State\n",
    "            column2 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[2]')\n",
    "            for c2 in column2:\n",
    "                row = c2.text\n",
    "                allrows_col2.append(row)\n",
    "            \n",
    "            #for scraping GSDP (Cr INR at Current prices)19-20\n",
    "            column3 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[3]')\n",
    "            for c3 in column3:\n",
    "                row = c3.text\n",
    "                allrows_col3.append(row)\n",
    "                \n",
    "            #for scraping GSDP (Cr INR at Current prices)18-19\n",
    "            column4 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[4]')\n",
    "            for c4 in column4:\n",
    "                row = c4.text\n",
    "                allrows_col4.append(row)    \n",
    "                \n",
    "            #for scraping Share(Cr INR at Current prices)18-19\n",
    "            column5 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[5]')\n",
    "            for c5 in column5:\n",
    "                row = c5.text\n",
    "                allrows_col5.append(row)    \n",
    "                \n",
    "            #for scraping GDP ($billion)\n",
    "            column6 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[6]')\n",
    "            for c6 in column6:\n",
    "                row = c6.text\n",
    "                allrows_col6.append(row)   \n",
    "    \n",
    "except NoSuchElementException as e:\n",
    "    #close ad raise and thrrow exception\n",
    "    #exception is handel we close the ad window \n",
    "    driver.get(\"https://www.statisticstimes.com/#google_vignette\")\n",
    "    ad = driver.find_element(By.XPATH,'//span[@class=\"ns-l1avi-e-16\"]')\n",
    "    ad.click()\n",
    "    time.sleep(3)\n",
    "    optiontag.click()\n",
    "    suboption.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "079c92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll down\n",
    "driver.execute_script(\"window.scrollBy(0,1900)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "76277883",
   "metadata": {},
   "outputs": [],
   "source": [
    "leadge_tabletags = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]')\n",
    "for i in leadge_tabletags:\n",
    "    allrowstags = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tr')\n",
    "    allrows_col1= []\n",
    "    allrows_col2= []\n",
    "    allrows_col3= []\n",
    "    allrows_col4= []\n",
    "    allrows_col5= []\n",
    "    allrows_col6= []\n",
    "    for rows in allrowstags[2:]:\n",
    "            #for scraping Rank\n",
    "            column1 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[1]')\n",
    "            for c1 in column1:\n",
    "                row = c1.text\n",
    "                allrows_col1.append(row[0:2])\n",
    "            \n",
    "            #for scraping State\n",
    "            column2 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[2]')\n",
    "            for c2 in column2:\n",
    "                row = c2.text\n",
    "                allrows_col2.append(row)\n",
    "            \n",
    "            #for scraping GSDP (Cr INR at Current prices)19-20\n",
    "            column3 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[3]')\n",
    "            for c3 in column3:\n",
    "                row = c3.text\n",
    "                allrows_col3.append(row)\n",
    "                \n",
    "            #for scraping GSDP (Cr INR at Current prices)18-19\n",
    "            column4 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[4]')\n",
    "            for c4 in column4:\n",
    "                row = c4.text\n",
    "                allrows_col4.append(row)    \n",
    "                \n",
    "            #for scraping Share(Cr INR at Current prices)18-19\n",
    "            column5 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[5]')\n",
    "            for c5 in column5:\n",
    "                row = c5.text\n",
    "                allrows_col5.append(row)    \n",
    "                \n",
    "            #for scraping GDP ($billion)\n",
    "            column6 = driver.find_elements(By.XPATH,'/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr/td[6]')\n",
    "            for c6 in column6:\n",
    "                row = c6.text\n",
    "                allrows_col6.append(row)       \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "488db98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122 1122 1122 1122 1122 1122\n"
     ]
    }
   ],
   "source": [
    "print(len(allrows_col1),len(allrows_col2),len(allrows_col3),len(allrows_col4),len(allrows_col5),len(allrows_col6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e4162862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr.No.</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(CrINRatCurrentprices)-(19-20)</th>\n",
       "      <th>GSDP(CrINRatCurrentprices)-(18-19)</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($billion)2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr.No.                      State GSDP(CrINRatCurrentprices)-(19-20)  \\\n",
       "0       1                Maharashtra                                  -   \n",
       "1       2                 Tamil Nadu                          1,845,853   \n",
       "2       3              Uttar Pradesh                          1,687,818   \n",
       "3       4                    Gujarat                                  -   \n",
       "4       5                  Karnataka                          1,631,977   \n",
       "5       6                West Bengal                          1,253,832   \n",
       "6       7                  Rajasthan                          1,020,989   \n",
       "7       8             Andhra Pradesh                            972,782   \n",
       "8       9                  Telangana                            969,604   \n",
       "9      10             Madhya Pradesh                            906,672   \n",
       "10     11                     Kerala                                  -   \n",
       "11     12                      Delhi                            856,112   \n",
       "12     13                    Haryana                            831,610   \n",
       "13     14                      Bihar                            611,804   \n",
       "14     15                     Punjab                            574,760   \n",
       "15     16                     Odisha                            521,275   \n",
       "16     17                      Assam                                  -   \n",
       "17     18               Chhattisgarh                            329,180   \n",
       "18     19                  Jharkhand                            328,598   \n",
       "19     20                Uttarakhand                                  -   \n",
       "20     21            Jammu & Kashmir                                  -   \n",
       "21     22           Himachal Pradesh                            165,472   \n",
       "22     23                        Goa                             80,449   \n",
       "23     24                    Tripura                             55,984   \n",
       "24     25                 Chandigarh                                  -   \n",
       "25     26                 Puducherry                             38,253   \n",
       "26     27                  Meghalaya                             36,572   \n",
       "27     28                     Sikkim                             32,496   \n",
       "28     29                    Manipur                             31,790   \n",
       "29     30                   Nagaland                                  -   \n",
       "30     31          Arunachal Pradesh                                  -   \n",
       "31     32                    Mizoram                             26,503   \n",
       "32     33  Andaman & Nicobar Islands                                  -   \n",
       "\n",
       "   GSDP(CrINRatCurrentprices)-(18-19) Share(18-19) GDP($billion)2019  \n",
       "0                           2,632,792       13.94%           399.921  \n",
       "1                           1,630,208        8.63%           247.629  \n",
       "2                           1,584,764        8.39%           240.726  \n",
       "3                           1,502,899        7.96%           228.290  \n",
       "4                           1,493,127        7.91%           226.806  \n",
       "5                           1,089,898        5.77%           165.556  \n",
       "6                             942,586        4.99%           143.179  \n",
       "7                             862,957        4.57%           131.083  \n",
       "8                             861,031        4.56%           130.791  \n",
       "9                             809,592        4.29%           122.977  \n",
       "10                            781,653        4.14%           118.733  \n",
       "11                            774,870        4.10%           117.703  \n",
       "12                            734,163        3.89%           111.519  \n",
       "13                            530,363        2.81%            80.562  \n",
       "14                            526,376        2.79%            79.957  \n",
       "15                            487,805        2.58%            74.098  \n",
       "16                            315,881        1.67%            47.982  \n",
       "17                            304,063        1.61%            46.187  \n",
       "18                            297,204        1.57%            45.145  \n",
       "19                            245,895        1.30%            37.351  \n",
       "20                            155,956        0.83%            23.690  \n",
       "21                            153,845        0.81%            23.369  \n",
       "22                             73,170        0.39%            11.115  \n",
       "23                             49,845        0.26%             7.571  \n",
       "24                             42,114        0.22%             6.397  \n",
       "25                             34,433        0.18%             5.230  \n",
       "26                             33,481        0.18%             5.086  \n",
       "27                             28,723        0.15%             4.363  \n",
       "28                             27,870        0.15%             4.233  \n",
       "29                             27,283        0.14%             4.144  \n",
       "30                             24,603        0.13%             3.737  \n",
       "31                             22,287        0.12%             3.385  \n",
       "32                                  -            -                 -  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_IndiaGDPStatewise = pd.DataFrame({\"Sr.No.\":allrows_col1[0:33],\"State\":allrows_col2[0:33],\"GSDP(CrINRatCurrentprices)-(19-20)\":allrows_col3[0:33],\"GSDP(CrINRatCurrentprices)-(18-19)\":allrows_col4[0:33],\"Share(18-19)\":allrows_col5[0:33],\"GDP($billion)2019\":allrows_col6[0:33]})\n",
    "df_IndiaGDPStatewise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21989e2",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "ASSIGNMENT\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f491bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6ddce6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7f9711b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "beeb904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #opening a github page on automated chrome browser\n",
    "    driver.get(\"https://github.com/\")\n",
    "    driver.maximize_window()\n",
    "    time.sleep(4)\n",
    "    #select - Open Source\n",
    "    optiontag = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button')\n",
    "    optiontag.click()\n",
    "    time.sleep(3)\n",
    "    #select - Trending option under repositories\n",
    "    suboptiontag = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/ul[3]/li[3]/a')\n",
    "    suboptiontag.click()\n",
    "    time.sleep(3)\n",
    "    #scroll down\n",
    "    driver.execute_script(\"window.scrollBy(0,300)\")\n",
    "    \n",
    "    # allrepostags object is created as \n",
    "    leadgetags = driver.find_elements(By.XPATH,'/html/body/div[1]/div[4]/main/div[3]/div/div[2]')\n",
    "    for repo in leadgetags:\n",
    "        #for scraching repo title\n",
    "        Repo_titletags = driver.find_elements(By.XPATH,'//h1[@class=\"h3 lh-condensed\"]')\n",
    "        Repo_Name = []\n",
    "        for name in Repo_titletags:\n",
    "            #Repo_URL.append(name.get_attribute(\"href\"))\n",
    "            reponame = name.text\n",
    "            Repo_Name.append(reponame)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #for scraching repoDescription\n",
    "    Repo_desctags = driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "    Repo_Description = []\n",
    "    for desc in Repo_desctags:\n",
    "        descptn = desc.text\n",
    "        Repo_Description.append(descptn)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #for scraping of languages used for project development    \n",
    "    lang_tags = driver.find_elements(By.XPATH,'//span[@class=\"d-inline-block ml-0 mr-3\"]')\n",
    "    Language_Used = []\n",
    "    for i in lang_tags:\n",
    "        lang = i.text\n",
    "        Language_Used.append(lang)    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    print(len(Language_Used))\n",
    "    print(len(Repo_Description))\n",
    "    print(len(Repo_Name))\n",
    "\n",
    "except NoSuchException as e:\n",
    "    Language_Used.append('-') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1efac006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for scraping all Repositories urls\n",
    "pagetags = driver.find_elements(By.XPATH,'//div[@class=\"Box\"]')\n",
    "Repo_URLS = []\n",
    "for page in pagetags: #for loop for scrapping 3 page\n",
    "    url = driver.find_elements(By.XPATH,'//h1[@class=\"h3 lh-condensed\"]//a')\n",
    "    for i in url:\n",
    "        Repo_URLS.append(i.get_attribute(\"href\"))\n",
    "time.sleep(2)\n",
    "len(Repo_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f1d09227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42']\n",
      "['42', '2']\n",
      "['42', '2', '3']\n",
      "['42', '2', '3', '192']\n",
      "['42', '2', '3', '192', '777']\n",
      "['42', '2', '3', '192', '777', 'Used by 4']\n",
      "['42', '2', '3', '192', '777', 'Used by 4', 'Packages']\n",
      "['42', '2', '3', '192', '777', 'Used by 4', 'Packages', '11']\n",
      "['42', '2', '3', '192', '777', 'Used by 4', 'Packages', '11', '76']\n",
      "['42', '2', '3', '192', '777', 'Used by 4', 'Packages', '11', '76', '3']\n"
     ]
    }
   ],
   "source": [
    "#for scraping contributors count \n",
    "Contri = []\n",
    "for r_url in Repo_URLS[0:10]:\n",
    "    driver.get(r_url)\n",
    "    time.sleep(9)\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollBy(0,700)\")\n",
    "        time.sleep(7)\n",
    "        leadge = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[4]/div/h2/a')\n",
    "        count = leadge.text.replace(\"Contributors \",\"\")\n",
    "        Contri.append(count)\n",
    "        print(Contri)\n",
    "       \n",
    "    except NoSuchElementException as e:\n",
    "        Contri.append('-')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "34dd7421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['42', '2', '3', '192', '777', '15', '141', '11', '76', '3']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#error values correction\n",
    "Contri[5]=\"15\"\n",
    "Contri[6]=\"141\"\n",
    "#contributors List\n",
    "Contri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d5903a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository_Name</th>\n",
       "      <th>Repo_Description</th>\n",
       "      <th>Language_Used</th>\n",
       "      <th>Contrunutors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAION-AI / Open-Assistant</td>\n",
       "      <td>OpenAssistant is a chat-based assistant that u...</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>karpathy / nanoGPT</td>\n",
       "      <td>The simplest, fastest repository for training/...</td>\n",
       "      <td>Python</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sczhou / CodeFormer</td>\n",
       "      <td>[NeurIPS 2022] Towards Robust Blind Face Resto...</td>\n",
       "      <td>Python</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DarkFlippers / unleashed-firmware</td>\n",
       "      <td>Flipper Zero Unleashed Firmware</td>\n",
       "      <td>C</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ytdl-org / youtube-dl</td>\n",
       "      <td>Command-line program to download videos from Y...</td>\n",
       "      <td>Python</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>google / osv-scanner</td>\n",
       "      <td>Vulnerability scanner written in Go which uses...</td>\n",
       "      <td>Go</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pola-rs / polars</td>\n",
       "      <td>Fast multi-threaded, hybrid-streaming DataFram...</td>\n",
       "      <td>Rust</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neonbjb / tortoise-tts</td>\n",
       "      <td>A multi-voice TTS system trained with an empha...</td>\n",
       "      <td>Python</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Developer-Y / cs-video-courses</td>\n",
       "      <td>List of Computer Science courses with video le...</td>\n",
       "      <td>Python</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dair-ai / Prompt-Engineering-Guide</td>\n",
       "      <td>🐙 Guide and resources for prompt engineering</td>\n",
       "      <td>C++</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Repository_Name  \\\n",
       "0           LAION-AI / Open-Assistant   \n",
       "1                  karpathy / nanoGPT   \n",
       "2                 sczhou / CodeFormer   \n",
       "3   DarkFlippers / unleashed-firmware   \n",
       "4               ytdl-org / youtube-dl   \n",
       "5                google / osv-scanner   \n",
       "6                    pola-rs / polars   \n",
       "7              neonbjb / tortoise-tts   \n",
       "8      Developer-Y / cs-video-courses   \n",
       "9  dair-ai / Prompt-Engineering-Guide   \n",
       "\n",
       "                                    Repo_Description     Language_Used  \\\n",
       "0  OpenAssistant is a chat-based assistant that u...  Jupyter Notebook   \n",
       "1  The simplest, fastest repository for training/...            Python   \n",
       "2  [NeurIPS 2022] Towards Robust Blind Face Resto...            Python   \n",
       "3                    Flipper Zero Unleashed Firmware                 C   \n",
       "4  Command-line program to download videos from Y...            Python   \n",
       "5  Vulnerability scanner written in Go which uses...                Go   \n",
       "6  Fast multi-threaded, hybrid-streaming DataFram...              Rust   \n",
       "7  A multi-voice TTS system trained with an empha...            Python   \n",
       "8  List of Computer Science courses with video le...            Python   \n",
       "9       🐙 Guide and resources for prompt engineering               C++   \n",
       "\n",
       "  Contrunutors  \n",
       "0           42  \n",
       "1            2  \n",
       "2            3  \n",
       "3          192  \n",
       "4          777  \n",
       "5           15  \n",
       "6          141  \n",
       "7           11  \n",
       "8           76  \n",
       "9            3  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repo = pd.DataFrame({\"Repository_Name\":Repo_Name[0:10],\"Repo_Description\":Repo_Description[0:10],\"Language_Used\":Language_Used[0:10],\"Contrunutors\":Contri[0:10]})\n",
    "df_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c9b5d",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b297e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7bfda917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a158cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dd3de807",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #opening a www.billboard.com on automated chrome browser\n",
    "    driver.get(\"http://www.billboard.com/\")\n",
    "    driver.maximize_window()\n",
    "    #time.sleep(4)\n",
    "    \n",
    "    #select - Chart option \n",
    "    optiontag = driver.find_element(By.XPATH,'/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "    optiontag.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #then select - BillboardHot100 option(insideChart) \n",
    "    optiontag = driver.find_element(By.XPATH,'/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div/div[2]/span/a')\n",
    "    optiontag.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "except NoSuchElemnetException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2c95f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll down\n",
    "driver.execute_script(\"window.scrollBy(0,1000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1b76e41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Billboard Hot 100'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scraping title BillBoard Hot 100\n",
    "headingtag = driver.find_elements(By.XPATH,'//div[@class=\"lrv-u-flex-grow-1 a-heading-border a-heading-border-after-remove@mobile-max a-heading-border-height-0875 a-heading-border-background-color-white\"]//h2')\n",
    "for title in headingtag:\n",
    "    title1 = title.text\n",
    "title1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d0db9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weeks = []\n",
    "Peakrank = []\n",
    "Last_week_rank = []\n",
    "Song_Name = []\n",
    "Artist_Name = []\n",
    "alldeatailstags = driver.find_elements(By.XPATH,'//div[@class=\"chart-results-list // lrv-u-padding-t-150 lrv-u-padding-t-050@mobile-max\"]')\n",
    "for i in alldeatailstags:\n",
    "    all_songnametags = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]//li//h3')\n",
    "    #for scraping song name\n",
    "    for j in all_songnametags:\n",
    "        songname = j.text\n",
    "        Song_Name.append(songname)\n",
    "    \n",
    "    #for scraping artist\n",
    "    all_artistnametags = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]//li[1]//span')\n",
    "    for j in all_artistnametags:\n",
    "        singername = j.text\n",
    "        Artist_Name.append(singername)\n",
    "        \n",
    "   #for scraping last week rank\n",
    "    Last_week_rankstags = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]//li[4]//span')\n",
    "    for j in Last_week_rankstags:\n",
    "        lst_weekrank = j.text\n",
    "        Last_week_rank.append(lst_weekrank)\n",
    "    \n",
    "    #for scraping Peak rank\n",
    "    Peakrank_tags = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]//li[5]//span')\n",
    "    for j in Peakrank_tags:\n",
    "        p_rank = j.text\n",
    "        Peakrank.append(p_rank)\n",
    "    \n",
    "    #for scraping Peak rank\n",
    "    wks_on_chart_tags = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]//li[6]//span')\n",
    "    for j in wks_on_chart_tags:\n",
    "        wks = j.text\n",
    "        Weeks.append(wks)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "460ddb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for weeks scraped testing purpose just print all weeks on board\n",
    "#Weeks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "67652e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing of last weel rank list --for removing the alternate blank entry\n",
    "Last_week_rank_req=Last_week_rank[::2]\n",
    "#Last_week_rank_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1f339f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing of Peak rank list --for removing the alternate blank entry from scraped data\n",
    "Peakrank_req=Peakrank[::2]\n",
    "#Peakrank_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "30c96a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Song_Name),len(Artist_Name),len(Last_week_rank_req),len(Peakrank_req),len(Weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0243255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artist_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f1d60404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Song_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d27852ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Billboard Hot 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Last_week_rank</th>\n",
       "      <th>Peak rank</th>\n",
       "      <th>Weeks on chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mariah Carey</td>\n",
       "      <td>All I Want For Christmas Is You</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brenda Lee</td>\n",
       "      <td>Rockin' Around The Christmas Tree</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bobby Helms</td>\n",
       "      <td>Jingle Bell Rock</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Burl Ives</td>\n",
       "      <td>A Holly Jolly Christmas</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wham!</td>\n",
       "      <td>Last Christmas</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Jimmie Allen</td>\n",
       "      <td>Down Home</td>\n",
       "      <td>-</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>One Thing At A Time</td>\n",
       "      <td>81</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Elton John &amp; Britney Spears</td>\n",
       "      <td>Hold Me Closer</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SZA</td>\n",
       "      <td>Far</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Brent Faiyaz</td>\n",
       "      <td>All Mine</td>\n",
       "      <td>95</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Artist_Name                          Song_Name  \\\n",
       "0                  Mariah Carey    All I Want For Christmas Is You   \n",
       "1                    Brenda Lee  Rockin' Around The Christmas Tree   \n",
       "2                   Bobby Helms                   Jingle Bell Rock   \n",
       "3                     Burl Ives            A Holly Jolly Christmas   \n",
       "4                         Wham!                     Last Christmas   \n",
       "..                          ...                                ...   \n",
       "95                 Jimmie Allen                          Down Home   \n",
       "96                Morgan Wallen                One Thing At A Time   \n",
       "97  Elton John & Britney Spears                     Hold Me Closer   \n",
       "98                          SZA                                Far   \n",
       "99                 Brent Faiyaz                           All Mine   \n",
       "\n",
       "   Last_week_rank Peak rank Weeks on chart  \n",
       "0               1         1             57  \n",
       "1               2         2             51  \n",
       "2               4         3             48  \n",
       "3               5         4             31  \n",
       "4               6         5             30  \n",
       "..            ...       ...            ...  \n",
       "95              -        88              5  \n",
       "96             81        37              3  \n",
       "97             89         6             17  \n",
       "98             61        61              2  \n",
       "99             95        42             20  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfhot100 = pd.DataFrame({\"Artist_Name\":Artist_Name[0:100],\"Song_Name\":Song_Name[0:100],\"Last_week_rank\":Last_week_rank_req[0:100],\"Peak rank\":Peakrank_req[0:100],\"Weeks on chart\":Weeks[0:100]})\n",
    "print(\"\\n\",title1)\n",
    "#first 20 entries dataframw\n",
    "#dfhot100.head(20)\n",
    "#for printing dataframe\n",
    "dfhot100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d65c31",
   "metadata": {},
   "source": [
    "# 7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "3e0ae6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f3475c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "858e4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a4e82c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50 48 50\n"
     ]
    }
   ],
   "source": [
    "#for storing scrap data created empty lists\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skillsthey_hire_for = []\n",
    "Location = []\n",
    "try:\n",
    "    #opening a www.billboard.com on automated chrome browser\n",
    "    driver.get(\"https://www.naukri.com/hr-recruiters-consultants\")\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    # entering designation \n",
    "    designation=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/div/div[1]/div[1]/div[2]/input\")\n",
    "    designation.send_keys('Data science')\n",
    "    \n",
    "    #clicking the Search button\n",
    "    search=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/button\")\n",
    "    search.click()\n",
    "    \n",
    "    #alldeatails tag object\n",
    "    all_rec_deatails_leadge = driver.find_elements(By.XPATH,'//div[@class=\"srp_container fl\"]')\n",
    "    #from alldeatails object scrap the deatails \n",
    "    for i in all_rec_deatails_leadge:\n",
    "        \n",
    "        #for scraping recruiter names\n",
    "        all_rec_namestags = driver.find_elements(By.XPATH,'//a[@class=\"ellipsis\"]//span')\n",
    "        for j in all_rec_namestags:\n",
    "            rec_name = j.text\n",
    "            Name.append(rec_name)\n",
    "    \n",
    "    \n",
    "        # for scraping Company\n",
    "        all_rec_company_tags = driver.find_elements(By.XPATH,'//a[2][@class=\"ellipsis\"]')\n",
    "        for j in all_rec_company_tags:\n",
    "            link = j.text\n",
    "            Company.append(link)\n",
    "        \n",
    "        # for scraping Designation    \n",
    "        alldesc_tags = driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]')\n",
    "        text = []\n",
    "        #Desc = []\n",
    "        for i in alldesc_tags:\n",
    "            text = i.text.split(\"\\n\")\n",
    "            Designation.append(text[1])\n",
    "        #print(text)\n",
    "        #text[1]    \n",
    "    \n",
    "        #for Scraping Location\n",
    "        all_rec_location = driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]//span[2]//small')\n",
    "        for j in all_rec_location:\n",
    "            location = j.text\n",
    "            Location.append(location)\n",
    "    \n",
    "        #for scraping --- Skillsthey_hire_for\n",
    "        allskills_tags = driver.find_elements(By.XPATH,'//div[@class=\"hireSec highlightable\"]')\n",
    "        for i in allskills_tags:\n",
    "            sk = i.text\n",
    "            Skillsthey_hire_for.append(sk)\n",
    "        \n",
    "    print(len(Name),len(Designation),len(Company),len(Location),len(Skillsthey_hire_for)) \n",
    "        \n",
    "except NoSuchElementException as e:\n",
    "    Location.append('-')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "7bdbf759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recruiter_Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Skillsthey_hire_for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Data Science Network</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Classic ASP Developer, Internet Marketing Prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shravan Kumar Gaddam</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Shore Infotech India Pvt. Ltd</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>.Net, Java, Data Science, Linux Administration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anik Agrawal</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Enerlytics Software Solutions Pvt Ltd</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Mean Stack, javascript, angularjs, mongodb, We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subhas patel</td>\n",
       "      <td>Founder CEO</td>\n",
       "      <td>LibraryXProject</td>\n",
       "      <td>UK - (london)</td>\n",
       "      <td>Hadoop, Spark, Digital Strategy, Data Architec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abhishek - Only Analytics Hiring - India and</td>\n",
       "      <td>Recruitment Lead Consultant</td>\n",
       "      <td>Apidel Technologies Division of Transpower</td>\n",
       "      <td>Vadodara / Baroda</td>\n",
       "      <td>Analytics, Business Intelligence, Business Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Institute for Financial Management and Resear</td>\n",
       "      <td>Programme Manager</td>\n",
       "      <td>IFMR</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balu Ramesh</td>\n",
       "      <td>HR Administrator</td>\n",
       "      <td>Techvantage Systems Pvt Ltd</td>\n",
       "      <td>Trivandrum</td>\n",
       "      <td>Machine Learning, algorithms, Go Getter, Compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Asif Lucknowi</td>\n",
       "      <td>Director</td>\n",
       "      <td>Weupskill- Live Wire India</td>\n",
       "      <td>Indore</td>\n",
       "      <td>Technical Training, Software Development, Pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InstaFinancials</td>\n",
       "      <td>Human Resource</td>\n",
       "      <td>CBL Data Science Private Limited</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Software Development, It Sales, Account Manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kalpana Dumpala</td>\n",
       "      <td>Executive Hiring</td>\n",
       "      <td>Innominds Software</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Qa, Ui/ux, Java Developer, Java Architect, C++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mubarak</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>MoneyTap</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Business Intelligence, Data Warehousing, Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kushal Rastogi</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>QuantMagnum Technologies Pvt. Ltd.</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Office Administration, Hr Administration, tele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mahesh Babu Channa</td>\n",
       "      <td>HR Team Lead</td>\n",
       "      <td>SocialPrachar.com</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Social Media, digital media maketing, seo, smm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vaishnavi Kudalkar</td>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Codeachive learning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Data Science, Python, Data Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Priyanka Akiri</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Infinitive Software Solutions</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Oracle Dba, Data Science, Data Warehousing, ET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kapil Devang</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>BISP Solutions</td>\n",
       "      <td>Bhopal</td>\n",
       "      <td>Big Data, Hadoop, Data Analytics, Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sakshi Chhikara</td>\n",
       "      <td>Assistant Manager HR</td>\n",
       "      <td>BIZ INFOTECNO PRIVATE LIMITED</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>React.js, Data Science, Java, Front End, Busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ruchi Dhote</td>\n",
       "      <td>Senior Executive Talent Acquisition</td>\n",
       "      <td>Bristlecone India Ltd</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Qlikview, Qlik Sense, Microsoft Azure, Power B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Manisha Yadav</td>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Easi Tax</td>\n",
       "      <td>Navi Mumbai</td>\n",
       "      <td>Telecalling, Client Interaction, Marketing, Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Riya Rajesh</td>\n",
       "      <td>Manager Talent Acquisition</td>\n",
       "      <td>Novelworx Digital Solutions</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Rashmi Bhattacharjee</td>\n",
       "      <td>HR Head</td>\n",
       "      <td>AXESTRACK SOFTWARE SOLUTIONS PRIVATE...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Corporate Sales, Software Development, Softwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Faizan Kareem</td>\n",
       "      <td>HR MANAGER</td>\n",
       "      <td>FirstTech Consaltants Pvt.Ltd</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Data Analytics, Data Science, Machine Learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rithika dadwal</td>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Affine Analytics</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Data Science, Machine Learning, Python, R, Dee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sandhya Khandagale</td>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Compumatrice Multimedia Pvt Ltd</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Big Data, Data Science, Artificial Intelligenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Shaun Rao</td>\n",
       "      <td>Manager Human Resources</td>\n",
       "      <td>Exela Technologies</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Java, Net, Angularjs, Hr, Infrastructure, Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Deeparchi Sharma</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>ZIGRAM</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Research, Digital Marketing, Analytics, Softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Azahar Shaikh</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>NEAL ANALYTICS SERVICES PVT LTD</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Manas</td>\n",
       "      <td>Lead Talent acquisition</td>\n",
       "      <td>Autumn Leaf Consulting Services Private...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Software Architecture, Vp Engineering, Product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>kumar</td>\n",
       "      <td>Proprietor</td>\n",
       "      <td>trainin</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Data Science, Hadoop, Rpas, Devops, Python, Aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sunil Vedula</td>\n",
       "      <td>CEO</td>\n",
       "      <td>Nanoprecise Sci Corp</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Signal Processing, Machine Learning, Neural Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Rajat Kumar</td>\n",
       "      <td>Founder CEO</td>\n",
       "      <td>R.S Consultancy &amp;amp; Services</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Web Technologies, Project Management, Software...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Dhruv Dev Dubey</td>\n",
       "      <td>Company Recruitment Head</td>\n",
       "      <td>Confidential</td>\n",
       "      <td>Mysoru / Mysore</td>\n",
       "      <td>Server Administartion, Verilog, Vhdl, Digital ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Jayanth N</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>Dollarbird Information Services Pvt, Ltd</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Data Analytics, Managed Services, Team Leading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Avodha</td>\n",
       "      <td>Business Development Associate</td>\n",
       "      <td>Nikitha Palaparthi</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Ethical Hacking, Security Operations Center, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Priya Khare</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>Independent Consultant</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Data Science, Artificial Intelligence, analyti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Amit Sharma</td>\n",
       "      <td>Consultant</td>\n",
       "      <td>ASCO consulting</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Machine Learning, Artificial Intelligence, Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Kanan</td>\n",
       "      <td>senior technology instructor</td>\n",
       "      <td>NY INST</td>\n",
       "      <td>Aligarh</td>\n",
       "      <td>C, C++, Artificial Intelligence, Python, Php, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Shashikant Chaudhary</td>\n",
       "      <td>HR Recruiter/HR Excutive</td>\n",
       "      <td>3D India Staffing Research &amp;amp; Consulting...</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>Relationship Management, Retail Sales, Private...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Brad</td>\n",
       "      <td>Manager, Technical Recruiting</td>\n",
       "      <td>O.C. Tanner</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Data Science, Software Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Rutuja Pawar</td>\n",
       "      <td>Technical Recruiter</td>\n",
       "      <td>Demand Matrix</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Data Science, Big Data Analytics, Digital Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Madhusudhan Sridhar</td>\n",
       "      <td>Erp Implementer</td>\n",
       "      <td>MADHUSUDHAN SRIDHAR</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Data Science, Recruitment, Salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Ankit Sinha</td>\n",
       "      <td>Head Analytics</td>\n",
       "      <td>Suntech Global</td>\n",
       "      <td>Indore</td>\n",
       "      <td>B.Tech, Tableau, Statistics, R, Analytics, Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Gaurav Chouhan</td>\n",
       "      <td>Chief Technical Officer</td>\n",
       "      <td>Strategic Consulting Lab</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Software Development, Business Intelligence, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Rashi Kacker</td>\n",
       "      <td>Sr Product Manager</td>\n",
       "      <td>Impel Labs Pvt. Ltd.</td>\n",
       "      <td>MYSORE</td>\n",
       "      <td>Data Science, Node.js, Angularjs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Ashwini</td>\n",
       "      <td>Director Global Delivery</td>\n",
       "      <td>MRP Advisers</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Data Science, Media Marketing, Resource Planni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Balaji Kolli</td>\n",
       "      <td>Co Founder</td>\n",
       "      <td>Saras Solutions India Pvt Ltd</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Data Analysis, Learning, Data Science, Compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Rajani Nagaraj</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>WildJasmine</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Java, Hadoop, R, Machine Learning, Spark, Flum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Recruiter_Name  \\\n",
       "0                                    Aakash Harit   \n",
       "1                            shravan Kumar Gaddam   \n",
       "2                        MARSIAN Technologies LLP   \n",
       "3                                    Anik Agrawal   \n",
       "4                                    subhas patel   \n",
       "5    Abhishek - Only Analytics Hiring - India and   \n",
       "6   Institute for Financial Management and Resear   \n",
       "7                                     Balu Ramesh   \n",
       "8                                   Asif Lucknowi   \n",
       "9                                 InstaFinancials   \n",
       "10                                Kalpana Dumpala   \n",
       "11                                        Mubarak   \n",
       "12                                 Kushal Rastogi   \n",
       "13                             Mahesh Babu Channa   \n",
       "14                             Vaishnavi Kudalkar   \n",
       "15                                 Priyanka Akiri   \n",
       "16                                   Kapil Devang   \n",
       "17                                Sakshi Chhikara   \n",
       "18                                    Ruchi Dhote   \n",
       "19                                  Manisha Yadav   \n",
       "20                                    Riya Rajesh   \n",
       "21                           Rashmi Bhattacharjee   \n",
       "22                                  Faizan Kareem   \n",
       "23                                 Rithika dadwal   \n",
       "24                             Sandhya Khandagale   \n",
       "25                                      Shaun Rao   \n",
       "26                               Deeparchi Sharma   \n",
       "27                                  Azahar Shaikh   \n",
       "28                                          Manas   \n",
       "29                                          kumar   \n",
       "30                                   Sunil Vedula   \n",
       "31                                    Rajat Kumar   \n",
       "32                                Dhruv Dev Dubey   \n",
       "33                                      Jayanth N   \n",
       "34                                         Avodha   \n",
       "35                                    Priya Khare   \n",
       "36                                    Amit Sharma   \n",
       "37                                          Kanan   \n",
       "38                           Shashikant Chaudhary   \n",
       "39                                           Brad   \n",
       "40                                   Rutuja Pawar   \n",
       "41                            Madhusudhan Sridhar   \n",
       "42                                    Ankit Sinha   \n",
       "43                                 Gaurav Chouhan   \n",
       "44                                   Rashi Kacker   \n",
       "45                                        Ashwini   \n",
       "46                                   Balaji Kolli   \n",
       "47                                 Rajani Nagaraj   \n",
       "\n",
       "                            Designation  \\\n",
       "0                            HR Manager   \n",
       "1                     Company Recruiter   \n",
       "2                            Company HR   \n",
       "3                     Company Recruiter   \n",
       "4                           Founder CEO   \n",
       "5           Recruitment Lead Consultant   \n",
       "6                     Programme Manager   \n",
       "7                      HR Administrator   \n",
       "8                              Director   \n",
       "9                        Human Resource   \n",
       "10                     Executive Hiring   \n",
       "11                           Company HR   \n",
       "12                           Company HR   \n",
       "13                         HR Team Lead   \n",
       "14                         HR Executive   \n",
       "15                           HR Manager   \n",
       "16                           HR Manager   \n",
       "17                 Assistant Manager HR   \n",
       "18  Senior Executive Talent Acquisition   \n",
       "19                         HR Executive   \n",
       "20           Manager Talent Acquisition   \n",
       "21                              HR Head   \n",
       "22                           HR MANAGER   \n",
       "23                         HR Recruiter   \n",
       "24                         HR Recruiter   \n",
       "25              Manager Human Resources   \n",
       "26                    Company Recruiter   \n",
       "27                    Company Recruiter   \n",
       "28              Lead Talent acquisition   \n",
       "29                           Proprietor   \n",
       "30                                  CEO   \n",
       "31                          Founder CEO   \n",
       "32             Company Recruitment Head   \n",
       "33                      Project Manager   \n",
       "34       Business Development Associate   \n",
       "35                       Senior Manager   \n",
       "36                           Consultant   \n",
       "37         senior technology instructor   \n",
       "38             HR Recruiter/HR Excutive   \n",
       "39        Manager, Technical Recruiting   \n",
       "40                  Technical Recruiter   \n",
       "41                      Erp Implementer   \n",
       "42                       Head Analytics   \n",
       "43              Chief Technical Officer   \n",
       "44                   Sr Product Manager   \n",
       "45             Director Global Delivery   \n",
       "46                           Co Founder   \n",
       "47                           HR Manager   \n",
       "\n",
       "                                           Company                  Location  \\\n",
       "0                             Data Science Network                     Delhi   \n",
       "1                    Shore Infotech India Pvt. Ltd  Hyderabad / Secunderabad   \n",
       "2                         MARSIAN Technologies LLP                      Pune   \n",
       "3            Enerlytics Software Solutions Pvt Ltd                 Ahmedabad   \n",
       "4                                  LibraryXProject             UK - (london)   \n",
       "5       Apidel Technologies Division of Transpower         Vadodara / Baroda   \n",
       "6                                             IFMR                   Chennai   \n",
       "7                      Techvantage Systems Pvt Ltd                Trivandrum   \n",
       "8                       Weupskill- Live Wire India                    Indore   \n",
       "9                 CBL Data Science Private Limited     Bengaluru / Bangalore   \n",
       "10                              Innominds Software  Hyderabad / Secunderabad   \n",
       "11                                        MoneyTap     Bengaluru / Bangalore   \n",
       "12              QuantMagnum Technologies Pvt. Ltd.                    Mumbai   \n",
       "13                               SocialPrachar.com  Hyderabad / Secunderabad   \n",
       "14                             Codeachive learning                    Mumbai   \n",
       "15                   Infinitive Software Solutions                 Hyderabad   \n",
       "16                                  BISP Solutions                    Bhopal   \n",
       "17                   BIZ INFOTECNO PRIVATE LIMITED                Chandigarh   \n",
       "18                           Bristlecone India Ltd                      Pune   \n",
       "19                                        Easi Tax               Navi Mumbai   \n",
       "20                     Novelworx Digital Solutions                    Cochin   \n",
       "21         AXESTRACK SOFTWARE SOLUTIONS PRIVATE...                     Delhi   \n",
       "22                   FirstTech Consaltants Pvt.Ltd  Hyderabad / Secunderabad   \n",
       "23                                Affine Analytics                      Pune   \n",
       "24                 Compumatrice Multimedia Pvt Ltd                      Pune   \n",
       "25                              Exela Technologies                      Pune   \n",
       "26                                          ZIGRAM                   Gurgaon   \n",
       "27                 NEAL ANALYTICS SERVICES PVT LTD                      Pune   \n",
       "28      Autumn Leaf Consulting Services Private...     Bengaluru / Bangalore   \n",
       "29                                         trainin     Bengaluru / Bangalore   \n",
       "30                            Nanoprecise Sci Corp                     Delhi   \n",
       "31                  R.S Consultancy &amp; Services     Bengaluru / Bangalore   \n",
       "32                                    Confidential           Mysoru / Mysore   \n",
       "33        Dollarbird Information Services Pvt, Ltd  Hyderabad / Secunderabad   \n",
       "34                              Nikitha Palaparthi     Bengaluru / Bangalore   \n",
       "35                          Independent Consultant                 New Delhi   \n",
       "36                                 ASCO consulting                   Chennai   \n",
       "37                                         NY INST                   Aligarh   \n",
       "38  3D India Staffing Research &amp; Consulting...            Salt Lake City   \n",
       "39                                     O.C. Tanner                      Pune   \n",
       "40                                   Demand Matrix     Bengaluru / Bangalore   \n",
       "41                             MADHUSUDHAN SRIDHAR                    Mumbai   \n",
       "42                                  Suntech Global                    Indore   \n",
       "43                        Strategic Consulting Lab     Bengaluru / Bangalore   \n",
       "44                            Impel Labs Pvt. Ltd.                    MYSORE   \n",
       "45                                    MRP Advisers  Hyderabad / Secunderabad   \n",
       "46                   Saras Solutions India Pvt Ltd     Bengaluru / Bangalore   \n",
       "47                                     WildJasmine                    Mumbai   \n",
       "\n",
       "                                  Skillsthey_hire_for  \n",
       "0   Classic ASP Developer, Internet Marketing Prof...  \n",
       "1   .Net, Java, Data Science, Linux Administration...  \n",
       "2   Data Science, Artificial Intelligence, Machine...  \n",
       "3   Mean Stack, javascript, angularjs, mongodb, We...  \n",
       "4   Hadoop, Spark, Digital Strategy, Data Architec...  \n",
       "5   Analytics, Business Intelligence, Business Ana...  \n",
       "6                                        Data Science  \n",
       "7   Machine Learning, algorithms, Go Getter, Compu...  \n",
       "8   Technical Training, Software Development, Pres...  \n",
       "9   Software Development, It Sales, Account Manage...  \n",
       "10  Qa, Ui/ux, Java Developer, Java Architect, C++...  \n",
       "11  Business Intelligence, Data Warehousing, Data ...  \n",
       "12  Office Administration, Hr Administration, tele...  \n",
       "13  Social Media, digital media maketing, seo, smm...  \n",
       "14               Data Science, Python, Data Analytics  \n",
       "15  Oracle Dba, Data Science, Data Warehousing, ET...  \n",
       "16     Big Data, Hadoop, Data Analytics, Data Science  \n",
       "17  React.js, Data Science, Java, Front End, Busin...  \n",
       "18  Qlikview, Qlik Sense, Microsoft Azure, Power B...  \n",
       "19  Telecalling, Client Interaction, Marketing, Re...  \n",
       "20                                       Data Science  \n",
       "21  Corporate Sales, Software Development, Softwar...  \n",
       "22  Data Analytics, Data Science, Machine Learning...  \n",
       "23  Data Science, Machine Learning, Python, R, Dee...  \n",
       "24  Big Data, Data Science, Artificial Intelligenc...  \n",
       "25  Java, Net, Angularjs, Hr, Infrastructure, Mana...  \n",
       "26  Research, Digital Marketing, Analytics, Softwa...  \n",
       "27  Data Science, Artificial Intelligence, Machine...  \n",
       "28  Software Architecture, Vp Engineering, Product...  \n",
       "29  Data Science, Hadoop, Rpas, Devops, Python, Aw...  \n",
       "30  Signal Processing, Machine Learning, Neural Ne...  \n",
       "31  Web Technologies, Project Management, Software...  \n",
       "32  Server Administartion, Verilog, Vhdl, Digital ...  \n",
       "33  Data Analytics, Managed Services, Team Leading...  \n",
       "34  Ethical Hacking, Security Operations Center, S...  \n",
       "35  Data Science, Artificial Intelligence, analyti...  \n",
       "36  Machine Learning, Artificial Intelligence, Dat...  \n",
       "37  C, C++, Artificial Intelligence, Python, Php, ...  \n",
       "38  Relationship Management, Retail Sales, Private...  \n",
       "39                 Data Science, Software Engineering  \n",
       "40  Data Science, Big Data Analytics, Digital Mark...  \n",
       "41                  Data Science, Recruitment, Salary  \n",
       "42  B.Tech, Tableau, Statistics, R, Analytics, Tim...  \n",
       "43  Software Development, Business Intelligence, B...  \n",
       "44                   Data Science, Node.js, Angularjs  \n",
       "45  Data Science, Media Marketing, Resource Planni...  \n",
       "46  Data Analysis, Learning, Data Science, Compute...  \n",
       "47  Java, Hadoop, R, Machine Learning, Spark, Flum...  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recruiter = pd.DataFrame({\"Recruiter_Name\":Name[0:48],\"Designation\":Designation[0:48],\"Company\":Company[0:48],\"Location\":Location[0:48],\"Skillsthey_hire_for\":Skillsthey_hire_for[0:48]})\n",
    "df_recruiter.head(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "434f05d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 150 1053 48 100\n"
     ]
    }
   ],
   "source": [
    "Name = []\n",
    "Desc = []\n",
    "Descignation = []\n",
    "Location = []\n",
    "all_rec_deatails_leadge = driver.find_elements(By.XPATH,'//div[@class=\"srp_container fl\"]')\n",
    "for i in all_rec_deatails_leadge:\n",
    "    #for scraping recruiter names\n",
    "    all_rec_namestags = driver.find_elements(By.XPATH,'//a[@class=\"ellipsis\"]//span')\n",
    "    for j in all_rec_namestags:\n",
    "        rec_name = j.text\n",
    "        Name.append(rec_name)\n",
    "    \n",
    "    \n",
    "    # for scraping Company\n",
    "    all_rec_company_tags = driver.find_elements(By.XPATH,'//a[2][@class=\"ellipsis\"]')\n",
    "    for j in all_rec_company_tags:\n",
    "        link = j.text\n",
    "        Company.append(link)\n",
    "        \n",
    "    # for scraping Designation    \n",
    "    alldesc_tags = driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]')\n",
    "    text = []\n",
    "    #Desc = []\n",
    "    for i in alldesc_tags:\n",
    "        text = i.text.split(\"\\n\")\n",
    "        Designation.append(text[1])\n",
    "    #print(text)\n",
    "    #text[1]    \n",
    "    \n",
    "    #for Scraping Location\n",
    "    all_rec_location = driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]//span[2]//small')\n",
    "    for j in all_rec_location:\n",
    "        location = j.text\n",
    "        Location.append(location)\n",
    "    \n",
    "    #for scraping --- Skillsthey_hire_for\n",
    "    allskills_tags = driver.find_elements(By.XPATH,'//div[@class=\"hireSec highlightable\"]')\n",
    "    for i in allskills_tags:\n",
    "        sk = i.text\n",
    "        Skillsthey_hire_for.append(sk)\n",
    "        \n",
    "    print(len(Name),len(Designation),len(Company),len(Location),len(Skillsthey_hire_for)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "926dae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "5866aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skillsthey_hire_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ddc6ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2d51542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amir Chowdhury', 'Managing Partner', 'Granular.ai']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Managing Partner'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logic for slicing designation of recruiter\n",
    "alltags = driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]')\n",
    "text = []\n",
    "Desc = []\n",
    "for i in alltags:\n",
    "    text=i.text.split(\"\\n\")\n",
    "    Desc.append(text)\n",
    "print(text)\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c8b99b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic for Location scraping\n",
    "Location = []\n",
    "all_rec_deatails_leadge = driver.find_elements(By.XPATH,'//div[@class=\"srp_container fl\"]')\n",
    "for i in all_rec_deatails_leadge:\n",
    "    # for scraping Designation\n",
    "    all_rec_location = driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]//span[2]//small')\n",
    "    for j in all_rec_location:\n",
    "        location = j.text\n",
    "        Location.append(location)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "327e033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "511724bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#login for---\n",
    "# for scraping Company\n",
    "all_rec_company_tags = driver.find_elements(By.XPATH,'//p//a[2][@class=\"ellipsis\"]')\n",
    "for j in all_rec_company_tags:\n",
    "    link = j.text\n",
    "    Company.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "aaa11188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "36519f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logic for  slicing data\n",
    "# for scraping Designation\n",
    "rec_desg = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis clr\"]')\n",
    "for j in rec_desg:\n",
    "    rec_desg = j.text\n",
    "    Designation.append(rec_desg)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "72f0185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acffb24",
   "metadata": {},
   "source": [
    "# 8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "66d2d706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "908589cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2992cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c9d501e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #opening a github page on automated chrome browser\n",
    "    driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "    driver.maximize_window()\n",
    "    #empty list to store columns in table\n",
    "    allrows_col1 = []\n",
    "    allrows_col2 = []\n",
    "    allrows_col3 = []\n",
    "    allrows_col4 = []\n",
    "    allrows_col5 = []\n",
    "    allrows_col6 = []\n",
    "    \n",
    "    #scroll down\n",
    "    driver.execute_script(\"window.scrollBy(0,2200)\")\n",
    "    \n",
    "    #tags for whole table\n",
    "    leadge_tabletags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]')\n",
    "    for i in leadge_tabletags:\n",
    "        # this is tags object for allrows in the table\n",
    "        allrowstags = driver.find_elements(By.XPATH,'//tbody')\n",
    "        #for given row scrape the columns\n",
    "        for i in allrowstags:\n",
    "            \n",
    "                #for Scraping Rank\n",
    "            column1 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[1]')\n",
    "            for c1 in column1:\n",
    "                row = c1.text\n",
    "                allrows_col1.append(row)\n",
    "            \n",
    "                #for Scraping Books Title\n",
    "            column2 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[2]')\n",
    "            for c2 in column2:\n",
    "                row = c2.text\n",
    "                allrows_col2.append(row)\n",
    "            \n",
    "                #for Scraping Author\n",
    "            column3 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[3]')\n",
    "            for c3 in column3:\n",
    "                row = c3.text\n",
    "                allrows_col3.append(row)\n",
    "                \n",
    "                #for Scraping Volume Sales     \n",
    "            column4 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[4]')\n",
    "            for c4 in column4:\n",
    "                row = c4.text\n",
    "                allrows_col4.append(row)\n",
    "                 \n",
    "                #for Scraping Publisher\n",
    "            column5 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[5]')\n",
    "            for c5 in column5:\n",
    "                row = c5.text\n",
    "                allrows_col5.append(row)\n",
    "                \n",
    "                \n",
    "                #for Scraping Publisher\n",
    "            column6 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[6]')\n",
    "            for c6 in column6:\n",
    "                row = c6.text\n",
    "                allrows_col6.append(row)\n",
    "                \n",
    "            print(len(allrows_col1),len(allrows_col2),len(allrows_col3),len(allrows_col4),len(allrows_col5),len(allrows_col6))\n",
    "                \n",
    "except NoSucElementException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "7f33604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume Sales</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                              Title            Author  \\\n",
       "0     1                                  Da Vinci Code,The        Brown, Dan   \n",
       "1     2               Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2     3           Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3     4          Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4     5                               Fifty Shades of Grey      James, E. L.   \n",
       "..  ...                                                ...               ...   \n",
       "95   96                                          Ghost,The    Harris, Robert   \n",
       "96   97                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97   98              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98   99  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume Sales        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Higestselling_novel = pd.DataFrame({\"Rank\":allrows_col1[0:100],\"Title\":allrows_col2[0:100],\"Author\":allrows_col3[0:100],\"Volume Sales\":allrows_col4[0:100],\"Publisher\":allrows_col5[0:100],\"Genre\":allrows_col6[0:100]})\n",
    "df_Higestselling_novel.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "fea29802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic for craping column 1\n",
    "allrows_col1 = []\n",
    "\n",
    "leadge_tabletags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]')\n",
    "for i in leadge_tabletags:\n",
    "    allrowstags = driver.find_elements(By.XPATH,'//tbody')\n",
    "    \n",
    "    for i in allrowstags:\n",
    "        #for Scraping Rank\n",
    "        column1 = driver.find_elements(By.XPATH,'/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[1]')\n",
    "        for c1 in column1:\n",
    "            row = c1.text\n",
    "            allrows_col1.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c7da5",
   "metadata": {},
   "source": [
    "# 9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "3762fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "1fe6d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "2d9b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "29833178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #create empty list to store the retrived data for top title ,Rank,Name,Year span,genre,run time, votes\n",
    "    title1 = []\n",
    "    \n",
    "    Rank = []\n",
    "    Name = []\n",
    "    Year_Span = []\n",
    "    Genre = []\n",
    "    Run_time = []\n",
    "    Ratings = []\n",
    "    Votes = []\n",
    "    \n",
    "    #opening a github page on automated chrome browser\n",
    "    driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "    driver.maximize_window()\n",
    "    #scraping Title at the top\n",
    "    titletag = driver.find_element(By.XPATH,'//h1[@class=\"header list-name\"]')\n",
    "    title = titletag.text\n",
    "    \n",
    "    #all deatails tag of 100 most watched tv shows of the all time\n",
    "    all100_mostview_tvshow_leadge = driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"]')\n",
    "    \n",
    "    #from alldeatails object scrap the deatails \n",
    "    for i in all100_mostview_tvshow_leadge:\n",
    "        \n",
    "        # scraping 100 top viwed tvshows Rank\n",
    "        all_tvshow_rankstags = driver.find_elements(By.XPATH,'//span[@class=\"lister-item-index unbold text-primary\"]')\n",
    "        for j in all_tvshow_rankstags:\n",
    "            rank_name = j.text\n",
    "            Rank.append(rank_name)\n",
    "        \n",
    "        #scraping 100 top viwed tvshows  names\n",
    "        all_tvshow_namestags = driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]//a')\n",
    "        for j in all_tvshow_namestags:\n",
    "            rec_name = j.text\n",
    "            Name.append(rec_name)\n",
    "        \n",
    "        #scraping 100 top viwed tvshows Year_Span \n",
    "        all_tvshow_yearspantags = driver.find_elements(By.XPATH,'//span[@class=\"lister-item-year text-muted unbold\"]')\n",
    "        for j in all_tvshow_yearspantags:\n",
    "            y_span = j.text\n",
    "            Year_Span.append(y_span)\n",
    "        \n",
    "        #scraping 100 top viwed tvshows genre\n",
    "        all_tvshow_Genre_tags = driver.find_elements(By.XPATH,'//span[@class=\"genre\"]')\n",
    "        for j in all_tvshow_Genre_tags:\n",
    "            genre = j.text\n",
    "            Genre.append(genre)\n",
    "            \n",
    "         #scraping 100 top viwed tvshows runtime\n",
    "        all_tvshow_runtime_tags = driver.find_elements(By.XPATH,'//span[@class=\"runtime\"]')\n",
    "        for j in all_tvshow_runtime_tags:\n",
    "            rtime = j.text\n",
    "            Run_time.append(rtime)   \n",
    "        \n",
    "        #scraping 100 top viwed tvshows Ratings\n",
    "        all_tvshow_Ratings_tags = driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-star small\"]//span[2]')\n",
    "        for j in all_tvshow_Ratings_tags:\n",
    "            rating = j.text\n",
    "            Ratings.append(rating)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #scraping 100 top viwed tvshows Votes\n",
    "        all_tvshow_Votes_tags = driver.find_elements(By.XPATH,'//p[4][@class=\"text-muted text-small\"]/span[2]')\n",
    "        for j in all_tvshow_Votes_tags:\n",
    "            vote = j.text\n",
    "            Votes.append(vote)\n",
    "        \n",
    "        print(len(Rank),len(Name),len(Year_Span),len(Genre),len(Run_time),len(Ratings),len(Votes))   \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "1f46d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 100 most watched tv shows of all time\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>TVshow_Name</th>\n",
       "      <th>Year_Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run_time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,100,493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,190,782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>994,342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>294,275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>253,376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>Orange Is the New Black</td>\n",
       "      <td>(2013–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>59 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>303,943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>Riverdale</td>\n",
       "      <td>(2017– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>45 min</td>\n",
       "      <td>6.6</td>\n",
       "      <td>146,063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>Grey's Anatomy</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>310,825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>The Flash</td>\n",
       "      <td>(2014–2023)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>347,282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>Arrow</td>\n",
       "      <td>(2012–2020)</td>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>432,780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>Money Heist</td>\n",
       "      <td>(2017–2021)</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>70 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>478,905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>(2007–2019)</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>810,569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>Black Mirror</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>60 min</td>\n",
       "      <td>8.8</td>\n",
       "      <td>549,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>(2010–2017)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>88 min</td>\n",
       "      <td>9.1</td>\n",
       "      <td>928,270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>(2013–2020)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.5</td>\n",
       "      <td>534,266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>Pretty Little Liars</td>\n",
       "      <td>(2010–2017)</td>\n",
       "      <td>Drama, Mystery, Romance</td>\n",
       "      <td>44 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>168,602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>The Vampire Diaries</td>\n",
       "      <td>(2009–2017)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>323,594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>American Horror Story</td>\n",
       "      <td>(2011– )</td>\n",
       "      <td>Drama, Horror, Sci-Fi</td>\n",
       "      <td>60 min</td>\n",
       "      <td>8</td>\n",
       "      <td>321,109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>Breaking Bad</td>\n",
       "      <td>(2008–2013)</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>49 min</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1,886,625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>Lucifer</td>\n",
       "      <td>(2016–2021)</td>\n",
       "      <td>Crime, Drama, Fantasy</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>326,350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>Supernatural</td>\n",
       "      <td>(2005–2020)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.4</td>\n",
       "      <td>447,665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>Prison Break</td>\n",
       "      <td>(2005–2017)</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.3</td>\n",
       "      <td>537,535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>How to Get Away with Murder</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>43 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>153,001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>Teen Wolf</td>\n",
       "      <td>(2011–2017)</td>\n",
       "      <td>Action, Drama, Fantasy</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>149,766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>The Simpsons</td>\n",
       "      <td>(1989– )</td>\n",
       "      <td>Animation, Comedy</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>408,996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>Once Upon a Time</td>\n",
       "      <td>(2011–2018)</td>\n",
       "      <td>Adventure, Fantasy, Romance</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>226,069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>Narcos</td>\n",
       "      <td>(2015–2017)</td>\n",
       "      <td>Biography, Crime, Drama</td>\n",
       "      <td>49 min</td>\n",
       "      <td>8.8</td>\n",
       "      <td>424,072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>Daredevil</td>\n",
       "      <td>(2015–2018)</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>54 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>442,478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>Friends</td>\n",
       "      <td>(1994–2004)</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.9</td>\n",
       "      <td>998,216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>How I Met Your Mother</td>\n",
       "      <td>(2005–2014)</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.3</td>\n",
       "      <td>685,788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                  TVshow_Name    Year_Span  \\\n",
       "0    1.              Game of Thrones  (2011–2019)   \n",
       "1    2.              Stranger Things     (2016– )   \n",
       "2    3.             The Walking Dead  (2010–2022)   \n",
       "3    4.               13 Reasons Why  (2017–2020)   \n",
       "4    5.                      The 100  (2014–2020)   \n",
       "5    6.      Orange Is the New Black  (2013–2019)   \n",
       "6    7.                    Riverdale     (2017– )   \n",
       "7    8.               Grey's Anatomy     (2005– )   \n",
       "8    9.                    The Flash  (2014–2023)   \n",
       "9   10.                        Arrow  (2012–2020)   \n",
       "10  11.                  Money Heist  (2017–2021)   \n",
       "11  12.          The Big Bang Theory  (2007–2019)   \n",
       "12  13.                 Black Mirror  (2011–2019)   \n",
       "13  14.                     Sherlock  (2010–2017)   \n",
       "14  15.                      Vikings  (2013–2020)   \n",
       "15  16.          Pretty Little Liars  (2010–2017)   \n",
       "16  17.          The Vampire Diaries  (2009–2017)   \n",
       "17  18.        American Horror Story     (2011– )   \n",
       "18  19.                 Breaking Bad  (2008–2013)   \n",
       "19  20.                      Lucifer  (2016–2021)   \n",
       "20  21.                 Supernatural  (2005–2020)   \n",
       "21  22.                 Prison Break  (2005–2017)   \n",
       "22  23.  How to Get Away with Murder  (2014–2020)   \n",
       "23  24.                    Teen Wolf  (2011–2017)   \n",
       "24  25.                 The Simpsons     (1989– )   \n",
       "25  26.             Once Upon a Time  (2011–2018)   \n",
       "26  27.                       Narcos  (2015–2017)   \n",
       "27  28.                    Daredevil  (2015–2018)   \n",
       "28  29.                      Friends  (1994–2004)   \n",
       "29  30.        How I Met Your Mother  (2005–2014)   \n",
       "\n",
       "                          Genre Run_time Ratings      Votes  \n",
       "0      Action, Adventure, Drama   57 min     9.2  2,100,493  \n",
       "1        Drama, Fantasy, Horror   51 min     8.7  1,190,782  \n",
       "2       Drama, Horror, Thriller   44 min     8.1    994,342  \n",
       "3      Drama, Mystery, Thriller   60 min     7.5    294,275  \n",
       "4        Drama, Mystery, Sci-Fi   43 min     7.6    253,376  \n",
       "5          Comedy, Crime, Drama   59 min     8.1    303,943  \n",
       "6         Crime, Drama, Mystery   45 min     6.6    146,063  \n",
       "7                Drama, Romance   41 min     7.6    310,825  \n",
       "8      Action, Adventure, Drama   43 min     7.6    347,282  \n",
       "9      Action, Adventure, Crime   42 min     7.5    432,780  \n",
       "10         Action, Crime, Drama   70 min     8.2    478,905  \n",
       "11              Comedy, Romance   22 min     8.2    810,569  \n",
       "12       Drama, Mystery, Sci-Fi   60 min     8.8    549,442  \n",
       "13        Crime, Drama, Mystery   88 min     9.1    928,270  \n",
       "14     Action, Adventure, Drama   44 min     8.5    534,266  \n",
       "15      Drama, Mystery, Romance   44 min     7.4    168,602  \n",
       "16       Drama, Fantasy, Horror   43 min     7.7    323,594  \n",
       "17        Drama, Horror, Sci-Fi   60 min       8    321,109  \n",
       "18       Crime, Drama, Thriller   49 min     9.5  1,886,625  \n",
       "19        Crime, Drama, Fantasy   42 min     8.1    326,350  \n",
       "20       Drama, Fantasy, Horror   44 min     8.4    447,665  \n",
       "21         Action, Crime, Drama   44 min     8.3    537,535  \n",
       "22        Crime, Drama, Mystery   43 min     8.1    153,001  \n",
       "23       Action, Drama, Fantasy   41 min     7.7    149,766  \n",
       "24            Animation, Comedy   22 min     8.7    408,996  \n",
       "25  Adventure, Fantasy, Romance   60 min     7.7    226,069  \n",
       "26      Biography, Crime, Drama   49 min     8.8    424,072  \n",
       "27         Action, Crime, Drama   54 min     8.6    442,478  \n",
       "28              Comedy, Romance   22 min     8.9    998,216  \n",
       "29              Comedy, Romance   22 min     8.3    685,788  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MostviwedTvShow = pd.DataFrame({\"Rank\":Rank[0:100],\"TVshow_Name\":Name[0:100],\"Year_Span\":Year_Span[0:100],\"Genre\":Genre[0:100],\"Run_time\":Run_time[0:100],\"Ratings\":Ratings[0:100],\"Votes\":Votes[0:100]})\n",
    "print(\"\\n\",title)\n",
    "df_MostviwedTvShow.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8f4c220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic for votes retrival\n",
    "Votes = []\n",
    "all_tvshow_Votes_tags = driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-star small\"]//span[2]')\n",
    "for j in all_tvshow_Votes_tags:\n",
    "        vote = j.text\n",
    "        Votes.append(vote)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "28ab80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic for scraping Tvshow names\n",
    "Rank = []\n",
    "Name = []\n",
    "#all deatails tag of 100 most watched tv shows of the all time\n",
    "all100_mostview_tvshow_leadge = driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"]')\n",
    "for i in all100_mostview_tvshow_leadge:    \n",
    "    #for  names\n",
    "    all_tvshow_namestags = driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]//a')\n",
    "    for j in all_tvshow_namestags:\n",
    "        rec_name = j.text\n",
    "        Name.append(rec_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879947",
   "metadata": {},
   "source": [
    "# 10. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d13b00ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "173b4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from csv import writer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "f4cde4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a8fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622 4361 3739 3739 3739 4361 4361\n"
     ]
    }
   ],
   "source": [
    "#for storing scrap data created empty lists\n",
    "#empty list to store columns in table\n",
    "allrows_col1 = []\n",
    "allrows_col2 = []\n",
    "allrows_col3 = []\n",
    "allrows_col4 = []\n",
    "allrows_col5 = []\n",
    "allrows_col6 = []\n",
    "allrows_col7 = []\n",
    "try:\n",
    "    #opening a www.billboard.com on automated chrome browser\n",
    "    driver.get(\"https://archive.ics.uci.edu/\")\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #clicking on view all data sets  _search button\n",
    "    alldataset_search = driver.find_element(By.XPATH,\"/html/body/table[2]/tbody/tr/td/span/b/a\")\n",
    "    alldataset_search.click()\n",
    "    \n",
    "    #scraping Machine Learning Reposetories dataset\n",
    "    #thi is to Scrape Top heading \"622 Data Set\"\n",
    "    titletag = driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[1]/p')\n",
    "    title = titletag.text\n",
    "    \n",
    "    #scroll down\n",
    "    driver.execute_script(\"window.scrollBy(0,2200)\")\n",
    "    #alltable tags object\n",
    "    leadge_tabletags = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody')\n",
    "    \n",
    "    for i in leadge_tabletags:\n",
    "        # this is tags object for allrows in the table\n",
    "        allrowstags = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr')\n",
    "        #for given row scrape the columns\n",
    "        for i in allrowstags:\n",
    "            \n",
    "            #for Scraping Datasets Name\n",
    "            column1 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/table/tbody/tr/td/p/b/a')\n",
    "            for c1 in column1:\n",
    "                row = c1.text\n",
    "                allrows_col1.append(row)\n",
    "            \n",
    "            #for Scraping Data_type\n",
    "            column2 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td')\n",
    "            for c2 in column2:\n",
    "                row = c2.text\n",
    "                allrows_col2.append(row)\n",
    "            \n",
    "                #for Scraping Task\n",
    "            column3 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/p')\n",
    "            for c3 in column3:\n",
    "                row = c3.text\n",
    "                allrows_col3.append(row)\n",
    "                \n",
    "                #for Scraping Attribute_type     \n",
    "            column4 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/p')\n",
    "            for c4 in column4:\n",
    "                row = c4.text\n",
    "                allrows_col4.append(row)\n",
    "                 \n",
    "                #for Scraping No_of_instances\n",
    "            column5 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/p')\n",
    "            for c5 in column5:\n",
    "                row = c5.text\n",
    "                allrows_col5.append(row)\n",
    "                \n",
    "                \n",
    "                #for Scraping No_of_attribute\n",
    "            column6 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td')\n",
    "            for c6 in column6:\n",
    "                row = c6.text\n",
    "                allrows_col6.append(row)\n",
    "            \n",
    "            #for Scraping Year\n",
    "            column7 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/p')\n",
    "            for c7 in column7:\n",
    "                row = c7.text\n",
    "                allrows_col7.append(row)\n",
    "                \n",
    "            print(len(allrows_col1),len(allrows_col2),len(allrows_col3),len(allrows_col4),len(allrows_col5),len(allrows_col6),len(allrows_col6))\n",
    "                \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MachineLearning_Repo = pd.DataFrame({\"Name\":allrows_col1[0:10],\"DataType\":allrows_col2[0:10],\"Task\":allrows_col3[0:10],\"Attibute Types\":allrows_col4[0:10],\"No.of Insuracne\":allrows_col5[0:10],\"Attributes\":allrows_col6[0:10],\"Year\":allrows_col7[0:10]})\n",
    "print(\"\\n\",title)\n",
    "df_MachineLearning_Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "ed40775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'622 Data Sets'"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titletag = driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[1]/p')\n",
    "title = titletag.text\n",
    "title  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "8253967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "allrows_col1 = []\n",
    "leadge_tabletags = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody')\n",
    "for i in leadge_tabletags:\n",
    "    # this is tags object for allrows in the table\n",
    "    allrowstags = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr')\n",
    "    #for given row scrape the columns\n",
    "    for i in allrowstags[0:2]:\n",
    "        #for Scraping Datasets Name\n",
    "        column1 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/table/tbody/tr/td/p/b/a')\n",
    "        for c1 in column1:\n",
    "            row = c1.text\n",
    "            allrows_col1.append(row)\n",
    "        #for Scraping Datasets Name\n",
    "        column1 = driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td/table/tbody/tr/td/p/b/a')\n",
    "        for c1 in column1:\n",
    "            row = c1.text\n",
    "            allrows_col1.append(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "178ac773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abalone',\n",
       " 'Adult',\n",
       " 'Annealing',\n",
       " 'Anonymous Microsoft Web Data',\n",
       " 'Arrhythmia',\n",
       " 'Artificial Characters',\n",
       " 'Audiology (Original)',\n",
       " 'Audiology (Standardized)',\n",
       " 'Auto MPG',\n",
       " 'Automobile',\n",
       " 'Badges',\n",
       " 'Balance Scale',\n",
       " 'Balloons',\n",
       " 'Breast Cancer',\n",
       " 'Breast Cancer Wisconsin (Original)',\n",
       " 'Breast Cancer Wisconsin (Prognostic)',\n",
       " 'Breast Cancer Wisconsin (Diagnostic)',\n",
       " 'Pittsburgh Bridges',\n",
       " 'Car Evaluation',\n",
       " 'Census Income',\n",
       " 'Chess (King-Rook vs. King-Knight)',\n",
       " 'Chess (King-Rook vs. King-Pawn)',\n",
       " 'Chess (King-Rook vs. King)',\n",
       " 'Chess (Domain Theories)',\n",
       " 'Bach Chorales',\n",
       " 'Connect-4',\n",
       " 'Credit Approval',\n",
       " 'Japanese Credit Screening',\n",
       " 'Computer Hardware',\n",
       " 'Contraceptive Method Choice',\n",
       " 'Covertype',\n",
       " 'Cylinder Bands',\n",
       " 'Dermatology',\n",
       " 'Diabetes',\n",
       " 'DGP2 - The Second Data Generation Program',\n",
       " 'Document Understanding',\n",
       " 'EBL Domain Theories',\n",
       " 'Echocardiogram',\n",
       " 'Ecoli',\n",
       " 'Flags',\n",
       " 'Function Finding',\n",
       " 'Glass Identification',\n",
       " \"Haberman's Survival\",\n",
       " 'Hayes-Roth',\n",
       " 'Heart Disease',\n",
       " 'Hepatitis',\n",
       " 'Horse Colic',\n",
       " 'ICU',\n",
       " 'Image Segmentation',\n",
       " 'Internet Advertisements',\n",
       " 'Ionosphere',\n",
       " 'Iris',\n",
       " 'ISOLET',\n",
       " 'Kinship',\n",
       " 'Labor Relations',\n",
       " 'LED Display Domain',\n",
       " 'Lenses',\n",
       " 'Letter Recognition',\n",
       " 'Liver Disorders',\n",
       " 'Logic Theorist',\n",
       " 'Lung Cancer',\n",
       " 'Lymphography',\n",
       " 'Mechanical Analysis',\n",
       " 'Meta-data',\n",
       " 'Mobile Robots',\n",
       " 'Molecular Biology (Promoter Gene Sequences)',\n",
       " 'Molecular Biology (Protein Secondary Structure)',\n",
       " 'Molecular Biology (Splice-junction Gene Sequences)',\n",
       " \"MONK's Problems\",\n",
       " 'Moral Reasoner',\n",
       " 'Multiple Features',\n",
       " 'Mushroom',\n",
       " 'Musk (Version 1)',\n",
       " 'Musk (Version 2)',\n",
       " 'Nursery',\n",
       " 'Othello Domain Theory',\n",
       " 'Page Blocks Classification',\n",
       " 'Optical Recognition of Handwritten Digits',\n",
       " 'Pen-Based Recognition of Handwritten Digits',\n",
       " 'Post-Operative Patient',\n",
       " 'Primary Tumor',\n",
       " 'Prodigy',\n",
       " 'Qualitative Structure Activity Relationships',\n",
       " 'Quadruped Mammals',\n",
       " 'Servo',\n",
       " 'Shuttle Landing Control',\n",
       " 'Solar Flare',\n",
       " 'Soybean (Large)',\n",
       " 'Soybean (Small)',\n",
       " 'Challenger USA Space Shuttle O-Ring',\n",
       " 'Low Resolution Spectrometer',\n",
       " 'Spambase',\n",
       " 'SPECT Heart',\n",
       " 'SPECTF Heart',\n",
       " 'Sponge',\n",
       " 'Statlog Project',\n",
       " 'Student Loan Relational',\n",
       " 'Teaching Assistant Evaluation',\n",
       " 'Tic-Tac-Toe Endgame',\n",
       " 'Thyroid Disease',\n",
       " 'Trains',\n",
       " 'University',\n",
       " 'Congressional Voting Records',\n",
       " 'Water Treatment Plant',\n",
       " 'Waveform Database Generator (Version 1)',\n",
       " 'Waveform Database Generator (Version 2)',\n",
       " 'Wine',\n",
       " 'Yeast',\n",
       " 'Zoo',\n",
       " 'Undocumented',\n",
       " 'Twenty Newsgroups',\n",
       " 'Australian Sign Language signs',\n",
       " 'Australian Sign Language signs (High Quality)',\n",
       " 'US Census Data (1990)',\n",
       " 'Census-Income (KDD)',\n",
       " 'Coil 1999 Competition Data',\n",
       " 'Corel Image Features',\n",
       " 'E. Coli Genes',\n",
       " 'EEG Database',\n",
       " 'El Nino',\n",
       " 'Entree Chicago Recommendation Data',\n",
       " 'CMU Face Images',\n",
       " 'Insurance Company Benchmark (COIL 2000)',\n",
       " 'Internet Usage Data',\n",
       " 'IPUMS Census Database',\n",
       " 'Japanese Vowels',\n",
       " 'KDD Cup 1998 Data',\n",
       " 'KDD Cup 1999 Data',\n",
       " 'M. Tuberculosis Genes',\n",
       " 'Movie',\n",
       " 'MSNBC.com Anonymous Web Data',\n",
       " 'NSF Research Award Abstracts 1990-2003',\n",
       " 'Pioneer-1 Mobile Robot Data',\n",
       " 'Pseudo Periodic Synthetic Time Series',\n",
       " 'Reuters-21578 Text Categorization Collection',\n",
       " 'Robot Execution Failures',\n",
       " 'Synthetic Control Chart Time Series',\n",
       " 'Syskill and Webert Web Page Ratings',\n",
       " 'UNIX User Data',\n",
       " 'Volcanoes on Venus - JARtool experiment',\n",
       " 'Statlog (Australian Credit Approval)',\n",
       " 'Statlog (German Credit Data)',\n",
       " 'Statlog (Heart)',\n",
       " 'Statlog (Landsat Satellite)',\n",
       " 'Statlog (Image Segmentation)',\n",
       " 'Statlog (Shuttle)',\n",
       " 'Statlog (Vehicle Silhouettes)',\n",
       " 'Connectionist Bench (Nettalk Corpus)',\n",
       " 'Connectionist Bench (Sonar, Mines vs. Rocks)',\n",
       " 'Connectionist Bench (Vowel Recognition - Deterding Data)',\n",
       " 'Economic Sanctions',\n",
       " 'Protein Data',\n",
       " 'Cloud',\n",
       " 'CalIt2 Building People Counts',\n",
       " 'Dodgers Loop Sensor',\n",
       " 'Poker Hand',\n",
       " 'MAGIC Gamma Telescope',\n",
       " 'UJI Pen Characters',\n",
       " 'Mammographic Mass',\n",
       " 'Forest Fires',\n",
       " 'Reuters Transcribed Subset',\n",
       " 'Bag of Words',\n",
       " 'Concrete Compressive Strength',\n",
       " 'Hill-Valley',\n",
       " 'Arcene',\n",
       " 'Dexter',\n",
       " 'Dorothea',\n",
       " 'Gisette',\n",
       " 'Madelon',\n",
       " 'Ozone Level Detection',\n",
       " 'Abscisic Acid Signaling Network',\n",
       " 'Parkinsons',\n",
       " 'Character Trajectories',\n",
       " 'Blood Transfusion Service Center',\n",
       " 'UJI Pen Characters (Version 2)',\n",
       " 'Semeion Handwritten Digit',\n",
       " 'SECOM',\n",
       " 'Plants',\n",
       " 'Libras Movement',\n",
       " 'Concrete Slump Test',\n",
       " 'Communities and Crime',\n",
       " 'Acute Inflammations',\n",
       " 'Wine Quality',\n",
       " 'URL Reputation',\n",
       " 'p53 Mutants',\n",
       " 'Parkinsons Telemonitoring',\n",
       " 'Demospongiae',\n",
       " 'Opinosis Opinion ⁄ Review',\n",
       " 'Breast Tissue',\n",
       " 'Cardiotocography',\n",
       " 'Wall-Following Robot Navigation Data',\n",
       " 'Spoken Arabic Digit',\n",
       " 'Localization Data for Person Activity',\n",
       " 'AutoUniv',\n",
       " 'Steel Plates Faults',\n",
       " 'MiniBooNE particle identification',\n",
       " 'YearPredictionMSD',\n",
       " 'PEMS-SF',\n",
       " 'OpinRank Review Dataset',\n",
       " 'Relative location of CT slices on axial axis',\n",
       " 'Online Handwritten Assamese Characters Dataset',\n",
       " 'PubChem Bioassay Data',\n",
       " 'Record Linkage Comparison Patterns',\n",
       " 'Communities and Crime Unnormalized',\n",
       " 'Vertebral Column',\n",
       " 'EMG Physical Action Data Set',\n",
       " 'Vicon Physical Action Data Set',\n",
       " 'Amazon Commerce reviews set',\n",
       " 'Amazon Access Samples',\n",
       " 'Reuter_50_50',\n",
       " 'Farm Ads',\n",
       " 'DBWorld e-mails',\n",
       " 'KEGG Metabolic Relation Network (Directed)',\n",
       " 'KEGG Metabolic Reaction Network (Undirected)',\n",
       " 'Bank Marketing',\n",
       " 'YouTube Comedy Slam Preference Data',\n",
       " 'Gas Sensor Array Drift Dataset',\n",
       " 'ILPD (Indian Liver Patient Dataset)',\n",
       " 'OPPORTUNITY Activity Recognition',\n",
       " 'Nomao',\n",
       " 'SMS Spam Collection',\n",
       " 'Skin Segmentation',\n",
       " 'Planning Relax',\n",
       " 'PAMAP2 Physical Activity Monitoring',\n",
       " 'Restaurant & consumer data',\n",
       " 'CNAE-9',\n",
       " 'Individual household electric power consumption',\n",
       " 'seeds',\n",
       " 'Northix',\n",
       " 'QtyT40I10D100K',\n",
       " 'Legal Case Reports',\n",
       " 'Human Activity Recognition Using Smartphones',\n",
       " 'One-hundred plant species leaves data set',\n",
       " 'Energy efficiency',\n",
       " 'Yacht Hydrodynamics',\n",
       " 'Fertility',\n",
       " 'Daphnet Freezing of Gait',\n",
       " '3D Road Network (North Jutland, Denmark)',\n",
       " 'ISTANBUL STOCK EXCHANGE',\n",
       " 'Buzz in social media',\n",
       " 'First-order theorem proving',\n",
       " 'Wearable Computing: Classification of Body Postures and Movements (PUC-Rio)',\n",
       " 'Gas sensor arrays in open sampling settings',\n",
       " 'Climate Model Simulation Crashes',\n",
       " 'MicroMass',\n",
       " 'QSAR biodegradation',\n",
       " 'BLOGGER',\n",
       " 'Daily and Sports Activities',\n",
       " 'User Knowledge Modeling',\n",
       " 'Reuters RCV1 RCV2 Multilingual, Multiview Text Categorization Test collection',\n",
       " 'NYSK',\n",
       " 'Turkiye Student Evaluation',\n",
       " \"ser Knowledge Modeling Data (Students' Knowledge Levels on DC Electrical Machines)\",\n",
       " 'EEG Eye State',\n",
       " 'Physicochemical Properties of Protein Tertiary Structure',\n",
       " 'seismic-bumps',\n",
       " 'banknote authentication',\n",
       " 'USPTO Algorithm Challenge, run by NASA-Harvard Tournament Lab and TopCoder Problem: Pat',\n",
       " 'YouTube Multiview Video Games Dataset',\n",
       " 'Gas Sensor Array Drift Dataset at Different Concentrations',\n",
       " 'Activities of Daily Living (ADLs) Recognition Using Binary Sensors',\n",
       " 'SkillCraft1 Master Table Dataset',\n",
       " 'Weight Lifting Exercises monitored with Inertial Measurement Units',\n",
       " 'SML2010',\n",
       " 'Bike Sharing Dataset',\n",
       " 'Predict keywords activities in a online social media',\n",
       " 'Thoracic Surgery Data',\n",
       " 'EMG dataset in Lower Limb',\n",
       " 'SUSY',\n",
       " 'HIGGS',\n",
       " 'Qualitative_Bankruptcy',\n",
       " 'LSVT Voice Rehabilitation',\n",
       " 'Dataset for ADL Recognition with Wrist-worn Accelerometer',\n",
       " 'Wilt',\n",
       " 'User Identification From Walking Activity',\n",
       " 'Activity Recognition from Single Chest-Mounted Accelerometer',\n",
       " 'Leaf',\n",
       " 'Dresses_Attribute_Sales',\n",
       " 'Tamilnadu Electricity Board Hourly Readings',\n",
       " 'Airfoil Self-Noise',\n",
       " 'Wholesale customers',\n",
       " 'Twitter Data set for Arabic Sentiment Analysis',\n",
       " 'Combined Cycle Power Plant',\n",
       " 'Urban Land Cover',\n",
       " 'Diabetes 130-US hospitals for years 1999-2008',\n",
       " 'Bach Choral Harmony',\n",
       " 'StoneFlakes',\n",
       " 'Tennis Major Tournament Match Statistics',\n",
       " 'Parkinson Speech Dataset with Multiple Types of Sound Recordings',\n",
       " 'Gesture Phase Segmentation',\n",
       " 'Perfume Data',\n",
       " 'BlogFeedback',\n",
       " 'REALDISP Activity Recognition Dataset',\n",
       " 'Newspaper and magazine images segmentation dataset',\n",
       " 'AAAI 2014 Accepted Papers',\n",
       " 'Gas sensor array under flow modulation',\n",
       " 'Gas sensor array exposed to turbulent gas mixtures',\n",
       " 'UJIIndoorLoc',\n",
       " 'Sentence Classification',\n",
       " 'Dow Jones Index',\n",
       " 'sEMG for Basic Hand movements',\n",
       " 'AAAI 2013 Accepted Papers',\n",
       " 'Geographical Original of Music',\n",
       " 'Condition Based Maintenance of Naval Propulsion Plants',\n",
       " 'Grammatical Facial Expressions',\n",
       " 'NoisyOffice',\n",
       " 'MHEALTH Dataset',\n",
       " 'Student Performance',\n",
       " 'ElectricityLoadDiagrams20112014',\n",
       " 'Gas sensor array under dynamic gas mixtures',\n",
       " 'microblogPCU',\n",
       " 'Firm-Teacher_Clave-Direction_Classification',\n",
       " 'Dataset for Sensorless Drive Diagnosis',\n",
       " 'TV News Channel Commercial Detection Dataset',\n",
       " 'Phishing Websites',\n",
       " 'Greenhouse Gas Observing Network',\n",
       " 'Diabetic Retinopathy Debrecen Data Set',\n",
       " 'HIV-1 protease cleavage',\n",
       " 'Sentiment Labelled Sentences',\n",
       " 'Online News Popularity',\n",
       " 'Forest type mapping',\n",
       " 'wiki4HE',\n",
       " 'Online Video Characteristics and Transcoding Time Dataset',\n",
       " 'Chronic_Kidney_Disease',\n",
       " 'Machine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014',\n",
       " 'Folio',\n",
       " 'Taxi Service Trajectory - Prediction Challenge, ECML PKDD 2015',\n",
       " 'Cuff-Less Blood Pressure Estimation',\n",
       " 'Smartphone-Based Recognition of Human Activities and Postural Transitions',\n",
       " 'Mice Protein Expression',\n",
       " 'UJIIndoorLoc-Mag',\n",
       " 'Heterogeneity Activity Recognition',\n",
       " 'Educational Process Mining (EPM): A Learning Analytics Data Set',\n",
       " 'HEPMASS',\n",
       " 'Indoor User Movement Prediction from RSS data',\n",
       " 'Open University Learning Analytics dataset',\n",
       " 'default of credit card clients',\n",
       " 'Mesothelioma’s disease data set',\n",
       " 'Online Retail',\n",
       " 'SIFT10M',\n",
       " 'GPS Trajectories',\n",
       " 'Detect Malacious Executable(AntiVirus)',\n",
       " 'Occupancy Detection',\n",
       " 'Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson’s Disease',\n",
       " 'News Aggregator',\n",
       " 'Air Quality',\n",
       " 'Twin gas sensor arrays',\n",
       " 'Gas sensors for home activity monitoring',\n",
       " 'Facebook Comment Volume Dataset',\n",
       " 'Smartphone Dataset for Human Activity Recognition (HAR) in Ambient Assisted Living (AAL)',\n",
       " 'Polish companies bankruptcy data',\n",
       " 'Activity Recognition system based on Multisensor data fusion (AReM)',\n",
       " 'Dota2 Games Results',\n",
       " 'Facebook metrics',\n",
       " 'UbiqLog (smartphone lifelogging)',\n",
       " 'NIPS Conference Papers 1987-2015',\n",
       " 'HTRU2',\n",
       " 'Drug consumption (quantified)',\n",
       " 'Appliances energy prediction',\n",
       " 'Miskolc IIS Hybrid IPS',\n",
       " 'KDC-4007 dataset Collection',\n",
       " 'Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone',\n",
       " 'DrivFace',\n",
       " 'Website Phishing',\n",
       " 'YouTube Spam Collection',\n",
       " 'Beijing PM2.5 Data',\n",
       " 'Cargo 2000 Freight Tracking and Tracing',\n",
       " 'Cervical cancer (Risk Factors)',\n",
       " 'Quality Assessment of Digital Colposcopies',\n",
       " 'KASANDR',\n",
       " 'FMA: A Dataset For Music Analysis',\n",
       " 'Air quality',\n",
       " 'Epileptic Seizure Recognition',\n",
       " 'Devanagari Handwritten Character Dataset',\n",
       " 'Stock portfolio performance',\n",
       " 'MoCap Hand Postures',\n",
       " 'Early biomarkers of Parkinson�s disease based on natural connected speech',\n",
       " 'Data for Software Engineering Teamwork Assessment in Education Setting',\n",
       " 'PM2.5 Data of Five Chinese Cities',\n",
       " 'Parkinson Disease Spiral Drawings Using Digitized Graphics Tablet',\n",
       " 'Sales_Transactions_Dataset_Weekly',\n",
       " 'Las Vegas Strip',\n",
       " 'Eco-hotel',\n",
       " 'MEU-Mobile KSD',\n",
       " 'Crowdsourced Mapping',\n",
       " 'gene expression cancer RNA-Seq',\n",
       " 'Hybrid Indoor Positioning Dataset from WiFi RSSI, Bluetooth and magnetometer',\n",
       " 'chestnut – LARVIC',\n",
       " 'Burst Header Packet (BHP) flooding attack on Optical Burst Switching (OBS) Network',\n",
       " 'Motion Capture Hand Postures',\n",
       " 'Anuran Calls (MFCCs)',\n",
       " 'TTC-3600: Benchmark dataset for Turkish text categorization',\n",
       " 'Gastrointestinal Lesions in Regular Colonoscopy',\n",
       " 'Daily Demand Forecasting Orders',\n",
       " 'Paper Reviews',\n",
       " 'extention of Z-Alizadeh sani dataset',\n",
       " 'Z-Alizadeh Sani',\n",
       " 'Dynamic Features of VirusShare Executables',\n",
       " 'IDA2016Challenge',\n",
       " 'DSRC Vehicle Communications',\n",
       " 'Mturk User-Perceived Clusters over Images',\n",
       " 'Character Font Images',\n",
       " 'DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels',\n",
       " 'Autistic Spectrum Disorder Screening Data for Children',\n",
       " 'Autistic Spectrum Disorder Screening Data for Adolescent',\n",
       " 'APS Failure at Scania Trucks',\n",
       " 'Wireless Indoor Localization',\n",
       " 'HCC Survival',\n",
       " 'CSM (Conventional and Social Media Movies) Dataset 2014 and 2015',\n",
       " 'University of Tehran Question Dataset 2016 (UTQD.2016)',\n",
       " 'Autism Screening Adult',\n",
       " 'Activity recognition with healthy older people using a batteryless wearable sensor',\n",
       " 'Immunotherapy Dataset',\n",
       " 'Cryotherapy Dataset',\n",
       " 'OCT data & Color Fundus Images of Left & Right Eyes',\n",
       " 'Discrete Tone Image Dataset',\n",
       " 'News Popularity in Multiple Social Media Platforms',\n",
       " 'Ultrasonic flowmeter diagnostics',\n",
       " 'ICMLA 2014 Accepted Papers Data Set',\n",
       " 'BLE RSSI Dataset for Indoor localization and Navigation',\n",
       " 'Container Crane Controller Data Set',\n",
       " 'Residential Building Data Set',\n",
       " 'Health News in Twitter',\n",
       " 'chipseq',\n",
       " 'SGEMM GPU kernel performance',\n",
       " 'Repeat Consumption Matrices',\n",
       " 'detection_of_IoT_botnet_attacks_N_BaIoT',\n",
       " 'Absenteeism at work',\n",
       " 'SCADI',\n",
       " 'Condition monitoring of hydraulic systems',\n",
       " 'Carbon Nanotubes',\n",
       " 'Optical Interconnection Network',\n",
       " 'Sports articles for objectivity analysis',\n",
       " 'Breast Cancer Coimbra',\n",
       " 'GNFUV Unmanned Surface Vehicles Sensor Data',\n",
       " 'Dishonest Internet users Dataset',\n",
       " 'Victorian Era Authorship Attribution',\n",
       " 'Simulated Falls and Daily Living Activities Data Set',\n",
       " 'Multimodal Damage Identification for Humanitarian Computing',\n",
       " 'EEG Steady-State Visual Evoked Potential Signals',\n",
       " 'Roman Urdu Data Set',\n",
       " 'Avila',\n",
       " 'PANDOR',\n",
       " 'Drug Review Dataset (Druglib.com)',\n",
       " 'Drug Review Dataset (Drugs.com)',\n",
       " 'Physical Unclonable Functions',\n",
       " 'Superconductivty Data',\n",
       " 'WESAD (Wearable Stress and Affect Detection)',\n",
       " 'GNFUV Unmanned Surface Vehicles Sensor Data Set 2',\n",
       " 'Student Academics Performance',\n",
       " 'Online Shoppers Purchasing Intention Dataset',\n",
       " 'PMU-UD',\n",
       " \"Parkinson's Disease Classification\",\n",
       " 'Electrical Grid Stability Simulated Data',\n",
       " 'Caesarian Section Classification Dataset',\n",
       " 'BAUM-1',\n",
       " 'BAUM-2',\n",
       " 'Audit Data',\n",
       " 'BuddyMove Data Set',\n",
       " 'Real estate valuation data set',\n",
       " 'Early biomarkers of Parkinson’s disease based on natural connected speech Data Set',\n",
       " 'Somerville Happiness Survey',\n",
       " '2.4 GHZ Indoor Channel Measurements',\n",
       " 'EMG data for gestures',\n",
       " 'Parking Birmingham',\n",
       " 'Behavior of the urban traffic of the city of Sao Paulo in Brazil',\n",
       " 'Travel Reviews',\n",
       " 'Tarvel Review Ratings',\n",
       " 'Rice Leaf Diseases',\n",
       " 'Gas sensor array temperature modulation',\n",
       " 'Facebook Live Sellers in Thailand',\n",
       " 'Parkinson Dataset with replicated acoustic features',\n",
       " 'Metro Interstate Traffic Volume',\n",
       " 'Query Analytics Workloads Dataset',\n",
       " 'Wave Energy Converters',\n",
       " 'PPG-DaLiA',\n",
       " 'Alcohol QCM Sensor Dataset',\n",
       " 'Divorce Predictors data set',\n",
       " 'Incident management process enriched event log',\n",
       " 'Opinion Corpus for Lebanese Arabic Reviews (OCLAR)',\n",
       " 'MEx',\n",
       " 'Beijing Multi-Site Air-Quality Data',\n",
       " 'Online Retail II',\n",
       " 'Hepatitis C Virus (HCV) for Egyptian patients',\n",
       " 'QSAR fish toxicity',\n",
       " 'QSAR aquatic toxicity',\n",
       " 'Human Activity Recognition from Continuous Ambient Sensor Data',\n",
       " 'WISDM Smartphone and Smartwatch Activity and Biometrics Dataset',\n",
       " 'QSAR oral toxicity',\n",
       " 'QSAR androgen receptor',\n",
       " 'QSAR Bioconcentration classes dataset',\n",
       " 'QSAR fish bioconcentration factor (BCF)',\n",
       " 'A study of Asian Religious and Biblical Texts',\n",
       " 'Real-time Election Results: Portugal 2019',\n",
       " 'Bias correction of numerical prediction model temperature forecast',\n",
       " 'Bar Crawl: Detecting Heavy Drinking',\n",
       " 'Kitsune Network Attack Dataset',\n",
       " 'Shoulder Implant X-Ray Manufacturer Classification',\n",
       " 'Speaker Accent Recognition',\n",
       " 'Heart failure clinical records',\n",
       " 'Deepfakes: Medical Image Tamper Detection',\n",
       " 'selfBACK',\n",
       " 'South German Credit',\n",
       " 'Exasens',\n",
       " 'Swarm Behaviour',\n",
       " 'Crop mapping using fused optical-radar data set',\n",
       " 'BitcoinHeistRansomwareAddressDataset',\n",
       " 'Facebook Large Page-Page Network',\n",
       " 'Amphibians',\n",
       " 'Early stage diabetes risk prediction dataset.',\n",
       " 'Turkish Spam V01',\n",
       " 'Stock keeping units',\n",
       " 'Demand Forecasting for a store',\n",
       " 'Detect Malware Types',\n",
       " 'Wave Energy Converters',\n",
       " 'Youtube cookery channels viewers comments in Hinglish',\n",
       " 'Pedestrian in Traffic Dataset',\n",
       " 'Cervical Cancer Behavior Risk',\n",
       " 'Sattriya_Dance_Single_Hand_Gestures Dataset',\n",
       " 'Divorce Predictors data set',\n",
       " '3W dataset',\n",
       " 'Malware static and dynamic features VxHeaven and Virus Total',\n",
       " 'Internet Firewall Data',\n",
       " 'User Profiling and Abusive Language Detection Dataset',\n",
       " 'Estimation of obesity levels based on eating habits and physical condition',\n",
       " 'Rice (Cammeo and Osmancik)',\n",
       " 'Vehicle routing and scheduling problems',\n",
       " 'Algerian Forest Fires Dataset',\n",
       " 'Breath Metabolomics',\n",
       " 'Horton General Hospital',\n",
       " 'UrbanGB, urban road accidents coordinates labelled by the urban center',\n",
       " 'Gas Turbine CO and NOx Emission Data Set',\n",
       " 'Activity recognition using wearable physiological measurements',\n",
       " 'clickstream data for online shopping',\n",
       " 'CNNpred: CNN-based stock market prediction using a diverse set of variables',\n",
       " 'Apartment for rent classified',\n",
       " ': Simulated Data set of Iraqi tourism places',\n",
       " 'Nasarian CAD Dataset',\n",
       " 'Monolithic Columns in Troad and Mysia Region',\n",
       " 'Bar Crawl: Detecting Heavy Drinking',\n",
       " 'Seoul Bike Sharing Demand',\n",
       " 'Person Classification Gait Data',\n",
       " 'Shill Bidding Dataset',\n",
       " 'Iranian Churn Dataset',\n",
       " 'Unmanned Aerial Vehicle (UAV) Intrusion Detection',\n",
       " 'Bone marrow transplant: children',\n",
       " 'Exasens',\n",
       " 'COVID-19 Surveillance',\n",
       " 'Refractive errors',\n",
       " 'Shoulder Implant X-Ray Manufacturer Classification',\n",
       " 'CLINC150',\n",
       " 'HCV data',\n",
       " 'Taiwanese Bankruptcy Prediction',\n",
       " 'South German Credit (UPDATE)',\n",
       " 'IIWA14-R820-Gazebo-Dataset-10Trajectories',\n",
       " 'Guitar Chords finger positions',\n",
       " 'Russian Corpus of Biographical Texts',\n",
       " 'Codon usage',\n",
       " 'Intelligent Media Accelerometer and Gyroscope (IM-AccGyro) Dataset',\n",
       " 'Myocardial infarction complications',\n",
       " 'Hungarian Chickenpox Cases',\n",
       " 'Simulated data for survival modelling',\n",
       " 'Student Performance on an entrance examination',\n",
       " 'Chemical Composition of Ceramic Samples',\n",
       " 'Labeled Text Forum Threads Dataset',\n",
       " 'Stock keeping units',\n",
       " 'BLE RSSI dataset for Indoor localization',\n",
       " 'Basketball dataset',\n",
       " 'GitHub MUSAE',\n",
       " 'Anticancer peptides',\n",
       " 'Monolithic Columns in Troad and Mysia Region',\n",
       " 'Gender by Name',\n",
       " 'Iranian Churn Dataset',\n",
       " 'Unmanned Aerial Vehicle (UAV) Intrusion Detection',\n",
       " 'Shoulder Implant Manufacture Classification',\n",
       " 'LastFM Asia Social Network',\n",
       " 'Wheat kernels',\n",
       " 'Productivity Prediction of Garment Employees',\n",
       " 'Multi-view Brain Networks',\n",
       " 'LastFM Asia Social Network',\n",
       " 'Wisesight Sentiment Corpus',\n",
       " 'AI4I 2020 Predictive Maintenance Dataset',\n",
       " 'Dry Bean Dataset',\n",
       " 'in-vehicle coupon recommendation',\n",
       " 'Gait Classification',\n",
       " 'Wikipedia Math Essentials',\n",
       " 'Wikipedia Math Essentials',\n",
       " 'Synchronous Machine Data Set',\n",
       " 'Average Localization Error (ALE) in sensor node localization process in WSNs',\n",
       " '9mers from cullpdb',\n",
       " 'TamilSentiMix',\n",
       " 'Accelerometer',\n",
       " 'Synchronous Machine Data Set',\n",
       " 'Pedal Me Bicycle Deliveries',\n",
       " 'Turkish Headlines Dataset',\n",
       " 'Secondary Mushroom Dataset',\n",
       " 'Power consumption of Tetouan city',\n",
       " 'Raisin Dataset',\n",
       " 'Steel Industry Energy Consumption Dataset',\n",
       " 'Gender Gap in Spanish WP',\n",
       " 'Non verbal tourists data',\n",
       " 'Roman Urdu Sentiment Analysis Dataset (RUSAD)',\n",
       " 'TUANDROMD ( Tezpur University Android Malware Dataset)',\n",
       " 'Higher Education Students Performance Evaluation Dataset',\n",
       " 'Risk Factor prediction of Chronic Kidney Disease',\n",
       " 'Lab Test',\n",
       " 'Shoulder Implant Manufacture Classification',\n",
       " 'Rocket League Skillshots Data Set',\n",
       " 'Sepsis survival minimal clinical records',\n",
       " 'Water Quality Prediction',\n",
       " 'Traffic Flow Forecasting',\n",
       " 'sentiment analysis in Saudi Arabia about distance education during Covid-19',\n",
       " 'Kain Tradisional Sambas',\n",
       " 'Image Recognition Task Execution Times in Mobile Edge Computing',\n",
       " 'REWEMA',\n",
       " 'REJAFADA',\n",
       " 'Steel Industry Energy Consumption Dataset',\n",
       " 'Influenza outbreak event prediction via Twitter data',\n",
       " 'Turkish Music Emotion Dataset',\n",
       " 'Maternal Health Risk Data Set',\n",
       " 'Room Occupancy Estimation',\n",
       " 'Image Recognition Task Execution Times in Mobile Edge Computing',\n",
       " 'Abalone',\n",
       " 'Adult',\n",
       " 'Annealing',\n",
       " 'Anonymous Microsoft Web Data',\n",
       " 'Arrhythmia',\n",
       " 'Artificial Characters',\n",
       " 'Audiology (Original)',\n",
       " 'Audiology (Standardized)',\n",
       " 'Auto MPG',\n",
       " 'Automobile',\n",
       " 'Badges',\n",
       " 'Balance Scale',\n",
       " 'Balloons',\n",
       " 'Breast Cancer',\n",
       " 'Breast Cancer Wisconsin (Original)',\n",
       " 'Breast Cancer Wisconsin (Prognostic)',\n",
       " 'Breast Cancer Wisconsin (Diagnostic)',\n",
       " 'Pittsburgh Bridges',\n",
       " 'Car Evaluation',\n",
       " 'Census Income',\n",
       " 'Chess (King-Rook vs. King-Knight)',\n",
       " 'Chess (King-Rook vs. King-Pawn)',\n",
       " 'Chess (King-Rook vs. King)',\n",
       " 'Chess (Domain Theories)',\n",
       " 'Bach Chorales',\n",
       " 'Connect-4',\n",
       " 'Credit Approval',\n",
       " 'Japanese Credit Screening',\n",
       " 'Computer Hardware',\n",
       " 'Contraceptive Method Choice',\n",
       " 'Covertype',\n",
       " 'Cylinder Bands',\n",
       " 'Dermatology',\n",
       " 'Diabetes',\n",
       " 'DGP2 - The Second Data Generation Program',\n",
       " 'Document Understanding',\n",
       " 'EBL Domain Theories',\n",
       " 'Echocardiogram',\n",
       " 'Ecoli',\n",
       " 'Flags',\n",
       " 'Function Finding',\n",
       " 'Glass Identification',\n",
       " \"Haberman's Survival\",\n",
       " 'Hayes-Roth',\n",
       " 'Heart Disease',\n",
       " 'Hepatitis',\n",
       " 'Horse Colic',\n",
       " 'ICU',\n",
       " 'Image Segmentation',\n",
       " 'Internet Advertisements',\n",
       " 'Ionosphere',\n",
       " 'Iris',\n",
       " 'ISOLET',\n",
       " 'Kinship',\n",
       " 'Labor Relations',\n",
       " 'LED Display Domain',\n",
       " 'Lenses',\n",
       " 'Letter Recognition',\n",
       " 'Liver Disorders',\n",
       " 'Logic Theorist',\n",
       " 'Lung Cancer',\n",
       " 'Lymphography',\n",
       " 'Mechanical Analysis',\n",
       " 'Meta-data',\n",
       " 'Mobile Robots',\n",
       " 'Molecular Biology (Promoter Gene Sequences)',\n",
       " 'Molecular Biology (Protein Secondary Structure)',\n",
       " 'Molecular Biology (Splice-junction Gene Sequences)',\n",
       " \"MONK's Problems\",\n",
       " 'Moral Reasoner',\n",
       " 'Multiple Features',\n",
       " 'Mushroom',\n",
       " 'Musk (Version 1)',\n",
       " 'Musk (Version 2)',\n",
       " 'Nursery',\n",
       " 'Othello Domain Theory',\n",
       " 'Page Blocks Classification',\n",
       " 'Optical Recognition of Handwritten Digits',\n",
       " 'Pen-Based Recognition of Handwritten Digits',\n",
       " 'Post-Operative Patient',\n",
       " 'Primary Tumor',\n",
       " 'Prodigy',\n",
       " 'Qualitative Structure Activity Relationships',\n",
       " 'Quadruped Mammals',\n",
       " 'Servo',\n",
       " 'Shuttle Landing Control',\n",
       " 'Solar Flare',\n",
       " 'Soybean (Large)',\n",
       " 'Soybean (Small)',\n",
       " 'Challenger USA Space Shuttle O-Ring',\n",
       " 'Low Resolution Spectrometer',\n",
       " 'Spambase',\n",
       " 'SPECT Heart',\n",
       " 'SPECTF Heart',\n",
       " 'Sponge',\n",
       " 'Statlog Project',\n",
       " 'Student Loan Relational',\n",
       " 'Teaching Assistant Evaluation',\n",
       " 'Tic-Tac-Toe Endgame',\n",
       " 'Thyroid Disease',\n",
       " 'Trains',\n",
       " 'University',\n",
       " 'Congressional Voting Records',\n",
       " 'Water Treatment Plant',\n",
       " 'Waveform Database Generator (Version 1)',\n",
       " 'Waveform Database Generator (Version 2)',\n",
       " 'Wine',\n",
       " 'Yeast',\n",
       " 'Zoo',\n",
       " 'Undocumented',\n",
       " 'Twenty Newsgroups',\n",
       " 'Australian Sign Language signs',\n",
       " 'Australian Sign Language signs (High Quality)',\n",
       " 'US Census Data (1990)',\n",
       " 'Census-Income (KDD)',\n",
       " 'Coil 1999 Competition Data',\n",
       " 'Corel Image Features',\n",
       " 'E. Coli Genes',\n",
       " 'EEG Database',\n",
       " 'El Nino',\n",
       " 'Entree Chicago Recommendation Data',\n",
       " 'CMU Face Images',\n",
       " 'Insurance Company Benchmark (COIL 2000)',\n",
       " 'Internet Usage Data',\n",
       " 'IPUMS Census Database',\n",
       " 'Japanese Vowels',\n",
       " 'KDD Cup 1998 Data',\n",
       " 'KDD Cup 1999 Data',\n",
       " 'M. Tuberculosis Genes',\n",
       " 'Movie',\n",
       " 'MSNBC.com Anonymous Web Data',\n",
       " 'NSF Research Award Abstracts 1990-2003',\n",
       " 'Pioneer-1 Mobile Robot Data',\n",
       " 'Pseudo Periodic Synthetic Time Series',\n",
       " 'Reuters-21578 Text Categorization Collection',\n",
       " 'Robot Execution Failures',\n",
       " 'Synthetic Control Chart Time Series',\n",
       " 'Syskill and Webert Web Page Ratings',\n",
       " 'UNIX User Data',\n",
       " 'Volcanoes on Venus - JARtool experiment',\n",
       " 'Statlog (Australian Credit Approval)',\n",
       " 'Statlog (German Credit Data)',\n",
       " 'Statlog (Heart)',\n",
       " 'Statlog (Landsat Satellite)',\n",
       " 'Statlog (Image Segmentation)',\n",
       " 'Statlog (Shuttle)',\n",
       " 'Statlog (Vehicle Silhouettes)',\n",
       " 'Connectionist Bench (Nettalk Corpus)',\n",
       " 'Connectionist Bench (Sonar, Mines vs. Rocks)',\n",
       " 'Connectionist Bench (Vowel Recognition - Deterding Data)',\n",
       " 'Economic Sanctions',\n",
       " 'Protein Data',\n",
       " 'Cloud',\n",
       " 'CalIt2 Building People Counts',\n",
       " 'Dodgers Loop Sensor',\n",
       " 'Poker Hand',\n",
       " 'MAGIC Gamma Telescope',\n",
       " 'UJI Pen Characters',\n",
       " 'Mammographic Mass',\n",
       " 'Forest Fires',\n",
       " 'Reuters Transcribed Subset',\n",
       " 'Bag of Words',\n",
       " 'Concrete Compressive Strength',\n",
       " 'Hill-Valley',\n",
       " 'Arcene',\n",
       " 'Dexter',\n",
       " 'Dorothea',\n",
       " 'Gisette',\n",
       " 'Madelon',\n",
       " 'Ozone Level Detection',\n",
       " 'Abscisic Acid Signaling Network',\n",
       " 'Parkinsons',\n",
       " 'Character Trajectories',\n",
       " 'Blood Transfusion Service Center',\n",
       " 'UJI Pen Characters (Version 2)',\n",
       " 'Semeion Handwritten Digit',\n",
       " 'SECOM',\n",
       " 'Plants',\n",
       " 'Libras Movement',\n",
       " 'Concrete Slump Test',\n",
       " 'Communities and Crime',\n",
       " 'Acute Inflammations',\n",
       " 'Wine Quality',\n",
       " 'URL Reputation',\n",
       " 'p53 Mutants',\n",
       " 'Parkinsons Telemonitoring',\n",
       " 'Demospongiae',\n",
       " 'Opinosis Opinion ⁄ Review',\n",
       " 'Breast Tissue',\n",
       " 'Cardiotocography',\n",
       " 'Wall-Following Robot Navigation Data',\n",
       " 'Spoken Arabic Digit',\n",
       " 'Localization Data for Person Activity',\n",
       " 'AutoUniv',\n",
       " 'Steel Plates Faults',\n",
       " 'MiniBooNE particle identification',\n",
       " 'YearPredictionMSD',\n",
       " 'PEMS-SF',\n",
       " 'OpinRank Review Dataset',\n",
       " 'Relative location of CT slices on axial axis',\n",
       " 'Online Handwritten Assamese Characters Dataset',\n",
       " 'PubChem Bioassay Data',\n",
       " 'Record Linkage Comparison Patterns',\n",
       " 'Communities and Crime Unnormalized',\n",
       " 'Vertebral Column',\n",
       " 'EMG Physical Action Data Set',\n",
       " 'Vicon Physical Action Data Set',\n",
       " 'Amazon Commerce reviews set',\n",
       " 'Amazon Access Samples',\n",
       " 'Reuter_50_50',\n",
       " 'Farm Ads',\n",
       " 'DBWorld e-mails',\n",
       " 'KEGG Metabolic Relation Network (Directed)',\n",
       " 'KEGG Metabolic Reaction Network (Undirected)',\n",
       " 'Bank Marketing',\n",
       " 'YouTube Comedy Slam Preference Data',\n",
       " 'Gas Sensor Array Drift Dataset',\n",
       " 'ILPD (Indian Liver Patient Dataset)',\n",
       " 'OPPORTUNITY Activity Recognition',\n",
       " 'Nomao',\n",
       " 'SMS Spam Collection',\n",
       " 'Skin Segmentation',\n",
       " 'Planning Relax',\n",
       " 'PAMAP2 Physical Activity Monitoring',\n",
       " 'Restaurant & consumer data',\n",
       " 'CNAE-9',\n",
       " 'Individual household electric power consumption',\n",
       " 'seeds',\n",
       " 'Northix',\n",
       " 'QtyT40I10D100K',\n",
       " 'Legal Case Reports',\n",
       " 'Human Activity Recognition Using Smartphones',\n",
       " 'One-hundred plant species leaves data set',\n",
       " 'Energy efficiency',\n",
       " 'Yacht Hydrodynamics',\n",
       " 'Fertility',\n",
       " 'Daphnet Freezing of Gait',\n",
       " '3D Road Network (North Jutland, Denmark)',\n",
       " 'ISTANBUL STOCK EXCHANGE',\n",
       " 'Buzz in social media',\n",
       " 'First-order theorem proving',\n",
       " 'Wearable Computing: Classification of Body Postures and Movements (PUC-Rio)',\n",
       " 'Gas sensor arrays in open sampling settings',\n",
       " 'Climate Model Simulation Crashes',\n",
       " 'MicroMass',\n",
       " 'QSAR biodegradation',\n",
       " 'BLOGGER',\n",
       " 'Daily and Sports Activities',\n",
       " 'User Knowledge Modeling',\n",
       " 'Reuters RCV1 RCV2 Multilingual, Multiview Text Categorization Test collection',\n",
       " 'NYSK',\n",
       " 'Turkiye Student Evaluation',\n",
       " \"ser Knowledge Modeling Data (Students' Knowledge Levels on DC Electrical Machines)\",\n",
       " 'EEG Eye State',\n",
       " 'Physicochemical Properties of Protein Tertiary Structure',\n",
       " 'seismic-bumps',\n",
       " 'banknote authentication',\n",
       " 'USPTO Algorithm Challenge, run by NASA-Harvard Tournament Lab and TopCoder Problem: Pat',\n",
       " 'YouTube Multiview Video Games Dataset',\n",
       " 'Gas Sensor Array Drift Dataset at Different Concentrations',\n",
       " 'Activities of Daily Living (ADLs) Recognition Using Binary Sensors',\n",
       " 'SkillCraft1 Master Table Dataset',\n",
       " 'Weight Lifting Exercises monitored with Inertial Measurement Units',\n",
       " 'SML2010',\n",
       " 'Bike Sharing Dataset',\n",
       " 'Predict keywords activities in a online social media',\n",
       " 'Thoracic Surgery Data',\n",
       " 'EMG dataset in Lower Limb',\n",
       " 'SUSY',\n",
       " 'HIGGS',\n",
       " 'Qualitative_Bankruptcy',\n",
       " 'LSVT Voice Rehabilitation',\n",
       " 'Dataset for ADL Recognition with Wrist-worn Accelerometer',\n",
       " 'Wilt',\n",
       " 'User Identification From Walking Activity',\n",
       " 'Activity Recognition from Single Chest-Mounted Accelerometer',\n",
       " 'Leaf',\n",
       " 'Dresses_Attribute_Sales',\n",
       " 'Tamilnadu Electricity Board Hourly Readings',\n",
       " 'Airfoil Self-Noise',\n",
       " 'Wholesale customers',\n",
       " 'Twitter Data set for Arabic Sentiment Analysis',\n",
       " 'Combined Cycle Power Plant',\n",
       " 'Urban Land Cover',\n",
       " 'Diabetes 130-US hospitals for years 1999-2008',\n",
       " 'Bach Choral Harmony',\n",
       " 'StoneFlakes',\n",
       " 'Tennis Major Tournament Match Statistics',\n",
       " 'Parkinson Speech Dataset with Multiple Types of Sound Recordings',\n",
       " 'Gesture Phase Segmentation',\n",
       " 'Perfume Data',\n",
       " 'BlogFeedback',\n",
       " 'REALDISP Activity Recognition Dataset',\n",
       " 'Newspaper and magazine images segmentation dataset',\n",
       " 'AAAI 2014 Accepted Papers',\n",
       " 'Gas sensor array under flow modulation',\n",
       " 'Gas sensor array exposed to turbulent gas mixtures',\n",
       " 'UJIIndoorLoc',\n",
       " 'Sentence Classification',\n",
       " 'Dow Jones Index',\n",
       " 'sEMG for Basic Hand movements',\n",
       " 'AAAI 2013 Accepted Papers',\n",
       " 'Geographical Original of Music',\n",
       " 'Condition Based Maintenance of Naval Propulsion Plants',\n",
       " 'Grammatical Facial Expressions',\n",
       " 'NoisyOffice',\n",
       " 'MHEALTH Dataset',\n",
       " 'Student Performance',\n",
       " 'ElectricityLoadDiagrams20112014',\n",
       " 'Gas sensor array under dynamic gas mixtures',\n",
       " 'microblogPCU',\n",
       " 'Firm-Teacher_Clave-Direction_Classification',\n",
       " 'Dataset for Sensorless Drive Diagnosis',\n",
       " 'TV News Channel Commercial Detection Dataset',\n",
       " 'Phishing Websites',\n",
       " 'Greenhouse Gas Observing Network',\n",
       " 'Diabetic Retinopathy Debrecen Data Set',\n",
       " 'HIV-1 protease cleavage',\n",
       " 'Sentiment Labelled Sentences',\n",
       " 'Online News Popularity',\n",
       " 'Forest type mapping',\n",
       " 'wiki4HE',\n",
       " 'Online Video Characteristics and Transcoding Time Dataset',\n",
       " 'Chronic_Kidney_Disease',\n",
       " 'Machine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014',\n",
       " 'Folio',\n",
       " 'Taxi Service Trajectory - Prediction Challenge, ECML PKDD 2015',\n",
       " 'Cuff-Less Blood Pressure Estimation',\n",
       " 'Smartphone-Based Recognition of Human Activities and Postural Transitions',\n",
       " 'Mice Protein Expression',\n",
       " 'UJIIndoorLoc-Mag',\n",
       " 'Heterogeneity Activity Recognition',\n",
       " 'Educational Process Mining (EPM): A Learning Analytics Data Set',\n",
       " 'HEPMASS',\n",
       " 'Indoor User Movement Prediction from RSS data',\n",
       " 'Open University Learning Analytics dataset',\n",
       " 'default of credit card clients',\n",
       " 'Mesothelioma’s disease data set',\n",
       " 'Online Retail',\n",
       " 'SIFT10M',\n",
       " 'GPS Trajectories',\n",
       " 'Detect Malacious Executable(AntiVirus)',\n",
       " 'Occupancy Detection',\n",
       " 'Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson’s Disease',\n",
       " 'News Aggregator',\n",
       " 'Air Quality',\n",
       " 'Twin gas sensor arrays',\n",
       " 'Gas sensors for home activity monitoring',\n",
       " 'Facebook Comment Volume Dataset',\n",
       " 'Smartphone Dataset for Human Activity Recognition (HAR) in Ambient Assisted Living (AAL)',\n",
       " 'Polish companies bankruptcy data',\n",
       " 'Activity Recognition system based on Multisensor data fusion (AReM)',\n",
       " 'Dota2 Games Results',\n",
       " 'Facebook metrics',\n",
       " 'UbiqLog (smartphone lifelogging)',\n",
       " 'NIPS Conference Papers 1987-2015',\n",
       " 'HTRU2',\n",
       " 'Drug consumption (quantified)',\n",
       " 'Appliances energy prediction',\n",
       " 'Miskolc IIS Hybrid IPS',\n",
       " 'KDC-4007 dataset Collection',\n",
       " 'Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone',\n",
       " 'DrivFace',\n",
       " 'Website Phishing',\n",
       " 'YouTube Spam Collection',\n",
       " 'Beijing PM2.5 Data',\n",
       " 'Cargo 2000 Freight Tracking and Tracing',\n",
       " 'Cervical cancer (Risk Factors)',\n",
       " 'Quality Assessment of Digital Colposcopies',\n",
       " 'KASANDR',\n",
       " 'FMA: A Dataset For Music Analysis',\n",
       " 'Air quality',\n",
       " 'Epileptic Seizure Recognition',\n",
       " 'Devanagari Handwritten Character Dataset',\n",
       " 'Stock portfolio performance',\n",
       " 'MoCap Hand Postures',\n",
       " 'Early biomarkers of Parkinson�s disease based on natural connected speech',\n",
       " 'Data for Software Engineering Teamwork Assessment in Education Setting',\n",
       " ...]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allrows_col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6123f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
