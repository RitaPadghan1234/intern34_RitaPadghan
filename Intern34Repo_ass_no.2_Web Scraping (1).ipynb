{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af0a2af",
   "metadata": {},
   "source": [
    "#      WEB SCRAPING – ASSIGNMENT 2 using selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53df35",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3df74315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "83641c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to know path----------testing purpose starts----\n",
    "#import os\n",
    "#import sys\n",
    "#os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce638bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing purpose\n",
    "#from time import sleep\n",
    "#from selenium import webdriver\n",
    "# this is tested on Chrome or you can use \"webdriver.Chrome()\"\n",
    "#browser = webdriver.Chrome()\n",
    "#browser.get('https://www.youtube.com/')\n",
    "#sleep(5)\n",
    "#browser.close()\n",
    "#-------------------testing purpose ends----------\n",
    "#print(\"selenium sucessfully installed........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9c174135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b4966700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "362964be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first connect to the driver --this is also one of the mwthod do connect to the driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "890dd23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f3096010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as requred in the question\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c00fe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering location as required in the question\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2442cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "28d6d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "exprerience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7576b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#Scraping Job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "#Scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "#Scraping Job Experience form the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    exprerience_required.append(experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b96084fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(company_name),len(job_location),len(exprerience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e908d9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>ExperienceRequired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data and Analytics Analyst</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Analyst - Data Management</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analyst- Web Data Management</td>\n",
       "      <td>Sigma Aldrich Chemicals</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Project Data Analyst</td>\n",
       "      <td>WSP</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Treebo Hotels</td>\n",
       "      <td>Temp. WFH - Bangalore/Bengaluru</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Executive - Data Analyst</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Transformation Analyst I</td>\n",
       "      <td>IHS Markit</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Analyst - Data Modeling/Quality</td>\n",
       "      <td>Get My Parking</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Yallas Technology Solutions Opc</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Job_Title  \\\n",
       "0                   Data and Analytics Analyst   \n",
       "1             Senior Analyst - Data Management   \n",
       "2                 Analyst- Web Data Management   \n",
       "3                       Senior Data Analyst II   \n",
       "4                         Project Data Analyst   \n",
       "5                                 Data Analyst   \n",
       "6                     Executive - Data Analyst   \n",
       "7                Data Transformation Analyst I   \n",
       "8  Senior Data Analyst - Data Modeling/Quality   \n",
       "9                          Senior Data Analyst   \n",
       "\n",
       "                      Company_Name  \\\n",
       "0                              IBM   \n",
       "1                        Accenture   \n",
       "2          Sigma Aldrich Chemicals   \n",
       "3                         Flipkart   \n",
       "4                              WSP   \n",
       "5                    Treebo Hotels   \n",
       "6                         Flipkart   \n",
       "7                       IHS Markit   \n",
       "8                   Get My Parking   \n",
       "9  Yallas Technology Solutions Opc   \n",
       "\n",
       "                                        Job_Location ExperienceRequired  \n",
       "0                                Bangalore/Bengaluru            2-5 Yrs  \n",
       "1                                Bangalore/Bengaluru            5-8 Yrs  \n",
       "2                                Bangalore/Bengaluru            0-1 Yrs  \n",
       "3                                Bangalore/Bengaluru            2-4 Yrs  \n",
       "4                                Bangalore/Bengaluru            0-4 Yrs  \n",
       "5                    Temp. WFH - Bangalore/Bengaluru            1-5 Yrs  \n",
       "6                                Bangalore/Bengaluru            1-5 Yrs  \n",
       "7                                Bangalore/Bengaluru            1-6 Yrs  \n",
       "8                                Bangalore/Bengaluru            4-9 Yrs  \n",
       "9  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...            2-7 Yrs  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating DataFrame from above data\n",
    "df=pd.DataFrame({'Job_Title':job_title,'Company_Name':company_name,'Job_Location':job_location,'ExperienceRequired':exprerience_required})\n",
    "#print DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a673d42",
   "metadata": {},
   "source": [
    "# Q2:Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "8832b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "7e839db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "ff1e67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "c4f47832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "9855c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as requred in the question\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "bbf64c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering location as required in the question\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "cd734dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the Search button\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "709848f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "exprerience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "c14e8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#Scraping Job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "#Scraping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "#Scraping Job Experience form the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    exprerience_required.append(experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "4722c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(company_name),len(job_location),len(exprerience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "eef335bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>ExperienceRequired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Tata Nexarc</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>Bangalore/Bengaluru, Nagpur, Pune</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data &amp; Analytics Lead, Geo Analytics - GAMMA</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weather and Climate Data Scientist</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mindtree</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Bizongo</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Baker Hughes</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manager-Data Science</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACN - Applied Intelligence - Data Scientist - 09</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Job_Title             Company_Name  \\\n",
       "0                  Analystics & Modeling Specialist                Accenture   \n",
       "1                                    Data Scientist              Tata Nexarc   \n",
       "2                                    Data Scientist            Tech Mahindra   \n",
       "3      Data & Analytics Lead, Geo Analytics - GAMMA  Boston Consulting Group   \n",
       "4                Weather and Climate Data Scientist            Shell Pvt Ltd   \n",
       "5                                    Data Scientist                 Mindtree   \n",
       "6                               Data Scientist - II                  Bizongo   \n",
       "7                             Senior Data Scientist             Baker Hughes   \n",
       "8                              Manager-Data Science         AMERICAN EXPRESS   \n",
       "9  ACN - Applied Intelligence - Data Scientist - 09                Accenture   \n",
       "\n",
       "                                        Job_Location ExperienceRequired  \n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...            6-8 Yrs  \n",
       "1  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...            4-8 Yrs  \n",
       "2                  Bangalore/Bengaluru, Nagpur, Pune            5-8 Yrs  \n",
       "3                                Bangalore/Bengaluru           7-10 Yrs  \n",
       "4                                Bangalore/Bengaluru           5-12 Yrs  \n",
       "5  Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...           5-10 Yrs  \n",
       "6     Bangalore/Bengaluru, India, Mumbai (All Areas)            3-6 Yrs  \n",
       "7                        Bangalore/Bengaluru, Mumbai            6-8 Yrs  \n",
       "8                                Bangalore/Bengaluru            3-4 Yrs  \n",
       "9                                Bangalore/Bengaluru            2-6 Yrs  "
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating DataFrame from above data\n",
    "df=pd.DataFrame({'Job_Title':job_title,'Company_Name':company_name,'Job_Location':job_location,'ExperienceRequired':exprerience_required})\n",
    "#print DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef23646",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "        You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d68f06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "433b4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing liabries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7489bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f8ff7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "346acf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as requred in the question\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c33c4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering location as required in the question\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys('Delhi/NCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b11a794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the Search button\n",
    "search1=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7804ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the checkbox for salary where salary is  \"3-6\"Lakes\n",
    "checkbox_salary=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "checkbox_salary.click()\n",
    "\n",
    "# clicking the checkbox for Location where Location is  “Delhi/NCR”\n",
    "#checkbox_location=driver.find_element(By.PATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[13]/div[3]/div[2]/div[1]/div/div[1]/div[4]/label/i\")\n",
    "#checkbox_location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e27e6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the checkbox for Location where Location is  “Delhi/NCR”\n",
    "checkbox_location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[13]/div[2]/div[4]/label/i\")\n",
    "checkbox_location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6858ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_title=[]\n",
    "j_loc=[]\n",
    "j_company_name=[]\n",
    "j_experience_req=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ce4c04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Job title from the given page\n",
    "j_title_tags1=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in j_title_tags1[0:10]:\n",
    "    title=i.text\n",
    "    j_title.append(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4421c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Job location from the given page\n",
    "j_loc_tags1=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in j_loc_tags1[0:10]:\n",
    "    location=i.text\n",
    "    j_loc.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fc41b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping company name from the given page\n",
    "j_c_name_tags1=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in j_c_name_tags1[0:10]:\n",
    "    comp_name=i.text\n",
    "    j_company_name.append(comp_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3bcc64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping experience required from the given page\n",
    "\n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    j_experience_req.append(experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a00a1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(j_title),len(j_loc),len(j_company_name),len(j_experience_req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9e39a460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>ExperienceRequired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence/Computer Vision Engine...</td>\n",
       "      <td>Vicara</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Activation Specialist - Adobe Target</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>NGI Ventures</td>\n",
       "      <td>Temp. WFH - Noida</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opening For Jr. Data Scientist with Tatras Dat...</td>\n",
       "      <td>Tatras Data Services</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>torcai digital media</td>\n",
       "      <td>Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Americana Restaurants (india)</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>URGENT: Data Scientist | Gurugram | 5 Days Wor...</td>\n",
       "      <td>Digilytics</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist | Boutique AI and data consulti...</td>\n",
       "      <td>unnati</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Alliance Recruitment Agency</td>\n",
       "      <td>Noida</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Artificial Intelligence/Computer Vision Engine...   \n",
       "1          Data Activation Specialist - Adobe Target   \n",
       "2                  Data Scientist - Engine Algorithm   \n",
       "3                                     Data Scientist   \n",
       "4  Opening For Jr. Data Scientist with Tatras Dat...   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7  URGENT: Data Scientist | Gurugram | 5 Days Wor...   \n",
       "8  Data Scientist | Boutique AI and data consulti...   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                    Company_Name  \\\n",
       "0                         Vicara   \n",
       "1                 Okda Solutions   \n",
       "2                   Primo Hiring   \n",
       "3                   NGI Ventures   \n",
       "4           Tatras Data Services   \n",
       "5           torcai digital media   \n",
       "6  Americana Restaurants (india)   \n",
       "7                     Digilytics   \n",
       "8                         unnati   \n",
       "9    Alliance Recruitment Agency   \n",
       "\n",
       "                                        Job_Location ExperienceRequired  \n",
       "0  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...            1-3 Yrs  \n",
       "1  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...           7-10 Yrs  \n",
       "2  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...            1-3 Yrs  \n",
       "3                                  Temp. WFH - Noida            1-5 Yrs  \n",
       "4                                        Delhi / NCR            2-4 Yrs  \n",
       "5  Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...            2-7 Yrs  \n",
       "6                                   Gurgaon/Gurugram            3-8 Yrs  \n",
       "7                                   Gurgaon/Gurugram            2-5 Yrs  \n",
       "8                 Noida, New Delhi, Gurgaon/Gurugram            2-7 Yrs  \n",
       "9                                              Noida            3-4 Yrs  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating DataFrame from above data\n",
    "df3=pd.DataFrame({'Job_Title':j_title,'Company_Name':j_company_name,'Job_Location':j_loc,'ExperienceRequired':j_experience_req})\n",
    "#print DataFrame\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9756c23",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "484615c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "83943d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "5ae812c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "44a5380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "#Closing login page\n",
    "close = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "40868312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “sunglasses” in the search field where “search for products, brands and more” is written \n",
    "item=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "item.send_keys(\"Sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "035d4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click the search icon\n",
    "search1 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "c6e7cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data from this page\n",
    "Brand=[]\n",
    "ProductDescription=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "78b0323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping page 1 data attribute Brand_tags\n",
    "#scraping Brand name from this page\n",
    "#all tags object is created that has decribed the product Brand using driver.find_elements function\n",
    "Brand_tags = driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "#for  all tags object  created\n",
    "for i in Brand_tags:\n",
    "    #name is retrive using object Brand_tags\n",
    "    name = i.text\n",
    "    #retrived Brand name is stored in Brand list\n",
    "    Brand.append(name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a568650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping page 1 data attribute Product_description\n",
    "#scraping ProductDescription from the web page\n",
    "#all tags object is created that has decribed the product describtion using driver.find_elements function\n",
    "Product_desc_tags = driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "#for all tags object created\n",
    "for i in Product_desc_tags:\n",
    "    #decription retirved using the object Product_desc_tags\n",
    "    Description = i.text\n",
    "    #retrived Product Description is stored in ProductDescription list\n",
    "    ProductDescription.append(Description)\n",
    "#to check whaeather code is working correct just try to print it\n",
    "#print(ProductDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c43a5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping page 1 data attribute Price\n",
    "#Scraping th Price from the web page\n",
    "Price_tags = driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "#for all tags object is created\n",
    "for i in Price_tags:\n",
    "    #price retrived using the object Price_tags\n",
    "    mrp=i.text\n",
    "    #retrived Price is stored in List Price\n",
    "    Price.append(mrp)\n",
    "#print(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7861f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 40\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),len(ProductDescription),len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "8cf32ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: element click intercepted: Element <a class=\"ge-49M\" href=\"/search?q=Sunglasses&amp;otracker=search&amp;otracker1=search&amp;marketplace=FLIPKART&amp;as-show=off&amp;as=off&amp;page=...\">2</a> is not clickable at point (707, 16). Other element would receive the click: <button class=\"L0Z3Pu\" type=\"submit\">...</button>\n",
      "  (Session info: chrome=108.0.5359.99)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\t(No symbol) [0x0076F243]\n",
      "\t(No symbol) [0x006F7FD1]\n",
      "\t(No symbol) [0x005ED04D]\n",
      "\t(No symbol) [0x006228B9]\n",
      "\t(No symbol) [0x006208CC]\n",
      "\t(No symbol) [0x0061E4CB]\n",
      "\t(No symbol) [0x0061D164]\n",
      "\t(No symbol) [0x006132A6]\n",
      "\t(No symbol) [0x0063858C]\n",
      "\t(No symbol) [0x00612BFF]\n",
      "\t(No symbol) [0x00638804]\n",
      "\t(No symbol) [0x0064C9EB]\n",
      "\t(No symbol) [0x00638386]\n",
      "\t(No symbol) [0x0061163C]\n",
      "\t(No symbol) [0x0061269D]\n",
      "\tGetHandleVerifier [0x00A09A22+2655074]\n",
      "\tGetHandleVerifier [0x009FCA24+2601828]\n",
      "\tGetHandleVerifier [0x00818C0A+619850]\n",
      "\tGetHandleVerifier [0x00817830+614768]\n",
      "\t(No symbol) [0x007005FC]\n",
      "\t(No symbol) [0x00705968]\n",
      "\t(No symbol) [0x00705A55]\n",
      "\t(No symbol) [0x0071051B]\n",
      "\tBaseThreadInitThunk [0x76F1FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77717BBE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77717B8E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #Scroll down    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #go to next page by creating object of next page and calling click() method  to go to the next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[2]\")\n",
    "    next_button.click()\n",
    "    \n",
    "    #scroll up\n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4f7f7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Data of page 2\n",
    "\n",
    "#scraping Brand name from this page\n",
    "#all tags object is created that has decribed the product Brand using driver.find_elements function\n",
    "Brand_tags = driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "#for  all tags object  created\n",
    "for i in Brand_tags:\n",
    "    #name is retrive using object Brand_tags\n",
    "    name = i.text\n",
    "    #retrived Brand name is stored in Brand list\n",
    "    Brand.append(name)\n",
    "    \n",
    "#scraping ProductDescription from the web page\n",
    "#all tags object is created that has decribed the product describtion using driver.find_elements function\n",
    "Product_desc_tags = driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "#for all tags object created\n",
    "for i in Product_desc_tags:\n",
    "    #decription retirved using the object Product_desc_tags\n",
    "    Description = i.text\n",
    "    #retrived Product Description is stored in ProductDescription list\n",
    "    ProductDescription.append(Description)\n",
    "#to check whaeather code is working correct just try to print it\n",
    "#print(ProductDescription)   \n",
    "\n",
    "#Scraping th Price from the web page\n",
    "Price_tags = driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "#for all tags object is created\n",
    "for i in Price_tags:\n",
    "    #price retrived using the object Price_tags\n",
    "    mrp=i.text\n",
    "    #retrived Price is stored in List Price\n",
    "    Price.append(mrp)\n",
    "#print(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5912571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80 80\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),\n",
    "len(ProductDescription),\n",
    "len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "19f5e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Data of page 3\n",
    "#scraping Brand name from this page\n",
    "#all tags object is created that has decribed the product Brand using driver.find_elements function\n",
    "Brand_tags = driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "#for  all tags object  created\n",
    "for i in Brand_tags[0:20]:\n",
    "    #name is retrive using object Brand_tags\n",
    "    name = i.text\n",
    "    #retrived Brand name is stored in Brand list\n",
    "    Brand.append(name)\n",
    "    \n",
    "#scraping ProductDescription from the web page\n",
    "#all tags object is created that has decribed the product describtion using driver.find_elements function\n",
    "Product_desc_tags = driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "#for all tags object created\n",
    "for i in Product_desc_tags[0:20]:\n",
    "    #decription retirved using the object Product_desc_tags\n",
    "    Description = i.text\n",
    "    #retrived Product Description is stored in ProductDescription list\n",
    "    ProductDescription.append(Description)\n",
    "#to check whaeather code is working correct just try to print it\n",
    "#print(ProductDescription)   \n",
    "\n",
    "#Scraping th Price from the web page\n",
    "Price_tags = driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "#for all tags object is created\n",
    "for i in Price_tags[0:20]:\n",
    "    #price retrived using the object Price_tags\n",
    "    mrp=i.text\n",
    "    #retrived Price is stored in List Price\n",
    "    Price.append(mrp)\n",
    "#print(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d1a58217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),\n",
    "len(ProductDescription),\n",
    "len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3e002c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame({\"Brand\":Brand[0:100],\n",
    "\"ProductDescription\":ProductDescription[0:100],\n",
    "\"Price\":Price[0:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "44cb5f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>ProductDescription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>UV Protection, Polarized Rectangular Sunglasse...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Rectangul...</td>\n",
       "      <td>₹719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                 ProductDescription Price\n",
       "0   ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...  ₹549\n",
       "1    VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...  ₹806\n",
       "2       LIZA ANGEL  Riding Glasses, Night Vision Spectacle Sunglas...  ₹129\n",
       "3        Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹149\n",
       "4         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹599\n",
       "..             ...                                                ...   ...\n",
       "95      LIZA ANGEL  UV Protection, Polarized Rectangular Sunglasse...  ₹129\n",
       "96   VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...  ₹719\n",
       "97   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...  ₹949\n",
       "98            SRPM             UV Protection Wayfarer Sunglasses (50)  ₹149\n",
       "99       Elligator                UV Protection Round Sunglasses (54)  ₹286\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "afef12b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>ProductDescription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (53)</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>₹649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Gradient, Toughened Glass Lens, UV Protection ...</td>\n",
       "      <td>₹597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand                                 ProductDescription Price\n",
       "0      ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...  ₹549\n",
       "1       VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...  ₹806\n",
       "2          LIZA ANGEL  Riding Glasses, Night Vision Spectacle Sunglas...  ₹129\n",
       "3            Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹489\n",
       "4            Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹599\n",
       "5           Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹149\n",
       "6          PHENOMENAL         UV Protection Retro Square Sunglasses (53)  ₹129\n",
       "7      ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ₹398\n",
       "8           Elligator                UV Protection Round Sunglasses (53)  ₹149\n",
       "9   SHAAH COLLECTIONS  UV Protection, Polarized, Mirrored Rectangular...  ₹159\n",
       "10     ROZZETTA CRAFT              UV Protection Aviator Sunglasses (55)  ₹649\n",
       "11       Singco India  Gradient, Toughened Glass Lens, UV Protection ...  ₹597\n",
       "12          Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹129\n",
       "13           Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹539\n",
       "14               SRPM             UV Protection Wayfarer Sunglasses (50)  ₹149\n",
       "15          Elligator                UV Protection Round Sunglasses (53)  ₹149"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e7e50",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manuall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "05db5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "af942653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "4eef076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "a209a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&q=APPLE+iPhone+11+%28Black%2C+64+GB%29&store=tyy%2F4io&spotlightTagId=BestsellerId_tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=d7a238c9-3ab3-4908-bfe6-d68f8861a521.MOBFWQ6BXGJCEYNY.SEARCH&ppt=sp&ppn=sp&ssid=flrlc3h7b40000001670706388561&qH=3230bb64f5cac56a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c61623d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll to Pixel 42000 of Webpage\n",
    "driver.execute_script(\"window.scrollTo(0,4000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "529fd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to click on All 10215 reviews drop down list of reviews\n",
    "#go the that location\n",
    "allrevstag=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[7]/div/a/div/span\")\n",
    "#calling click() method of selenium\n",
    "allrevstag.click()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "1fa541ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function defination for scraping all attributes i.e.Rating,Review_summary,Full_review\n",
    "#empty lists to save retrived data elements\n",
    "\n",
    "def RatingFunction(Rating,Review_summary,Full_review):\n",
    "    Rating = []\n",
    "    Review_summary = []\n",
    "    Full_review = []\n",
    "\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "36d4ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping page 1 data\n",
    "try:\n",
    "    #Scraping page 1 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    Rating = []\n",
    "    Review_summary = []\n",
    "    Full_review = []\n",
    "\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #Scroll down\n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #click on page no.2\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[2]\")\n",
    "    next_button.click()\n",
    "    #Scroll up\n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 2 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[3]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 3 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[4]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 4 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[5]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 5 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[6]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 6 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 7 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    # Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[8]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 8 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[9]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 9 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[10]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Scraping page 10 data\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    #scroll down \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #Go to next page\n",
    "    next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    #Scroll up    \n",
    "    js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n",
    "    driver.execute_script(js)\n",
    "    time.sleep(3)\n",
    "    #RatingFunction(Rating,Review_summary,Full_review)\n",
    "    #scraping rating,Review_summary,Full_review\n",
    "    r_tags=driver.find_elements(By.XPATH,\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for j in r_tags:\n",
    "        R = j.text\n",
    "        Rating.append(R)\n",
    "    #print(len(Rating))\n",
    "\n",
    "    #Scraping Review_summary\n",
    "    review_tags=driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\")\n",
    "    for j in review_tags:\n",
    "        R_text = j.text\n",
    "        Review_summary.append(R_text)\n",
    "    #print(len(Review_summary))\n",
    "\n",
    "    #scraping Full review\n",
    "    fulrevtags=driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']\")\n",
    "    for j in fulrevtags:\n",
    "        F_R_text = j.text\n",
    "        Full_review.append(F_R_text)\n",
    "    #print(len(Full_review))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "86060dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 110 110\n"
     ]
    }
   ],
   "source": [
    "print(len(Rating),len(Review_summary),len(Full_review)) #some entires are blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4304387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>FullReview_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Got delivered by good packing from Flipkart wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Iphone is best ever but Flipkart delivered is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Quite osm more then expected . Purple color tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>My first iPhone. Still wondering where was I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>PROS:\\n1. Ballistic performance\\n2. Amazing im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>iPhone 11 is good but if someone want Superior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>Everything good as expected from Apple, but on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>Great product as usual. Handy phone with best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Works seamlessly, I get an avg 1.5 days batter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Terrific!!! Lucky to get this phone in first l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                 Review  \\\n",
       "0       5              Just wow!   \n",
       "1       5              Fabulous!   \n",
       "2       5  Mind-blowing purchase   \n",
       "3       5              Just wow!   \n",
       "4       5      Worth every penny   \n",
       "..    ...                    ...   \n",
       "95      4              Just wow!   \n",
       "96      5                 Super!   \n",
       "97      5            Good choice   \n",
       "98      5              Wonderful   \n",
       "99      4          Great product   \n",
       "\n",
       "                                   FullReview_Summary  \n",
       "0   Got delivered by good packing from Flipkart wi...  \n",
       "1   Iphone is best ever but Flipkart delivered is ...  \n",
       "2   Quite osm more then expected . Purple color tr...  \n",
       "3   My first iPhone. Still wondering where was I l...  \n",
       "4   PROS:\\n1. Ballistic performance\\n2. Amazing im...  \n",
       "..                                                ...  \n",
       "95  iPhone 11 is good but if someone want Superior...  \n",
       "96  Everything good as expected from Apple, but on...  \n",
       "97  Great product as usual. Handy phone with best ...  \n",
       "98  Works seamlessly, I get an avg 1.5 days batter...  \n",
       "99  Terrific!!! Lucky to get this phone in first l...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5=pd.DataFrame({\"Rating\":Rating[0:100],\"Review\":Review_summary[0:100],\"FullReview_Summary\":Full_review[0:100]})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "89a7af65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>FullReview_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Got delivered by good packing from Flipkart wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Iphone is best ever but Flipkart delivered is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Quite osm more then expected . Purple color tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>My first iPhone. Still wondering where was I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>PROS:\\n1. Ballistic performance\\n2. Amazing im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>iPhone 11 is good but if someone want Superior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>Everything good as expected from Apple, but on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>Great product as usual. Handy phone with best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Works seamlessly, I get an avg 1.5 days batter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Terrific!!! Lucky to get this phone in first l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>Value-for-money</td>\n",
       "      <td>As usual a great product from Apple. but the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Good buy.. working perfectly !\\n\\nThat was upg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Value-for-money</td>\n",
       "      <td>After 20 days above writing this review\\n\\nI a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Bought iphone 11 last month. I must say that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Very good phone. All the apps are running smoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Awesome Product. Initially I was looking for g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>This iphone 11 by apple is simply amazing . Io...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first I phone and I'm very happy to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Finally switched to iOS from android\\nThere is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Excellent service by Flipkart. The phone is tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                 Review  \\\n",
       "0       5              Just wow!   \n",
       "1       5              Fabulous!   \n",
       "2       5  Mind-blowing purchase   \n",
       "3       5              Just wow!   \n",
       "4       5      Worth every penny   \n",
       "5       5              Just wow!   \n",
       "6       5                 Super!   \n",
       "7       4            Good choice   \n",
       "8       5              Wonderful   \n",
       "9       5          Great product   \n",
       "10      4        Value-for-money   \n",
       "11      5               Terrific   \n",
       "12      4        Value-for-money   \n",
       "13      4              Wonderful   \n",
       "14      5      Terrific purchase   \n",
       "15      5       Perfect product!   \n",
       "16      5          Great product   \n",
       "17      5              Fabulous!   \n",
       "18      5         Classy product   \n",
       "19      5       Perfect product!   \n",
       "\n",
       "                                   FullReview_Summary  \n",
       "0   Got delivered by good packing from Flipkart wi...  \n",
       "1   Iphone is best ever but Flipkart delivered is ...  \n",
       "2   Quite osm more then expected . Purple color tr...  \n",
       "3   My first iPhone. Still wondering where was I l...  \n",
       "4   PROS:\\n1. Ballistic performance\\n2. Amazing im...  \n",
       "5   iPhone 11 is good but if someone want Superior...  \n",
       "6   Everything good as expected from Apple, but on...  \n",
       "7   Great product as usual. Handy phone with best ...  \n",
       "8   Works seamlessly, I get an avg 1.5 days batter...  \n",
       "9   Terrific!!! Lucky to get this phone in first l...  \n",
       "10  As usual a great product from Apple. but the l...  \n",
       "11  Good buy.. working perfectly !\\n\\nThat was upg...  \n",
       "12  After 20 days above writing this review\\n\\nI a...  \n",
       "13  Bought iphone 11 last month. I must say that t...  \n",
       "14  Very good phone. All the apps are running smoo...  \n",
       "15  Awesome Product. Initially I was looking for g...  \n",
       "16  This iphone 11 by apple is simply amazing . Io...  \n",
       "17  This is my first I phone and I'm very happy to...  \n",
       "18  Finally switched to iOS from android\\nThere is...  \n",
       "19  Excellent service by Flipkart. The phone is tr...  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961ccf",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "852b1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "927acdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d83b5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cbb1bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart page on automated chrome browser\n",
    "driver.get(\"https://flipkart.com\")\n",
    "#Closing login page\n",
    "close = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0acf3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “sneakers” in the search field where “search for products, brands and more” is written \n",
    "item=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "item.send_keys(\"sneakers\")\n",
    "#click the search icon\n",
    "search1 = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "61b108d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data from this page\n",
    "Brand=[]\n",
    "ProductDescription=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9604d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping page 1 data\n",
    "#scraping Brand name from this page\n",
    "#all tags object is created that has decribed the product Brand using driver.find_elements function\n",
    "Brand_tags = driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "#for  all tags object  created\n",
    "for i in Brand_tags:\n",
    "    #name is retrive using object Brand_tags\n",
    "    name = i.text\n",
    "    #retrived Brand name is stored in Brand list\n",
    "    Brand.append(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0e6b7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping page 1 data\n",
    "#scraping ProductDescription from the web page\n",
    "#all tags object is created that has decribed the product describtion using driver.find_elements function\n",
    "Product_desc_tags = driver.find_elements(By.XPATH,\"//div[@class='_2B099V']\")\n",
    "#for all tags object created\n",
    "for i in Product_desc_tags:\n",
    "    #decription retirved using the object Product_desc_tags\n",
    "    Description = i.text\n",
    "    #retrived Product Description is stored in ProductDescription list\n",
    "    ProductDescription.append(Description)\n",
    "#to check whaeather code is working correct just try to print it\n",
    "#print(ProductDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f109cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping page 1 data\n",
    "#Scraping th Price from the web page\n",
    "Price_tags = driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "#for all tags object is created\n",
    "for i in Price_tags:\n",
    "    #price retrived using the object Price_tags\n",
    "    mrp=i.text\n",
    "    #retrived Price is stored in List Price\n",
    "    Price.append(mrp)\n",
    "#print(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1a0a6f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 40\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),len(ProductDescription),len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "87d29ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go to next page by creating object of next page and calling click() method  to go to the next page\n",
    "next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[2]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "924d712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Data of page 2\n",
    "\n",
    "#scraping Brand name from this page\n",
    "#all tags object is created that has decribed the product Brand using driver.find_elements function\n",
    "Brand_tags = driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "#for  all tags object  created\n",
    "for i in Brand_tags:\n",
    "    #name is retrive using object Brand_tags\n",
    "    name = i.text\n",
    "    #retrived Brand name is stored in Brand list\n",
    "    Brand.append(name)\n",
    "    \n",
    "#scraping ProductDescription from the web page\n",
    "#all tags object is created that has decribed the product describtion using driver.find_elements function\n",
    "Product_desc_tags = driver.find_elements(By.XPATH,\"//div[@class='_2B099V']\")\n",
    "#for all tags object created\n",
    "for i in Product_desc_tags:\n",
    "    #decription retirved using the object Product_desc_tags\n",
    "    Description = i.text\n",
    "    #retrived Product Description is stored in ProductDescription list\n",
    "    ProductDescription.append(Description)\n",
    "#to check whaeather code is working correct just try to print it\n",
    "#print(ProductDescription)   \n",
    "\n",
    "#Scraping th Price from the web page\n",
    "Price_tags = driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "#for all tags object is created\n",
    "for i in Price_tags:\n",
    "    #price retrived using the object Price_tags\n",
    "    mrp=i.text\n",
    "    #retrived Price is stored in List Price\n",
    "    Price.append(mrp)\n",
    "#print(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "253ebf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80 80\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),len(ProductDescription),len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2dace4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go to next page by creating object of next page and calling click() method  to go to the next page\n",
    "next_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[2]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "497e2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Data of page 3\n",
    "\n",
    "#scraping Brand name from this page\n",
    "#all tags object is created that has decribed the product Brand using driver.find_elements function\n",
    "Brand_tags = driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "#for  all tags object  created\n",
    "for i in Brand_tags[0:20]:\n",
    "    #name is retrive using object Brand_tags\n",
    "    name = i.text\n",
    "    #retrived Brand name is stored in Brand list\n",
    "    Brand.append(name)\n",
    "    \n",
    "#scraping ProductDescription from the web page\n",
    "#all tags object is created that has decribed the product describtion using driver.find_elements function\n",
    "Product_desc_tags = driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "#for all tags object created\n",
    "for i in Product_desc_tags[0:20]:\n",
    "    #decription retirved using the object Product_desc_tags\n",
    "    Description = i.text\n",
    "    #retrived Product Description is stored in ProductDescription list\n",
    "    ProductDescription.append(Description)\n",
    "#to check whaeather code is working correct just try to print it\n",
    "#print(ProductDescription)   \n",
    "\n",
    "#Scraping th Price from the web page\n",
    "Price_tags = driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "#for all tags object is created\n",
    "for i in Price_tags[0:20]:\n",
    "    #price retrived using the object Price_tags\n",
    "    mrp=i.text\n",
    "    #retrived Price is stored in List Price\n",
    "    Price.append(mrp)\n",
    "#print(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4988695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand),\n",
    "len(ProductDescription),\n",
    "len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4c41fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>ProductDescription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RED CHIEF</td>\n",
       "      <td>RED CHIEF\\nRC3522 107 Sneakers For Men\\n₹2,294...</td>\n",
       "      <td>₹2,294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RED CHIEF</td>\n",
       "      <td>RED CHIEF\\nRC3483 022 Sneakers For Men\\n₹2,067...</td>\n",
       "      <td>₹2,067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labbin</td>\n",
       "      <td>Labbin\\nSneakers For Men\\n₹399₹99960% off\\nFre...</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shozie</td>\n",
       "      <td>Shozie\\nSneakers For Men\\n₹295₹99970% off\\nFre...</td>\n",
       "      <td>₹295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aadi</td>\n",
       "      <td>aadi\\nMesh | Ultralightweight | Comfortable | ...</td>\n",
       "      <td>₹359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Mast &amp; Harbour</td>\n",
       "      <td>Casuals, Canvas, Partywear Sneakers For Men</td>\n",
       "      <td>₹824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Kraasa</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                 ProductDescription  \\\n",
       "0             RED CHIEF  RED CHIEF\\nRC3522 107 Sneakers For Men\\n₹2,294...   \n",
       "1             RED CHIEF  RED CHIEF\\nRC3483 022 Sneakers For Men\\n₹2,067...   \n",
       "2                Labbin  Labbin\\nSneakers For Men\\n₹399₹99960% off\\nFre...   \n",
       "3                Shozie  Shozie\\nSneakers For Men\\n₹295₹99970% off\\nFre...   \n",
       "4                  aadi  aadi\\nMesh | Ultralightweight | Comfortable | ...   \n",
       "..                  ...                                                ...   \n",
       "95  World Wear Footwear                                   Sneakers For Men   \n",
       "96             RapidBox                                   Sneakers For Men   \n",
       "97       Mast & Harbour        Casuals, Canvas, Partywear Sneakers For Men   \n",
       "98               Kraasa                                 Sneakers For Women   \n",
       "99            Deals4you                                   Sneakers For Men   \n",
       "\n",
       "     Price  \n",
       "0   ₹2,294  \n",
       "1   ₹2,067  \n",
       "2     ₹399  \n",
       "3     ₹295  \n",
       "4     ₹359  \n",
       "..     ...  \n",
       "95    ₹259  \n",
       "96    ₹599  \n",
       "97    ₹824  \n",
       "98    ₹449  \n",
       "99    ₹398  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6=pd.DataFrame({\"Brand\":Brand,\n",
    "\"ProductDescription\":ProductDescription,\n",
    "\"Price\":Price})\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d6721",
   "metadata": {},
   "source": [
    "# Q7: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "c8032cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "48c4d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "315b7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "0cbf732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the amazon page on automated chrome browser\n",
    "driver.get(\"http://www.amazon.in\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "3cd8a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “ iphone 10” in the search field where “search for products, brands and more” is written \n",
    "item7=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "item7.send_keys(\"Laptop\")\n",
    "#click the search icon\n",
    "search7 = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "beebb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the checkbox for  set CPU Type filter to “Intel Core i7”\n",
    "\n",
    "checkbox_CPU_Type=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]/ul[6]/li[12]/span/a/div/label/i\")\n",
    "checkbox_CPU_Type.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e5b8b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for scraping\n",
    "Title = []   #empty list to store Title of top 10 model\n",
    "Ratings = [] #Empty list to store Rating of top 10 model\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "81cbfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping top 10 Laptop models Ratings\n",
    "\n",
    "#all page contents where name is Title of Lapto\n",
    "title_tags = driver.find_elements(By.XPATH,\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "#Scraping top 10 Laptop models Title\n",
    "for i in title_tags[0:10]:\n",
    "    name = i.text\n",
    "    Title.append(name)\n",
    "#print(len(Title))\n",
    "\n",
    "#create object for whole page rating tags \n",
    "Rating_tags = driver.find_elements(By.XPATH,\"//span[@class='a-size-base']\")\n",
    "for i in Rating_tags[0:10]:\n",
    "    #retrive text in object is stored into rtng variable\n",
    "    rtng=i.text\n",
    "    #List for Rating is appended\n",
    "    Ratings.append(rtng)\n",
    "#print(len(Ratings))\n",
    "\n",
    "#Scraping Price\n",
    "price_tags=driver.find_elements(By.XPATH,\"//span[@class='a-price-whole']\")\n",
    "for i in price_tags[0:10]:\n",
    "    prc=i.text\n",
    "    Price.append(prc)\n",
    "#print(len(price_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "0e99b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "#check len of all attribute elements should be same to create dataframe\n",
    "print(len(Title),len(Ratings),len(Price ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "a294a9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Name</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acer Predator Helios 300 Gaming Laptop Intel C...</td>\n",
       "      <td>(3.9)</td>\n",
       "      <td>1,15,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dell New Alienware x15 R2 Gaming Laptop, Intel...</td>\n",
       "      <td>(4.2)</td>\n",
       "      <td>3,56,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS Zenbook Pro 14 Duo OLED (2022) Dual Scree...</td>\n",
       "      <td>(4.7)</td>\n",
       "      <td>2,14,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Omen 12th Gen Intel Core i9-12900HX 17.3 in...</td>\n",
       "      <td>(3.9)</td>\n",
       "      <td>3,39,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acer Predator Helios 300 Gaming Laptop Intel C...</td>\n",
       "      <td>(5.0)</td>\n",
       "      <td>1,49,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Acer Predator Helios 300 Gaming Laptop Intel C...</td>\n",
       "      <td>(3.7)</td>\n",
       "      <td>1,89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MSI Raider GE67 HX 12UHS,Intel 12th Gen. Core ...</td>\n",
       "      <td>(3.0)</td>\n",
       "      <td>4,29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dell G15-5521 Gaming Laptop, i9-12900H, 16GB D...</td>\n",
       "      <td>(3.1)</td>\n",
       "      <td>1,81,833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Acer Predator Helios 500 Gaming Laptop (11Th G...</td>\n",
       "      <td>(4.7)</td>\n",
       "      <td>4,32,435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hp Envy 15- 11Th Gen Intel Core I9/32Gb/1Tb Ss...</td>\n",
       "      <td>(1.0)</td>\n",
       "      <td>1,87,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Laptop Name Ratings     Price\n",
       "0  Acer Predator Helios 300 Gaming Laptop Intel C...   (3.9)  1,15,990\n",
       "1  Dell New Alienware x15 R2 Gaming Laptop, Intel...   (4.2)  3,56,600\n",
       "2  ASUS Zenbook Pro 14 Duo OLED (2022) Dual Scree...   (4.7)  2,14,990\n",
       "3  HP Omen 12th Gen Intel Core i9-12900HX 17.3 in...   (3.9)  3,39,500\n",
       "4  Acer Predator Helios 300 Gaming Laptop Intel C...   (5.0)  1,49,990\n",
       "5  Acer Predator Helios 300 Gaming Laptop Intel C...   (3.7)  1,89,990\n",
       "6  MSI Raider GE67 HX 12UHS,Intel 12th Gen. Core ...   (3.0)  4,29,990\n",
       "7  Dell G15-5521 Gaming Laptop, i9-12900H, 16GB D...   (3.1)  1,81,833\n",
       "8  Acer Predator Helios 500 Gaming Laptop (11Th G...   (4.7)  4,32,435\n",
       "9  Hp Envy 15- 11Th Gen Intel Core I9/32Gb/1Tb Ss...   (1.0)  1,87,990"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Data Frame by passing directory using pandas DataFrame method/function\n",
    "df7 = pd.DataFrame({\"Laptop Name\":Title,\"Ratings\":Ratings,\"Price\":Price})\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b74f8e",
   "metadata": {},
   "source": [
    "# Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1.First get the webpage https://www.azquotes.com/\n",
    "2. Click on Top Quotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "c671d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "71196188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "cc468635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "fd390853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the azquotes page on automated chrome browser\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "#time.sleep(20)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "6b7f5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clicking on to the top quotes first click on drop down list\n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "1d0af203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For selecting Top Quotes option \n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d60b5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list to store scraped data\n",
    "Quote = [] \n",
    "Author = []\n",
    "TypeOf_Quotes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "70a7661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Scraping Quote\n",
    "quotes_tags=driver.find_elements(By.XPATH,\"//a[@class='title']\")\n",
    "for i in quotes_tags:\n",
    "    quote_text = i.text\n",
    "    Quote.append(quote_text)\n",
    "\n",
    "#Scraping Author\n",
    "author_tag = driver.find_elements(By.XPATH,\"//div[@class='author']\")\n",
    "for i in author_tag:\n",
    "    name = i.text\n",
    "    Author.append(name)\n",
    "#print(Author)\n",
    "\n",
    "#Scraping TypeOfQuotes\n",
    "TypeofQ_tags = driver.find_elements(By.XPATH,\"//div[@class='tags']\")\n",
    "for i in TypeofQ_tags:\n",
    "    about = i.text\n",
    "    TypeOf_Quotes.append(about)\n",
    "\n",
    "#for i in Type_of_Quotes:\n",
    "#pprint(Type_of_Quotes)\n",
    "#Click on next button\n",
    "  #  driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a\").click()\n",
    "#print(len(Quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "760260b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Quote),len(Author),len(TypeOf_Quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "0c055814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Type_Of_Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>When the going gets weird, the weird turn pro.</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Music, Sports, Hunting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>When a train goes through a tunnel and it gets...</td>\n",
       "      <td>Corrie Ten Boom</td>\n",
       "      <td>Trust, Encouraging, Uplifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>If you think you are too small to make a diffe...</td>\n",
       "      <td>Dalai Lama</td>\n",
       "      <td>Inspirational, Funny, Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>God doesn't require us to succeed, he only req...</td>\n",
       "      <td>Mother Teresa</td>\n",
       "      <td>Success, God, Mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Change your thoughts and you change your world.</td>\n",
       "      <td>Norman Vincent Peale</td>\n",
       "      <td>Inspirational, Motivational, Change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Quote                Author  \\\n",
       "0   The essence of strategy is choosing what not t...        Michael Porter   \n",
       "1   One cannot and must not try to erase the past ...            Golda Meir   \n",
       "2   Patriotism means to stand by the country. It d...    Theodore Roosevelt   \n",
       "3   Death is something inevitable. When a man has ...        Nelson Mandela   \n",
       "4   You have to love a nation that celebrates its ...          Erma Bombeck   \n",
       "..                                                ...                   ...   \n",
       "95     When the going gets weird, the weird turn pro.    Hunter S. Thompson   \n",
       "96  When a train goes through a tunnel and it gets...       Corrie Ten Boom   \n",
       "97  If you think you are too small to make a diffe...            Dalai Lama   \n",
       "98  God doesn't require us to succeed, he only req...         Mother Teresa   \n",
       "99    Change your thoughts and you change your world.  Norman Vincent Peale   \n",
       "\n",
       "                              Type_Of_Quotes  \n",
       "0   Essence, Deep Thought, Transcendentalism  \n",
       "1                  Inspiration, Past, Trying  \n",
       "2                        Country, Peace, War  \n",
       "3         Inspirational, Motivational, Death  \n",
       "4               4th Of July, Food, Patriotic  \n",
       "..                                       ...  \n",
       "95                    Music, Sports, Hunting  \n",
       "96             Trust, Encouraging, Uplifting  \n",
       "97              Inspirational, Funny, Change  \n",
       "98                      Success, God, Mother  \n",
       "99       Inspirational, Motivational, Change  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8=pd.DataFrame({\"Quote\":Quote,\"Author\":Author,\"Type_Of_Quotes\":TypeOf_Quotes})\n",
    "df8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a16c46",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to display list of respected former Prime Ministers of India (i.e. Name, Born-Dead,Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "26648a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7cd5301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from pprint import pprint\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "72e562c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "346330f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the azquotes page on automated chrome browser\n",
    "driver.get(\"https://www.jagranjosh.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "23d38f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GK_optiontag=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div[1]/div/div[6]/div/div[1]/header/div[3]/ul/li[9]/a\")\n",
    "GK_optiontag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "53eb92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrimeOFIND_tag=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/div/div[10]/div/div/ul/li[2]/a\")\n",
    "PrimeOFIND_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0c433bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page \n",
    "#Scroll to Pixel 42000 of Webpage\n",
    "driver.execute_script(\"window.scrollTo(0,1600)\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c11d1757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The List of all Prime Ministers of India\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN. Name Born-Dead_Year Term of office Remark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.  Jawahar Lal Nehru  (1889–1964)  15 August ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List of International Awards received by Naren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.  Gulzarilal Nanda (Acting)  (1898-1998)  27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.  Lal Bahadur Shastri  (1904–1966)  9 June 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.   Gulzari Lal Nanda  (Acting)  (1898-1998) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.  Indira Gandhi  (1917–1984)  24 January 196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.  Morarji Desai  (1896–1995)  24 March 1977 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.  Charan Singh  (1902–1987)  28 July 1979 to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.  Indira Gandhi  (1917–1984)  14 January 198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.  Rajiv Gandhi  (1944–1991)  31 October 1984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.  V. P. Singh  (1931–2008)  2 December 1989...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.  Chandra Shekhar  (1927–2007)  10 November...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.  P. V. Narasimha Rao  (1921–2004)  21 June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.  Atal Bihari Vajpayee  (1924- 2018)  16 Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.  H. D. Deve Gowda  (born 1933)  1 June 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.  Inder Kumar Gujral  (1919–2012)  21 April...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.  Atal Bihari Vajpayee  (1924-2018)  19 Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.  Manmohan Singh  (born 1932)  22 May 2004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.  Narendra Modi  (born 1950)  26 May 2014 -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SN. Name Born-Dead_Year Term of office Remark\n",
       "0   1.  Jawahar Lal Nehru  (1889–1964)  15 August ...\n",
       "1   List of International Awards received by Naren...\n",
       "2   2.  Gulzarilal Nanda (Acting)  (1898-1998)  27...\n",
       "3   3.  Lal Bahadur Shastri  (1904–1966)  9 June 1...\n",
       "4   4.   Gulzari Lal Nanda  (Acting)  (1898-1998) ...\n",
       "5   5.  Indira Gandhi  (1917–1984)  24 January 196...\n",
       "6   6.  Morarji Desai  (1896–1995)  24 March 1977 ...\n",
       "7   7.  Charan Singh  (1902–1987)  28 July 1979 to...\n",
       "8   8.  Indira Gandhi  (1917–1984)  14 January 198...\n",
       "9   9.  Rajiv Gandhi  (1944–1991)  31 October 1984...\n",
       "10  10.  V. P. Singh  (1931–2008)  2 December 1989...\n",
       "11  11.  Chandra Shekhar  (1927–2007)  10 November...\n",
       "12  12.  P. V. Narasimha Rao  (1921–2004)  21 June...\n",
       "13  13.  Atal Bihari Vajpayee  (1924- 2018)  16 Ma...\n",
       "14  14.  H. D. Deve Gowda  (born 1933)  1 June 199...\n",
       "15  15.  Inder Kumar Gujral  (1919–2012)  21 April...\n",
       "16  16.  Atal Bihari Vajpayee  (1924-2018)  19 Mar...\n",
       "17  17.  Manmohan Singh  (born 1932)  22 May 2004 ...\n",
       "18  18.  Narendra Modi  (born 1950)  26 May 2014 -..."
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading=[]#empty list to keep Columns names\n",
    "#leadge table for all enteries i.e. tablebody\n",
    "leadgefor_rows = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr\")\n",
    "\n",
    "#for printing heading\n",
    "header_row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[1]\")\n",
    "for title in header_row:\n",
    "    title=(title.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    heading.append(title)\n",
    "#print(heading)\n",
    "\n",
    "all_rows = []\n",
    "#for printing row no.2\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[2]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "#print(all_rows)\n",
    "\n",
    "#for printing row no.3\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[3]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.4\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[4]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "\n",
    "#for printing row no.5\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[5]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "\n",
    "#for printing row no.6\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[6]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "\n",
    "#for printing row no.7\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[7]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "\n",
    "#for printing row no.8\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[8]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "\n",
    "#for printing row no.9\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[9]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.10\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[10]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "\n",
    "#for printing row no.11\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[11]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.12\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[12]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.13\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[13]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.14\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[14]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.15\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[15]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.16\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[16]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.17\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[17]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.18\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[18]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#for printing row no.19\n",
    "row = driver.find_elements(By.XPATH,\"//div[@class='table-box']//table//tbody//tr[19]\")\n",
    "for detail in row:\n",
    "    detail=(detail.text).rstrip(\"\\n\") #convert the element to text and strip\n",
    "    #append clean column name to headings\n",
    "    all_rows.append(detail)\n",
    "\n",
    "#df9=pd.DataFrame({\"header\":heading,\"Data\":all_rows)\n",
    "#print(len(all_rows))\n",
    "\n",
    "#create a Data Frame \n",
    "print(\"The List of all Prime Ministers of India\")\n",
    "cleanall_rows = []\n",
    "#print(\"Original List:   \"+str(all_rows))\n",
    "# Replace substring in list of strings\n",
    "#rec = [sub.replace('\\n','  ') for sub in all_rows]\n",
    "#cleanall_rows = rec.split()\n",
    "#print(\"\\n\\nModified   : \"+str(rec))\n",
    "#print(\"****length of list\",len(rec))\n",
    "\n",
    "cleanall_rows=rec\n",
    "\n",
    "df9=pd.DataFrame({\"    SN. Name Born-Dead_Year Term of office Remark\":cleanall_rows})\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252aebca",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name ,Description and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on leftside.\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "779899ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\atharv\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "bb2b660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "#import required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from pprint import pprint\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "1bb13b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Atharv\\Downloads\\chromedriver_win32.zip\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0193221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the azquotes page on automated chrome browser\n",
    "driver.get(\"https://www.motor1.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "5b11ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to click on drop down list option\n",
    "dwltags = driver.find_element(By.XPATH,\"/html/body/div[3]/div[2]/div/div/div[1]/div\")\n",
    "dwltags.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "5d0e6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clicking/selecting Features from list\n",
    "featuretag = driver.find_element(By.XPATH,\"/html/body/div[4]/div[1]/div[3]/ul/li[5]/button\")\n",
    "featuretag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6665608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for select LISTS \n",
    "lst_tag=driver.find_element(By.XPATH,\"/html/body/div[4]/div[1]/div[3]/ul/li[6]/ul/li[1]/a\")\n",
    "lst_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "aa406e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to click on the 50 Most Expensive cars in the world\n",
    "mostexpcartags=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[1]/div[1]/div/div/div[2]/div/div[1]/h3/a\")\n",
    "mostexpcartags.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "72622a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ee5eb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CarName = []\n",
    "Price = []\n",
    "Car_details = []\n",
    "\n",
    "#print(\"\\n50 Most Expensive Cars In The WorldPage_heading\")\n",
    "#scroll down \n",
    "\n",
    "allcartags = driver.find_elements(By.XPATH,\"//h3[@class='subheader']\")\n",
    "#scraping top 50 most expensive Car Names\n",
    "for i in allcartags[0:50]:\n",
    "    name = i.text\n",
    "    CarName.append(name)\n",
    "#print(len(CarName))#Cheaking purpose\n",
    "\n",
    "#Scraping price\n",
    "for i in allcartags[0:50]:\n",
    "    prc= driver.find_elements(By.XPATH,\"/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p/strong\")\n",
    "    for j in prc:\n",
    "        daam = j.text\n",
    "        Price.append(daam)\n",
    "#print(Price)\n",
    "\n",
    "#Scraping Description\n",
    "for i in allcartags[0:50]:\n",
    "    desctag = driver.find_elements(By.XPATH,\"/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p\")\n",
    "    for j in desctag:\n",
    "        carinfo = j.text\n",
    "        Car_details.append(carinfo)\n",
    "#print(len(Car_details))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "340c6e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 2550 5150\n"
     ]
    }
   ],
   "source": [
    "#print and check if all attributes elements are same in number\n",
    "print(len(CarName),len(Price),len(Car_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "fdec7326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Car_Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Detail_Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drako GTE</td>\n",
       "      <td>Price: $1.2 Million</td>\n",
       "      <td>When it comes to high-end hypercars and super-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>De Tomaso P72</td>\n",
       "      <td>Price: $1.3 Million</td>\n",
       "      <td>But in order to find out which of these unatta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ferrari LaFerrari</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "      <td>We should note, though: The prices listed here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pagani Huayra</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "      <td>Price: $1.2 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>McLaren Elva</td>\n",
       "      <td></td>\n",
       "      <td>The Drako GTE is a super sedan in every sense ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Czinger 21C</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>Price: $1.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ferrari Monza</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>The De Tomaso P72 is basically the definition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gordon Murray T.33</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Koenigsegg Gemera</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>At $1.4 million new, the Ferrari LaFerrari is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zenvo TSR-S</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hennessey Venom F5</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "      <td>Inarguably one of the prettiest cars on this l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bentley Bacalar</td>\n",
       "      <td>Price: $1.8 Million</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hispano Suiza Carmen Boulogne</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "      <td>The McLaren Elva is one of the latest addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bentley Mulliner Batur</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Deus Vayanne</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "      <td>You might not know the name Czinger yet, but t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SSC Tuatara</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lotus Evija</td>\n",
       "      <td>Price: $2.0 Million*</td>\n",
       "      <td>Much like the roof-less McLaren Elva, the Ferr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Aston Martin Vulcan</td>\n",
       "      <td>Price: $2.1 Million</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Delage D12</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "      <td>The second and slightly more affordable superc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>McLaren Speedtail</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rimac Nevera</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "      <td>One of two Koenigsegg models on this list, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pagani Utopia</td>\n",
       "      <td>Price: $2.4 Million</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Pininfarina Battista</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "      <td>Hailing from Denmark, the Zenvo TSR-S debuted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ferrari FXX K Evo</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "      <td>Price: $1.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gordon Murray T.50</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "      <td>The Hennessey Venom GT was a record-breaker, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lamborghini Countach</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mercedes-AMG Project One</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "      <td>With just 12 total units produced, the Bentley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aston Martin Victor</td>\n",
       "      <td>Price: $2.7 Million</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hennessey Venom F5 Roadster</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "      <td>To call the Hispano Suiza Carmen Boulogne beau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Koenigsegg Jesko</td>\n",
       "      <td>$3.0 Million</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Aston Martin Valkyrie</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "      <td>The electric onslaught is coming. Bentley says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>W Motors Lykan Hypersport</td>\n",
       "      <td>Price: $3.2 Million</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>McLaren Solus</td>\n",
       "      <td>Price: $3.4 Million</td>\n",
       "      <td>The Deus Vayanne may not be a household name (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Pagani Huayra Roadster BC</td>\n",
       "      <td>$3.5 Million</td>\n",
       "      <td>Price: $2.0 Million*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Bugatti Chiron Pur Sport</td>\n",
       "      <td>Price: $3.5 Million</td>\n",
       "      <td>Although initially cloaked in controversy, SSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Lamborghini Sian</td>\n",
       "      <td>Price: $3.6 Million</td>\n",
       "      <td>Price: $2.1 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Koenigsegg CC850</td>\n",
       "      <td>Price: $3.6 million</td>\n",
       "      <td>With a new Emira sports car and an Eletre elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bugatti Chiron Super Sport 300+</td>\n",
       "      <td>Price: $3.7 Million</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Lamborghini Veneno</td>\n",
       "      <td>Price: $3.9 Million</td>\n",
       "      <td>As with a few other cars on this list, the Ast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Bugatti Bolide</td>\n",
       "      <td>Price: $4.5 Million</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Bugatti Mistral</td>\n",
       "      <td>Price: $4.7 Million</td>\n",
       "      <td>You may have heard of Delage before. In the ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Pagani Huayra Imola</td>\n",
       "      <td>Price: $5.0 Million</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Bugatti Divo</td>\n",
       "      <td>Price: $5.4 Million</td>\n",
       "      <td>What would you pay for the fastest production ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SP Automotive Chaos</td>\n",
       "      <td>Price: $5.8 Million</td>\n",
       "      <td>Price: $2.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Pagani Codalunga</td>\n",
       "      <td>Price: $6.4 Million</td>\n",
       "      <td>The Rimac Nevera takes the title of most expen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mercedes-Maybach Exelero</td>\n",
       "      <td>Price: $7.4 Million</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Bugatti Centodieci</td>\n",
       "      <td>Price: $8.0 Million</td>\n",
       "      <td>First came the Zonda, then the Huayra, and now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Rolls-Royce Sweptail</td>\n",
       "      <td>Price: $9.0 Million</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Bugatti La Voiture Noire</td>\n",
       "      <td>Price: $12.8 Million</td>\n",
       "      <td>Aptly named after the company’s founder, Batti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Rolls-Royce Boat Tail*</td>\n",
       "      <td>Price: $13.4 Million</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Car_Name                 Price  \\\n",
       "0                         Drako GTE   Price: $1.2 Million   \n",
       "1                     De Tomaso P72   Price: $1.3 Million   \n",
       "2                 Ferrari LaFerrari   Price: $1.4 Million   \n",
       "3                     Pagani Huayra   Price: $1.4 Million   \n",
       "4                      McLaren Elva                         \n",
       "5                       Czinger 21C   Price: $1.7 Million   \n",
       "6                     Ferrari Monza   Price: $1.7 Million   \n",
       "7                Gordon Murray T.33   Price: $1.7 Million   \n",
       "8                 Koenigsegg Gemera   Price: $1.7 Million   \n",
       "9                       Zenvo TSR-S   Price: $1.7 Million   \n",
       "10               Hennessey Venom F5   Price: $1.7 Million   \n",
       "11                  Bentley Bacalar   Price: $1.8 Million   \n",
       "12    Hispano Suiza Carmen Boulogne   Price: $1.9 Million   \n",
       "13           Bentley Mulliner Batur   Price: $1.9 Million   \n",
       "14                     Deus Vayanne   Price: $2.0 Million   \n",
       "15                      SSC Tuatara   Price: $2.0 Million   \n",
       "16                      Lotus Evija  Price: $2.0 Million*   \n",
       "17              Aston Martin Vulcan   Price: $2.1 Million   \n",
       "18                       Delage D12   Price: $2.3 Million   \n",
       "19                McLaren Speedtail   Price: $2.3 Million   \n",
       "20                     Rimac Nevera   Price: $2.3 Million   \n",
       "21                    Pagani Utopia   Price: $2.4 Million   \n",
       "22             Pininfarina Battista   Price: $2.5 Million   \n",
       "23                Ferrari FXX K Evo   Price: $2.5 Million   \n",
       "24               Gordon Murray T.50   Price: $2.6 Million   \n",
       "25             Lamborghini Countach   Price: $2.6 Million   \n",
       "26         Mercedes-AMG Project One   Price: $2.6 Million   \n",
       "27              Aston Martin Victor   Price: $2.7 Million   \n",
       "28      Hennessey Venom F5 Roadster   Price: $3.0 Million   \n",
       "29                 Koenigsegg Jesko          $3.0 Million   \n",
       "30            Aston Martin Valkyrie   Price: $3.0 Million   \n",
       "31        W Motors Lykan Hypersport   Price: $3.2 Million   \n",
       "32                    McLaren Solus   Price: $3.4 Million   \n",
       "33        Pagani Huayra Roadster BC          $3.5 Million   \n",
       "34         Bugatti Chiron Pur Sport   Price: $3.5 Million   \n",
       "35                 Lamborghini Sian   Price: $3.6 Million   \n",
       "36                 Koenigsegg CC850   Price: $3.6 million   \n",
       "37  Bugatti Chiron Super Sport 300+   Price: $3.7 Million   \n",
       "38               Lamborghini Veneno   Price: $3.9 Million   \n",
       "39                   Bugatti Bolide   Price: $4.5 Million   \n",
       "40                  Bugatti Mistral   Price: $4.7 Million   \n",
       "41              Pagani Huayra Imola   Price: $5.0 Million   \n",
       "42                     Bugatti Divo   Price: $5.4 Million   \n",
       "43              SP Automotive Chaos   Price: $5.8 Million   \n",
       "44                 Pagani Codalunga   Price: $6.4 Million   \n",
       "45         Mercedes-Maybach Exelero   Price: $7.4 Million   \n",
       "46               Bugatti Centodieci   Price: $8.0 Million   \n",
       "47             Rolls-Royce Sweptail   Price: $9.0 Million   \n",
       "48         Bugatti La Voiture Noire  Price: $12.8 Million   \n",
       "49           Rolls-Royce Boat Tail*  Price: $13.4 Million   \n",
       "\n",
       "                                   Detail_Information  \n",
       "0   When it comes to high-end hypercars and super-...  \n",
       "1   But in order to find out which of these unatta...  \n",
       "2   We should note, though: The prices listed here...  \n",
       "3                                 Price: $1.2 Million  \n",
       "4   The Drako GTE is a super sedan in every sense ...  \n",
       "5                                 Price: $1.3 Million  \n",
       "6   The De Tomaso P72 is basically the definition ...  \n",
       "7                                 Price: $1.4 Million  \n",
       "8   At $1.4 million new, the Ferrari LaFerrari is ...  \n",
       "9                                 Price: $1.4 Million  \n",
       "10  Inarguably one of the prettiest cars on this l...  \n",
       "11                                Price: $1.7 Million  \n",
       "12  The McLaren Elva is one of the latest addition...  \n",
       "13                                Price: $1.7 Million  \n",
       "14  You might not know the name Czinger yet, but t...  \n",
       "15                                Price: $1.7 Million  \n",
       "16  Much like the roof-less McLaren Elva, the Ferr...  \n",
       "17                                Price: $1.7 Million  \n",
       "18  The second and slightly more affordable superc...  \n",
       "19                                Price: $1.7 Million  \n",
       "20  One of two Koenigsegg models on this list, the...  \n",
       "21                                Price: $1.7 Million  \n",
       "22  Hailing from Denmark, the Zenvo TSR-S debuted ...  \n",
       "23                                Price: $1.8 Million  \n",
       "24  The Hennessey Venom GT was a record-breaker, t...  \n",
       "25                                Price: $1.9 Million  \n",
       "26  With just 12 total units produced, the Bentley...  \n",
       "27                                Price: $1.9 Million  \n",
       "28  To call the Hispano Suiza Carmen Boulogne beau...  \n",
       "29                                Price: $2.0 Million  \n",
       "30  The electric onslaught is coming. Bentley says...  \n",
       "31                                Price: $2.0 Million  \n",
       "32  The Deus Vayanne may not be a household name (...  \n",
       "33                               Price: $2.0 Million*  \n",
       "34  Although initially cloaked in controversy, SSC...  \n",
       "35                                Price: $2.1 Million  \n",
       "36  With a new Emira sports car and an Eletre elec...  \n",
       "37                                Price: $2.3 Million  \n",
       "38  As with a few other cars on this list, the Ast...  \n",
       "39                                Price: $2.3 Million  \n",
       "40  You may have heard of Delage before. In the ea...  \n",
       "41                                Price: $2.3 Million  \n",
       "42  What would you pay for the fastest production ...  \n",
       "43                                Price: $2.4 Million  \n",
       "44  The Rimac Nevera takes the title of most expen...  \n",
       "45                                Price: $2.5 Million  \n",
       "46  First came the Zonda, then the Huayra, and now...  \n",
       "47                                Price: $2.5 Million  \n",
       "48  Aptly named after the company’s founder, Batti...  \n",
       "49                                Price: $2.6 Million  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10=pd.DataFrame({\"Car_Name\":CarName[0:50],\"Price\":Price[0:50],\"Detail_Information\":Car_details[0:50]})\n",
    "#print(df10)\n",
    "df10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d03d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f6a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
